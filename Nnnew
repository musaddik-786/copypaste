Nice — understood. I updated your project so that instead of writing the reference into the local stubbed.xlsx, the code will:
	•	Download the fetched-emails Excel from blob (same source),
	•	Find the last non-empty value in column A (scan upward) of that fetched file,
	•	Download the target Excel that lives on blob:
https://agenticail.blob.core.windows.net/eligibility-results/dummyeligibility.xlsx
	•	Append the reference into column A of that blob-hosted workbook one row below the last non-empty cell (scan upwards to find it),
	•	Write a timestamp in the same row in column B in this exact formatting: MM/DD/YYYY hh:mm:ss AM/PM (example: 11/19/2025 11:00:00 AM) using the Asia/Kolkata timezone,
	•	Upload the modified workbook back to the same blob path (overwrite),
	•	Keep the input/stubbed.xlsx path unchanged in the code (so you can use it later) — we do not write to local stubbed file anymore as per your instruction,
	•	We do not add duplicate-checking logic (per your request).

Below are the complete updated files. Only handler.py contains the new blob-write logic; other files are unchanged except for imports where needed. Copy-paste these into your project.

⸻

file_router.py

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import compare_name_with_sanctions

router = APIRouter()


class CompareRequest(BaseModel):
    """
    This endpoint is invoked by the Data Extraction Agent.
    It passes the FULL Azure Blob URL of the extracted JSON in 'jsonfilepath'.

    Example payload:
    {
      "jsonfilepath": "https://<account>.blob.core.windows.net/<container>/<path>/file.json"
    }
    """
    jsonfilepath: str = Field(..., description="Full Azure Blob URL to the extracted JSON.")


@router.post(
    "/Licensing_&_Sanction_Checker_MCP",
    operation_id="Licensing_&_Sanction_Checker_MCP",
    summary="Compare JSON (Owner/Insured/Contact/NAMED INSURED(S)) from blob URL with local sanctions CSV."
)
async def compare_files(request: CompareRequest):
    try:
        result = await compare_name_with_sanctions(json_file_url=request.jsonfilepath)
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": result
            },
            status_code=200
        )
    except Exception as e:
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": {"status": False, "error": f"Unexpected server error: {str(e)}"}
            },
            status_code=200
        )


⸻

handler.py

import os
import json
from typing import Optional, Dict
import pandas as pd
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError
from dotenv import load_dotenv
from io import BytesIO
import openpyxl
from datetime import datetime
from zoneinfo import ZoneInfo

from service import _require_env, _parse_blob_url

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
LOCAL_CSV_PATH = os.path.join("input", "sanctions.csv")

# Source fetched-emails excel (we read the last reference from here)
FETCHED_EMAILS_EXCEL_BLOB_URL = "https://agenticai1.blob.core.windows.net/fetched-emails-data/Fetched_emails_data_new.xlsx"

# Target blob excel where we now append the reference + timestamp
TARGET_BLOB_EXCEL_URL = "https://agenticail.blob.core.windows.net/eligibility-results/dummyeligibility.xlsx"

# Local stub path kept unchanged (not used to write now, but preserved per your instruction)
LOCAL_STUBBED_EXCEL_PATH = os.path.join("input", "stubbed.xlsx")


async def compare_name_with_sanctions(json_file_url: str) -> Dict:
    """
    Existing sanction flow + new behavior:
      - After sanction comparison, downloads fetched-emails excel and reads last ref in column A
      - Appends that reference (and timestamp in column B) into the TARGET_BLOB_EXCEL_URL workbook on blob
      - Returns combined result including excel_update status
    """
    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    # ---------- Download and parse JSON (existing logic) ----------
    try:
        container_name, blob_name = _parse_blob_url(json_file_url)
    except Exception as e:
        return {"status": False, "error": f"Invalid jsonfilepath: {e}"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_name)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"JSON blob not found: container='{container_name}', blob='{blob_name}'"}

            stream = await blob_client.download_blob()
            raw_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error connecting to Azure Blob Storage: {e}"}

    try:
        data = json.loads(raw_bytes.decode("utf-8"))
    except Exception as e:
        return {"status": False, "error": f"Failed to parse JSON: {e}"}

    # Target labels to search for dynamically
    target_labels = {"ownername", "insuredname", "contactname", "namedinsured(s)",
                     "applicant/firstnameinsured", "insured/applicant", "namedinsured",
                     "namedinsureds", "firstnameinsured", "applicantname"}
    extracted_name: Optional[str] = None

    def search_nested(data: dict) -> Optional[str]:
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, dict):
                    result = search_nested(value)
                    if result:
                        return result
                elif isinstance(value, list):
                    for elem in value:
                        if isinstance(elem, dict):
                            result = search_nested(elem)
                            if result:
                                return result
                elif isinstance(key, str):
                    normalized_key = key.lower().replace(" ", "").replace(":", "")
                    if normalized_key in target_labels:
                        val = str(value)
                        return val.split(",")[0].strip()
        return None

    extracted_name = search_nested(data)

    if not extracted_name:
        # proceed with excel update even if sanction extraction failed (per earlier behavior)
        excel_status = await _append_reference_to_target_blob_excel()
        return {"status": False, "error": "No matching name field found in JSON.", "excel_update": excel_status}

    # ---------- Read local CSV and compare ----------
    if not os.path.exists(LOCAL_CSV_PATH):
        excel_status = await _append_reference_to_target_blob_excel()
        return {"status": False, "error": f"Local CSV not found at '{LOCAL_CSV_PATH}'", "excel_update": excel_status}

    try:
        csv_df = pd.read_csv(LOCAL_CSV_PATH)
    except Exception as e:
        excel_status = await _append_reference_to_target_blob_excel()
        return {"status": False, "error": f"Failed to read CSV '{LOCAL_CSV_PATH}': {e}", "excel_update": excel_status}

    if "entity_name" not in csv_df.columns:
        excel_status = await _append_reference_to_target_blob_excel()
        return {"status": False, "error": "CSV must contain 'entity_name' column", "excel_update": excel_status}

    def norm(s: str) -> str:
        return s.strip().lower() if isinstance(s, str) else ""

    entity_set = {norm(x) for x in csv_df["entity_name"].dropna() if isinstance(x, str) and x.strip()}
    is_unique = norm(extracted_name) in entity_set

    results = [{"Extracted Name": extracted_name, "is_Sanctioned": is_unique}]

    try:
        os.makedirs("./output", exist_ok=True)
        json_output = "./output/comparison_results.json"
        with open(json_output, "w", encoding="utf-8") as f:
            json.dump({"results": results}, f, indent=4, ensure_ascii=False)
    except Exception as e:
        excel_status = await _append_reference_to_target_blob_excel()
        return {"status": False, "error": f"Failed saving results: {e}", "excel_update": excel_status}

    # ---------- NEW: append reference + timestamp to target blob excel ----------
    excel_status = await _append_reference_to_target_blob_excel()

    return {"status": True, "results": results, "excel_update": excel_status}


async def _append_reference_to_target_blob_excel() -> Dict:
    """
    1) Download fetched-emails excel (FETCHED_EMAILS_EXCEL_BLOB_URL) and read last non-empty value in col A (scan upwards).
    2) Download the target blob excel (TARGET_BLOB_EXCEL_URL), open it, find last non-empty row in col A (scan upwards),
       write the reference to next row in col A and write timestamp in col B.
    3) Upload the modified workbook back to the target blob path (overwrite).
    Returns dict with status and message.
    Note: No duplicate-checking (per your instruction).
    """

    if not AZURE_STORAGE_CONNECTION_STRING:
        return {"status": False, "error": "Missing AZURE_STORAGE_CONNECTION_STRING for Excel blob operations."}

    # --- 1) Download fetched-emails excel and extract last value in col A ---
    try:
        src_container, src_blob = _parse_blob_url(FETCHED_EMAILS_EXCEL_BLOB_URL)
    except Exception as e:
        return {"status": False, "error": f"Invalid fetched emails excel blob URL: {e}"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            src_container_client = blob_service.get_container_client(src_container)
            src_blob_client = src_container_client.get_blob_client(src_blob)
            try:
                await src_blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"Fetched emails Excel blob not found: container='{src_container}', blob='{src_blob}'"}

            src_stream = await src_blob_client.download_blob()
            src_bytes = await src_stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error downloading fetched emails Excel from blob: {e}"}

    try:
        src_bio = BytesIO(src_bytes)
        src_wb = openpyxl.load_workbook(filename=src_bio, read_only=True, data_only=True)
        src_ws = src_wb[src_wb.sheetnames[0]]
        last_value = None
        src_max_row = src_ws.max_row or 0
        for r in range(src_max_row, 0, -1):
            v = src_ws.cell(row=r, column=1).value
            if v is not None and str(v).strip() != "":
                last_value = str(v).strip()
                break
        src_wb.close()
        if last_value is None:
            return {"status": False, "error": "No non-empty values found in column A of fetched emails Excel."}
    except Exception as e:
        return {"status": False, "error": f"Failed parsing fetched emails Excel: {e}"}

    # --- 2) Download target blob excel to modify ---
    try:
        tgt_container, tgt_blob = _parse_blob_url(TARGET_BLOB_EXCEL_URL)
    except Exception as e:
        return {"status": False, "error": f"Invalid target blob excel URL: {e}"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            tgt_container_client = blob_service.get_container_client(tgt_container)
            tgt_blob_client = tgt_container_client.get_blob_client(tgt_blob)
            try:
                await tgt_blob_client.get_blob_properties()
                # Blob exists -> download
                tgt_stream = await tgt_blob_client.download_blob()
                tgt_bytes = await tgt_stream.readall()
                targ_bio = BytesIO(tgt_bytes)
                wb = openpyxl.load_workbook(filename=targ_bio)  # read/write workbook
            except ResourceNotFoundError:
                # Blob doesn't exist -> create a new workbook
                wb = openpyxl.Workbook()
            except Exception as e:
                return {"status": False, "error": f"Error accessing target blob excel: {e}"}
            # now wb is the workbook to modify
            ws = wb[wb.sheetnames[0]]
            # Find last non-empty row in column A by scanning upwards
            last_non_empty_local = 0
            local_max = ws.max_row or 0
            for r in range(local_max, 0, -1):
                val = ws.cell(row=r, column=1).value
                if val is not None and str(val).strip() != "":
                    last_non_empty_local = r
                    break
            # compute write row following the "append after last non-empty" rule
            if last_non_empty_local >= 1:
                write_row = last_non_empty_local + 1
            else:
                # no values present -> write to row 2 (reserve A1 for header if you manually put it)
                write_row = 2

            # Write the reference into column A
            ws.cell(row=write_row, column=1, value=last_value)

            # Create timestamp in Asia/Kolkata timezone with format "MM/DD/YYYY hh:mm:ss AM/PM"
            try:
                tz = ZoneInfo("Asia/Kolkata")
            except Exception:
                tz = None
            now = datetime.now(tz) if tz is not None else datetime.now()
            timestamp_str = now.strftime("%m/%d/%Y %I:%M:%S %p")
            ws.cell(row=write_row, column=2, value=timestamp_str)

            # Save workbook to bytes and upload back to blob (overwrite)
            out_bio = BytesIO()
            wb.save(out_bio)
            out_bio.seek(0)
            # upload
            try:
                await tgt_blob_client.upload_blob(out_bio.getvalue(), overwrite=True)
            except Exception as e:
                return {"status": False, "error": f"Failed uploading modified target excel back to blob: {e}"}

            # close workbook
            wb.close()
            return {"status": True, "message": f"Wrote reference '{last_value}' to target blob excel at row {write_row} and timestamp '{timestamp_str}' in column B."}
    except Exception as e:
        return {"status": False, "error": f"General error during target blob excel handling: {e}"}


⸻

main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn

from file_router import router as file_router


def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )


def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    sub = FastAPI(title=title, description=description, version=version)
    apply_cors(sub)
    return sub


app = FastAPI()
apply_cors(app)

file_app = create_sub_app(
    title="Licensing & Sanction Checker MCP",
    description="Receives JSON blob URL from Data Extraction Agent and compares extracted name with local sanctions CSV."
)
file_app.include_router(file_router)

# Expose ONLY this operation id via MCP HTTP (client will use transport='http')
FastApiMCP(file_app, include_operations=["Licensing_&_Sanction_Checker_MCP"]).mount_http()

# Mount the sub-app under this prefix (keep as-is per your request)
app.mount("/api/v1/eligibility_agent", file_app)


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8601)


⸻

service.py

import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")


def _require_env() -> None:
    """Ensure required environment variables exist."""
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING in .env")


def _parse_blob_url(url: str) -> tuple[str, str]:
    """
    Parse an Azure Blob URL and return (container_name, blob_path).
    Example URL:
      https://account.blob.core.windows.net/output-results/folder/name.json
    Returns:
      ("output-results", "folder/name.json")
    """
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("jsonfilepath must be a valid http(s) URL to Azure Blob Storage.")

    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("jsonfilepath must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)


⸻

test_scripts.py

import requests
import json

BASE_URL = "http://localhost:8601/api/v1/eligibility_agent/Licensing_&_Sanction_Checker_MCP"

def run_test():
    payload = {
          "jsonfilepath": "https://agenticai1.blob.core.windows.net/output-results/AASDEWQ213SF_attachment_Acord_125_extraction_20251149_1226666.json"
    }
    try:
        resp = requests.post(BASE_URL, json=payload, timeout=180)
        print("HTTP Status:", resp.status_code)
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except json.JSONDecodeError:
            print("Response not JSON:", resp.text)
    except Exception as e:
        print("Error calling API:", str(e))

if __name__ == "__main__":
    print("Testing /Licensing_&_Sanction_Checker_MCP ...")
    run_test()


⸻

Quick notes & assumptions
	1.	No duplicate-checking — as you asked, the code will always append the last value from fetched-emails Excel into the target blob excel (i.e., duplicates possible).
	2.	Timestamp timezone & format — uses Asia/Kolkata timezone and the requested format "%m/%d/%Y %I:%M:%S %p". If your environment lacks zoneinfo (older Python), let me know and I’ll switch to pytz.
	3.	Blob overwrite — the target excel is overwritten on blob after modification (upload with overwrite=True).
	4.	Local stubbed file — LOCAL_STUBBED_EXCEL_PATH remains in code unchanged for future use but is not written to in this flow.
	5.	Permissions — ensure the AZURE_STORAGE_CONNECTION_STRING in your .env has write permission to the eligibility-results container.
	6.	Python packages — ensure you have:

pip install openpyxl pandas azure-storage-blob python-dotenv


	7.	Edge cases — if the fetched-emails file has no non-empty column A value, the function returns an error and does not change the target blob.

⸻

If you want any of the following next, tell me and I’ll update quickly:
	•	Also append the timestamp in a third column containing ISO8601 for easier programmatic use.
	•	Keep a separate log file ./output/excel_append_log.csv with appended rows and timestamps.
	•	Make the target blob URL and/or fetched-excel URL configurable via .env.

Would you like the target blob URL moved to .env now (recommended for production)?
