Great â€” I refactored Code 1 into the Code 2-style structure and kept the original behavior exactly: Gmail polling + webhook processing + attachment-vs-Azure-blob comparison. Drop these files into a directory named EmailAgent/ (subfolders as shown) and run python main.py â€” everything will start, and the polling will run as a FastAPI app with MCP-style structure.

Below are all files (full code). I kept function names and logic close to your originals so behavior stays identical.

â¸»

Project structure

EmailAgent/
â”œâ”€ main.py
â”œâ”€ .env.example
â”œâ”€ requirements.txt
â”œâ”€ history_state.json        # (optionally pre-create; code will create if missing)
â”œâ”€ storage_output/           # output folder (created automatically)
â”œâ”€ credentials/              # Put your Google OAuth client_secret.json here (same as original)
â”œâ”€ routers/
â”‚  â”œâ”€ gmail_router.py
â”‚  â””â”€ attachment_router.py
â”œâ”€ services/
â”‚  â”œâ”€ gmail_service.py
â”‚  â””â”€ attachment_service.py
â””â”€ utils/
   â””â”€ state_manager.py


â¸»

1) main.py

# main.py
import os
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn
from routers import gmail_router, attachment_router
from dotenv import load_dotenv

# Load .env (if present)
load_dotenv()

def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )

# Main app
app = FastAPI(title="EmailAgent Main App")
apply_cors(app)

# Sub-app for Email handling (structured like Code 2 style)
email_app = FastAPI(title="Email Agent", description="Gmail watcher + attachment comparator")
apply_cors(email_app)

# Include routers
email_app.include_router(gmail_router.router, prefix="", tags=["gmail"])
email_app.include_router(attachment_router.router, prefix="", tags=["attachments"])

# Mount FastApiMCP similar to Code 2 usage
# Include operation ids from routers automatically
FastApiMCP(email_app, include_operations=["process_notification", "create_watch"]).mount_http()

# Mount the sub-app at /api/v1/email
app.mount("/api/v1/email", email_app)

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 8054))
    uvicorn.run(app, host="0.0.0.0", port=port)


â¸»

2) routers/gmail_router.py

# routers/gmail_router.py
from fastapi import APIRouter, BackgroundTasks
from pydantic import BaseModel
import asyncio
import os
from services.gmail_service import get_gmail_service
from services.attachment_service import save_attachments_from_message
from utils.state_manager import load_state, save_state
from datetime import datetime
from dotenv import load_dotenv

load_dotenv()

router = APIRouter()

# Global control flags similar to your original mcp_enail_server.py
running = True
last_check_time = None

class EmailNotification(BaseModel):
    emailAddress: str
    historyId: int

async def check_new_emails():
    """Check for new emails and process them (same logic as original)."""
    global last_check_time
    service = get_gmail_service()

    try:
        if not last_check_time:
            results = service.users().messages().list(userId="me", maxResults=5).execute()
        else:
            query = f"after:{int(last_check_time.timestamp())}"
            results = service.users().messages().list(userId="me", q=query).execute()
        messages = results.get("messages", [])
        for message in messages:
            msg = service.users().messages().get(userId="me", id=message["id"], format="full").execute()
            print(f"ðŸ“© Processing message ID: {message['id']}")
            comparison_result = save_attachments_from_message(service, msg)
            print(f"ðŸ“Ž Comparison result: {comparison_result}")
        last_check_time = datetime.now()
    except Exception as e:
        print(f"Error checking emails: {e}")

async def email_polling_loop():
    """Continuously poll for new emails."""
    global running
    while running:
        try:
            await check_new_emails()
        except Exception as e:
            print("Polling iteration error:", e)
        await asyncio.sleep(int(os.environ.get("POLL_INTERVAL_SECONDS", 10)))

@router.on_event("startup")
async def startup_event():
    """Start polling in background on startup."""
    global running
    try:
        print("ðŸ“§ Starting email polling (background task)...")
        # fire-and-forget
        asyncio.create_task(email_polling_loop())
    except Exception as e:
        print("Startup non-fatal error:", e)

@router.on_event("shutdown")
async def shutdown_event():
    global running
    running = False
    print("ðŸ“§ Shutting down email polling...")

@router.post("/create_watch", summary="Create Gmail watch (same usage as original script)")
async def create_watch_endpoint(project_id: str, topic_full_name: str):
    """Create a Gmail watch for Pub/Sub notifications via simple HTTP call."""
    try:
        service = get_gmail_service()
        body = {
            "labelIds": ["INBOX"],
            "topicName": topic_full_name
        }
        resp = service.users().watch(userId="me", body=body).execute()
        print("Watch created:", resp)
        return {"watch_response": resp}
    except Exception as e:
        return {"error": str(e)}

@router.post("/process_notification", summary="Process Gmail push notification")
async def process_notification(notification: EmailNotification, background_tasks: BackgroundTasks):
    """
    Process a Gmail notification and compare attachment filenames with Azure Blob Storage.
    Keeps same behavior as original mcp_enail_server.process_notification
    """
    service = get_gmail_service()
    email = notification.emailAddress
    history_id = notification.historyId

    state = load_state()
    last_hist = state.get(email)

    if not last_hist:
        state[email] = history_id
        save_state(state)
        return {"message": f"Initialized history id {history_id}"}

    processed_messages = []
    try:
        resp = service.users().history().list(
            userId="me",
            startHistoryId=str(last_hist),
            historyTypes="messageAdded"
        ).execute()
        histories = resp.get("history", [])
        message_ids = []
        for h in histories:
            for ma in h.get("messagesAdded", []):
                message_ids.append(ma["message"]["id"])
    except Exception as e:
        print("History list failed:", e)
        # Fallback: get recent messages (same as original)
        res = service.users().messages().list(userId="me", q="newer_than:7d", maxResults=20).execute()
        message_ids = [m["id"] for m in res.get("messages", [])]

    for mid in message_ids:
        msg = service.users().messages().get(userId="me", id=mid, format="full").execute()
        comparison_result = save_attachments_from_message(service, msg)
        processed_messages.append({
            "message_id": mid,
            "comparison_result": comparison_result
        })

    state[email] = history_id
    save_state(state)

    return {
        "message": f"Processed {len(processed_messages)} messages",
        "processed": processed_messages
    }


â¸»

3) routers/attachment_router.py

# routers/attachment_router.py
from fastapi import APIRouter
from pydantic import BaseModel
from services.attachment_service import get_blob_names_from_container, save_attachments_from_message
from services.gmail_service import get_gmail_service

router = APIRouter()

class AttachmentCheckRequest(BaseModel):
    message_id: str

@router.get("/list_blobs", summary="List blob names in Azure container")
async def list_blobs():
    """Return all blob names from the configured container."""
    try:
        blobs = get_blob_names_from_container()
        return {"blobs": blobs}
    except Exception as e:
        return {"error": str(e)}

@router.post("/check_message_attachments", summary="Check attachments for a given message id")
async def check_message_attachments(req: AttachmentCheckRequest):
    """
    Downloads message and compares attachments with blob storage.
    This is useful for manual trigger/tests.
    """
    try:
        service = get_gmail_service()
        msg = service.users().messages().get(userId="me", id=req.message_id, format="full").execute()
        result = save_attachments_from_message(service, msg)
        return {"result": result}
    except Exception as e:
        return {"error": str(e)}


â¸»

4) services/gmail_service.py

# services/gmail_service.py
import os
import pickle
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build

# Scopes (same as original)
SCOPES = [
    'https://www.googleapis.com/auth/gmail.readonly'
]

TOKEN_PATH = "token.pickle"
CLIENT_SECRET_PATH = os.path.join("credentials", "client_secret.json")

def get_gmail_service():
    """Authenticate with Gmail API and return the service object."""
    creds = None
    token_path = TOKEN_PATH

    if os.path.exists(token_path):
        with open(token_path, "rb") as f:
            creds = pickle.load(f)

    if not creds or not creds.valid:
        if not os.path.exists(CLIENT_SECRET_PATH):
            raise RuntimeError(f"Missing Gmail client secret at {CLIENT_SECRET_PATH}")
        flow = InstalledAppFlow.from_client_secrets_file(CLIENT_SECRET_PATH, SCOPES)
        creds = flow.run_local_server(port=0)
        with open(token_path, "wb") as f:
            pickle.dump(creds, f)

    service = build("gmail", "v1", credentials=creds, cache_discovery=False)
    return service


â¸»

5) services/attachment_service.py

# services/attachment_service.py
import os
import json
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv

load_dotenv()  # Load environment variables if present

STATE_FILE = os.environ.get("STATE_FILE", "history_state.json")
OUTPUT_DIR = os.environ.get("OUTPUT_DIR", "storage_output")
os.makedirs(OUTPUT_DIR, exist_ok=True)

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
AZURE_BLOB_CONTAINER = os.environ.get("AZURE_BLOB_CONTAINER", "")

def _get_blob_service_client():
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING")
    return BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

def _ensure_container_exists(blob_service_client):
    if not AZURE_BLOB_CONTAINER:
        raise RuntimeError("Missing AZURE_BLOB_CONTAINER")
    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER)
    try:
        container_client.create_container()
    except Exception:
        pass
    return container_client

def get_blob_names_from_container():
    """Retrieve all blob names from the Azure Blob Storage container."""
    blob_names = []
    try:
        blob_service_client = _get_blob_service_client()
        container_client = _ensure_container_exists(blob_service_client)
        blobs = container_client.list_blobs()
        blob_names = [blob.name for blob in blobs]
    except Exception as e:
        print(f"Error retrieving blob names: {e}")
    return blob_names

def load_state():
    try:
        with open(STATE_FILE, "r") as f:
            return json.load(f)
    except Exception:
        return {}

def save_state(state):
    with open(STATE_FILE, "w") as f:
        json.dump(state, f, indent=2)

def _walk_parts(parts):
    for p in parts or []:
        yield p
        for sub in p.get("parts", []) or []:
            yield from _walk_parts([sub])

def save_attachments_from_message(service, message):
    """
    Extract attachment filenames from Gmail message and compare with Azure Blob Storage.

    Returns a dictionary with attachment names and comparison results, including a "is_duplicate" flag.
    """
    saved_attachments = []
    parts = message.get("payload", {}).get("parts", [])

    for part in _walk_parts(parts):
        filename = part.get("filename")
        if filename:
            saved_attachments.append(filename)

    blob_service_client = _get_blob_service_client()
    container_client = _ensure_container_exists(blob_service_client)

    attachment_results = []

    for filename in saved_attachments:
        is_duplicate = False
        try:
            blob_client = container_client.get_blob_client(blob=filename)
            if blob_client.exists():
                is_duplicate = True
        except Exception as e:
            print(f"Error checking blob existence for {filename}: {e}")

        attachment_results.append({
            "filename": filename,
            "is_duplicate": is_duplicate
        })

    comparison_result = {
        "attachments": attachment_results
    }

    try:
        out_path = os.path.join(OUTPUT_DIR, f"{message.get('id')}_comparison.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(comparison_result, f, indent=2)
    except Exception as exc:
        print(f"Failed to write comparison result file: {exc}")

    return comparison_result


â¸»

6) utils/state_manager.py

# utils/state_manager.py
import json
import os
from dotenv import load_dotenv

load_dotenv()

STATE_FILE = os.environ.get("STATE_FILE", "history_state.json")

def load_state():
    try:
        with open(STATE_FILE, "r") as f:
            return json.load(f)
    except Exception:
        return {}

def save_state(state):
    with open(STATE_FILE, "w") as f:
        json.dump(state, f, indent=2)


â¸»

7) .env.example

# .env.example - copy to .env and fill values
AZURE_STORAGE_CONNECTION_STRING=
AZURE_BLOB_CONTAINER=
STATE_FILE=history_state.json
OUTPUT_DIR=storage_output
POLL_INTERVAL_SECONDS=10
PORT=8054


â¸»

8) requirements.txt

fastapi>=0.95.0
uvicorn>=0.22.0
google-auth-oauthlib>=1.0.0
google-api-python-client>=2.70.0
azure-storage-blob>=12.14.1
python-dotenv>=1.0.0
fastapi-mcp>=0.1.0
pydantic>=1.10.0


â¸»

How to run
	1.	Place all files exactly as shown (create folders routers, services, utils, credentials).
	2.	Put your Google OAuth client_secret.json inside credentials/client_secret.json.
	3.	Copy .env.example â†’ .env and fill:
	â€¢	AZURE_STORAGE_CONNECTION_STRING
	â€¢	AZURE_BLOB_CONTAINER
	â€¢	(optional) POLL_INTERVAL_SECONDS or use default 10
	4.	Install packages:

pip install -r requirements.txt

	5.	Start the app:

python main.py

	â€¢	The FastAPI app will be available (by default) at http://0.0.0.0:8054/. The Gmail polling background task will start automatically (same behavior as your original mcp_enail_server.py startup event).
	â€¢	Use /api/v1/email/process_notification to POST Gmail notifications (same signature as original) or /api/v1/email/create_watch to create watch programmatically.

â¸»

Notes / design decisions I followed
	â€¢	Kept the Gmail authentication flow unchanged (still creates token.pickle on first run).
	â€¢	Kept attachment-comparison logic identical (still writes comparison JSON to storage_output/).
	â€¢	Polling interval can be controlled with POLL_INTERVAL_SECONDS env var.
	â€¢	FastApiMCP is mounted in main.py same as Code 2 style.
	â€¢	I split code into routers, services, and utils to match Code 2 modularity while preserving original flow.

â¸»

If you want, I can:
	â€¢	Add an OpenAPI-friendly operation_id values for FastApiMCP mapping (so MCP shows nicer names).
	â€¢	Add unit-test stubs or a minimal example Gmail message for local testing without Gmail.
	â€¢	Convert get_gmail_service() to support service-account or non-interactive flows.

Would you like me to add operation IDs & include_operations for nicer MCP picks now?
