Got it — you want the code to always use the container from the full blob URL when a full URL is provided (i.e., pull the input container directly from the URL), instead of relying on AZURE_BLOB_CONTAINER_INPUT in .env. If a plain filename (not a full URL) is given, the code will still fall back to the AZURE_BLOB_CONTAINER_INPUT value in .env (so we keep backward compatibility).

Below is a complete, consistent set of files for your project. Drop these into your project (overwrite the corresponding files). The behavior:
	•	If file_path is a full blob URL → we parse container and blob path from the URL and download from that container.
	•	If file_path is a plain filename (no scheme/netloc) → we use AZURE_BLOB_CONTAINER_INPUT from .env.
	•	If you don’t have AZURE_STORAGE_CONNECTION_STRING but the URL has a SAS token, the code will attempt HTTP download via requests (using the full URL including SAS).
	•	Downloaded files are stored in ./input/ and read_json_blob() reads local files first.

⸻

File: main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from router import router

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(router, prefix="/api/v1/hazard")

if __name__ == "__main__":
    # dev server
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)


⸻

File: router.py

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import process_blob_file
from service import get_coordinates_from_openweather, check_earthquake_risk

router = APIRouter()

class BlobRequest(BaseModel):
    # Accepts either a full blob URL or a plain filename
    file_path: str = Field(..., description="Full blob URL or plain filename for the JSON file.")

providers = {
    "geocode": get_coordinates_from_openweather,
    "earthquake": check_earthquake_risk,
    "geocoder_name": "OpenWeather"
}

@router.post("/get-coordinates", operation_id="hazard_profile_checker")
async def get_coordinates_from_blob(p_body: BlobRequest):
    result = await process_blob_file(p_body.file_path, providers)
    return JSONResponse(content=result, status_code=200)


⸻

File: handler.py

# handler.py
from typing import Dict, Any
from service import download_blob_to_local, read_json_blob, extract_address

async def process_blob_file(blob_url_or_name: str, providers: Dict[str, Any]) -> dict:
    """
    Main handler:
    - If blob_url_or_name is a full URL: use container from URL and download from there.
    - If plain filename: use AZURE_BLOB_CONTAINER_INPUT from env.
    Downloads to ./input/ and reads the JSON locally.
    """
    try:
        # download_blob_to_local returns the local file path where the JSON is saved
        local_path = download_blob_to_local(blob_url_or_name, local_dir="input")

        json_data = read_json_blob(local_path)
        fields = json_data.get("extracted_fields", [])
        address_value = None

        for field in fields:
            if "MAILING ADDRESS" in field.get("Field", ""):
                address_value = field.get("Value")
                break

        if not address_value:
            return {"error": "No mailing address found in extracted_fields."}

        address = extract_address(address_value)
        coordinates = providers["geocode"](address)

        if not isinstance(coordinates, dict) or "latitude" not in coordinates or "longitude" not in coordinates:
            return {
                "coordinates": coordinates,
                "earthquake_prone": "Unknown (geocoding failed)",
                "construction_type": "Unknown",
                "distance_to_fire_hydrant": "Unknown",
                "distance_to_fire_station": "Unknown",
                "year_built": "Unknown"
            }

        latitude = coordinates["latitude"]
        longitude = coordinates["longitude"]

        earthquake_risk = providers["earthquake"](latitude, longitude)

        return {
            "coordinates": coordinates,
            "earthquake_count": earthquake_risk,
            "construction_type": "Cement",
            "distance_to_fire_hydrant": "20 FT",
            "distance_to_fire_station": "3 MI",
            "year_built": 2018
        }

    except Exception as e:
        return {"error": f"Failed to process blob file: {str(e)}"}


⸻

File: service.py

# service.py
import json
import os
from urllib.parse import urlparse
import requests
from dotenv import load_dotenv
from azure.storage.blob import BlobServiceClient
import re
from datetime import datetime, timedelta
from typing import Tuple, Optional

load_dotenv()

# Keep .env as you have it — we read what's present.
AZURE_STORAGE_CONNECTION_STRING = os.getenv("AZURE_STORAGE_CONNECTION_STRING", "")
# We'll only use AZURE_BLOB_CONTAINER_INPUT if the user passes a plain filename.
AZURE_BLOB_CONTAINER_INPUT = os.getenv("AZURE_BLOB_CONTAINER_INPUT", "")
AZURE_BLOB_CONTAINER_OUTPUT = os.getenv("AZURE_BLOB_CONTAINER_OUTPUT", "")

OPENWEATHER_API_KEY = os.getenv("OPENWEATHER_API_KEY")
USGS_API_URL = os.getenv("USGS_EARTHQUAKE_API")

def _get_blob_service_client() -> Optional[BlobServiceClient]:
    """
    Returns a BlobServiceClient if a connection string is present, else None.
    """
    if not AZURE_STORAGE_CONNECTION_STRING:
        return None
    return BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

def extract_container_and_blob_path_from_url(url: str) -> Tuple[str, str]:
    """
    Parse a full blob URL and return (container_name, blob_path).
    e.g.
    https://account.blob.core.windows.net/container/folder/file.json?sv=...
    returns ("container", "folder/file.json")
    """
    parsed = urlparse(url)
    if not parsed.scheme or not parsed.netloc:
        raise ValueError("Not a full URL")
    path = parsed.path.lstrip("/")
    if not path:
        raise ValueError("URL missing container/blob path")
    parts = path.split("/", 1)
    if len(parts) == 1:
        container_name = parts[0]
        blob_path = ""
    else:
        container_name, blob_path = parts[0], parts[1]
    return container_name, blob_path

def _local_file_name_from_blob_path(blob_path: str) -> str:
    return os.path.basename(blob_path) or f"downloaded_{int(datetime.utcnow().timestamp())}.json"

def download_blob_to_local(blob_url_or_name: str, local_dir: str = "input") -> str:
    """
    Downloads either:
      - full blob URL -> uses container parsed from URL
      - or plain filename -> uses AZURE_BLOB_CONTAINER_INPUT
    Returns the local file path.
    """
    os.makedirs(local_dir, exist_ok=True)

    # Test whether the input is a full URL
    try:
        parsed = urlparse(blob_url_or_name)
        is_full_url = bool(parsed.scheme and parsed.netloc)
    except Exception:
        is_full_url = False

    blob_service_client = _get_blob_service_client()

    if is_full_url:
        # extract container and blob path from URL and download from that container
        container_name, blob_path = extract_container_and_blob_path_from_url(blob_url_or_name)
        if not blob_path:
            raise ValueError("Blob URL does not contain a blob path")

        local_name = _local_file_name_from_blob_path(blob_path)
        local_path = os.path.join(local_dir, local_name)

        # Priority 1: use SDK if connection string available
        if blob_service_client:
            container_client = blob_service_client.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob=blob_path)
            if not blob_client.exists():
                raise FileNotFoundError(f"Blob '{blob_path}' not found in container '{container_name}'")
            blob_bytes = blob_client.download_blob().readall()
            with open(local_path, "wb") as fh:
                fh.write(blob_bytes)
            return local_path

        # Priority 2: If no connection string, try direct HTTP download (works if URL has SAS token)
        if parsed.query:
            resp = requests.get(blob_url_or_name, stream=True)
            if resp.status_code != 200:
                raise RuntimeError(f"Failed HTTP download of blob URL (status {resp.status_code})")
            with open(local_path, "wb") as fh:
                for chunk in resp.iter_content(chunk_size=8192):
                    if chunk:
                        fh.write(chunk)
            return local_path

        # If we reach here, we cannot download
        raise RuntimeError("No AZURE_STORAGE_CONNECTION_STRING and URL has no SAS query; cannot download blob")

    else:
        # Plain filename — use AZURE_BLOB_CONTAINER_INPUT from .env
        if not AZURE_BLOB_CONTAINER_INPUT:
            raise RuntimeError("Input is a filename but AZURE_BLOB_CONTAINER_INPUT is not set in environment")
        if not blob_service_client:
            raise RuntimeError("AZURE_STORAGE_CONNECTION_STRING must be set to download by filename from container")

        container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER_INPUT)
        blob_client = container_client.get_blob_client(blob=blob_url_or_name)
        if not blob_client.exists():
            raise FileNotFoundError(f"Blob '{blob_url_or_name}' not found in container '{AZURE_BLOB_CONTAINER_INPUT}'")
        blob_bytes = blob_client.download_blob().readall()
        local_path = os.path.join(local_dir, os.path.basename(blob_url_or_name))
        with open(local_path, "wb") as fh:
            fh.write(blob_bytes)
        return local_path

def read_json_blob(filename_or_localpath: str) -> dict:
    """
    Read JSON from a local path (if exists) or from AZURE_BLOB_CONTAINER_INPUT (if given a filename).
    """
    # If it's already a local path (exists), return it
    if os.path.exists(filename_or_localpath):
        with open(filename_or_localpath, "r", encoding="utf-8") as fh:
            return json.load(fh)

    # Fallback: treat as blob name in AZURE_BLOB_CONTAINER_INPUT
    if not AZURE_BLOB_CONTAINER_INPUT:
        raise RuntimeError("File not found locally and AZURE_BLOB_CONTAINER_INPUT not configured")

    blob_service_client = _get_blob_service_client()
    if not blob_service_client:
        raise RuntimeError("AZURE_STORAGE_CONNECTION_STRING must be set to read blob by name from container")

    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER_INPUT)
    blob_client = container_client.get_blob_client(blob=filename_or_localpath)
    if not blob_client.exists():
        raise FileNotFoundError(f"Blob '{filename_or_localpath}' not found in container '{AZURE_BLOB_CONTAINER_INPUT}'")
    blob_data = blob_client.download_blob().readall()
    return json.loads(blob_data.decode("utf-8"))

def write_json_to_blob(filename: str, data: dict):
    """
    Upload JSON to AZURE_BLOB_CONTAINER_OUTPUT (if configured).
    """
    if not AZURE_BLOB_CONTAINER_OUTPUT:
        raise RuntimeError("AZURE_BLOB_CONTAINER_OUTPUT not configured")
    blob_service_client = _get_blob_service_client()
    if not blob_service_client:
        raise RuntimeError("AZURE_STORAGE_CONNECTION_STRING must be set to write to blob storage")
    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER_OUTPUT)
    try:
        container_client.create_container()
    except Exception:
        pass
    blob_client = container_client.get_blob_client(blob=filename)
    json_bytes = json.dumps(data, indent=4).encode("utf-8")
    blob_client.upload_blob(json_bytes, overwrite=True)

# Address extraction from the value (keeps your previous logic)
def extract_address(value: str) -> str:
    pattern = r"\b(?:Street|Road|Lane|Avenue|Boulevard|Drive|Way|Circle|Court|Place|Terrace|Ln|St)\b,?\s*(.*)"
    match = re.search(pattern, value, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return ""

# OpenWeather geocoding (unchanged)
def get_coordinates_from_openweather(address: str) -> dict:
    if not OPENWEATHER_API_KEY:
        return {"error": "OPENWEATHER_API_KEY not set in environment"}
    url = "http://api.openweathermap.org/geo/1.0/direct"
    params = {
        "q": address,
        "limit": 1,
        "appid": OPENWEATHER_API_KEY
    }
    response = requests.get(url, params=params)
    if response.status_code == 404:
        return {"error": f"Location not found for '{address}'."}
    response.raise_for_status()
    data = response.json()
    if not data:
        return {"error": f"No location found for '{address}'."}
    location = data[0]
    return {"latitude": location.get("lat"), "longitude": location.get("lon")}

# USGS earthquake check (unchanged)
def check_earthquake_risk(lat: float, lon: float, radius_km: int = 100, years: int = 5) -> int:
    if not USGS_API_URL:
        return 0
    end = datetime.utcnow()
    start = end - timedelta(days=365 * years)
    params = {
        "format": "geojson",
        "latitude": lat,
        "longitude": lon,
        "maxradiuskm": radius_km,
        "starttime": start.strftime("%Y-%m-%d"),
        "endtime": end.strftime("%Y-%m-%d")
    }
    response = requests.get(USGS_API_URL, params=params)
    response.raise_for_status()
    data = response.json()
    # USGS may have count at data["metadata"]["count"] or data["count"]
    return data.get("metadata", {}).get("count", data.get("count", 0))


⸻

File: test.py

# test.py
import requests

# Replace with your actual blob URL; container will be extracted from this URL
blob_url = "https://yourstorage.blob.core.windows.net/output-results/20251027_101657_8b635bd693b247899be4f0295a02807c_extracted_20251027_101657.json"

endpoint = "http://localhost:8000/api/v1/hazard/get-coordinates"

payload = {"file_path": blob_url}

resp = requests.post(endpoint, json=payload)
print("Status Code:", resp.status_code)
print("Response JSON:", resp.json())


⸻

File: requirements.txt

fastapi
uvicorn[standard]
python-dotenv
azure-storage-blob
requests
pydantic


⸻

Example .env (leave as you currently have; but at minimum you must provide connection string)

You said not to change .env. If you currently have AZURE_STORAGE_CONNECTION_STRING in .env, keep it. If not, add it.

Minimal required if you want SDK downloads:

AZURE_STORAGE_CONNECTION_STRING="DefaultEndpointsProtocol=...;AccountName=...;AccountKey=...;EndpointSuffix=core.windows.net"

Optional (if used):

AZURE_BLOB_CONTAINER_INPUT=your-default-input-container   # used only when a plain filename is passed
AZURE_BLOB_CONTAINER_OUTPUT=your-output-container
OPENWEATHER_API_KEY=...
USGS_EARTHQUAKE_API=https://earthquake.usgs.gov/fdsnws/event/1/query


⸻

Quick notes & gotchas (short)
	•	This update explicitly uses the container parsed from the blob URL when a full URL is passed — that satisfies your request to “take the INPUT from here”.
	•	If the URL contains a SAS token (query string) and you do not have AZURE_STORAGE_CONNECTION_STRING, the code will attempt HTTP download using requests and the full URL (SAS included).
	•	If you want to ignore AZURE_BLOB_CONTAINER_INPUT completely from .env going forward, simply remove it — the code will still work for full URLs. But keep AZURE_STORAGE_CONNECTION_STRING if you want SDK-based downloads.
	•	Downloaded JSON is saved to ./input/<filename> (same filename as blob). Existing files are overwritten.

⸻

If you want now:
	•	a unified git diff / patch file to apply these changes, OR
	•	a variant that always downloads via HTTP requests using SAS URLs (no SDK at all), OR
	•	a variant that streams large blobs (so we don’t call .readall()),

tell me which and I’ll paste it immediately.
