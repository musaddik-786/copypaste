Code 1

test_main.py
 
 
from fastapi import FastAPI
 
from fastapi.middleware.cors import CORSMiddleware
 
from fastapi_mcp import FastApiMCP
 
import uvicorn
 
from test_email_router import router as email_router
 
from email_handler import get_latest_email, start_email_polling, stop_email_polling
 
from contextlib import asynccontextmanager
 
@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await start_email_polling()
    yield
    # Shutdown
    await stop_email_polling()
app = FastAPI(lifespan=lifespan)
 
def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )
 
def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    app = FastAPI(title=title, description=description, version=version, lifespan=lifespan)
    apply_cors(app)
    return app
 
apply_cors(app)
 
email_reader_app = create_sub_app(title="email_reader_mcp", description="Reads the email from inbox of Underwriter and return the email body")
email_reader_app.include_router(email_router)
FastApiMCP(email_reader_app, include_operations=["email_reader_mcp"]).mount_http()
app.mount("/mcp",email_reader_app)
 
if __name__=="__main__":
    uvicorn.run(app, host="0.0.0.0", port=8502)
 
 
test_email_router.py
 
 
from dotenv import load_dotenv
load_dotenv()
 
from fastapi import APIRouter
import asyncio
from email_handler import get_latest_email, start_email_polling, stop_email_polling
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
 
from contextlib import asynccontextmanager
 
router = APIRouter()
 
 
 
 
 
 
class EmailReaderMCP(BaseModel):
    """Represents the functionality of reading emails from the inbox of Underwriter"""
    AgentName: str = Field(default="EmailIntentAgent", description=("The unique agent name of the agent that is being called"))
    UserId: str = Field(default="markRuffalo", description=("The unique user id of a specific user (default: 'markRuffalo')."))
 
@router.post("/email_reader_mcp", operation_id="email_reader_mcp")
async def email_reader_mcp(p_body: EmailReaderMCP):
    """Reads the email from inbox of Underwriter and return the email body
    Args:
 
        p_body (EmailReaderMCP): Request body containing:
 
            AgentName (str): The unique agent name of the agent that is being called.
 
            UserId (str): The unique user id of a specific user (default: 'markRuffalo').
 
    """
    try:
        result = get_latest_email()
    except Exception as e:
        return {"error": f"Unable to get the latest email: {e}"}
 
    return JSONResponse(content={
        "jsonrpc": "2.0",
        "id": 1,
        "result": result
    })
 
email_handler.py
 
import json
import base64
import asyncio
from datetime import datetime
from gmail_watch import get_gmail_service
 
# Global variable to track the last check time
last_check_time = None
is_polling = True
 
def extract_email_body(service, message):
    """Extract email body from the message"""
    try:
        if "payload" not in message:
            return {
                "is_body_extracted": False,
                "content": "",
                "error": "No payload in message"
            }
 
        def decode_body(data):
            """Helper function to decode base64 data"""
            try:
                # Fix padding if needed
                pad = 4 - (len(data) % 4)
                if pad != 4:
                    data += '=' * pad
                decoded = base64.urlsafe_b64decode(data.encode('ASCII')).decode('utf-8')
                # Clean the content
                cleaned = decoded.strip()
                # Remove empty HTML tags
                cleaned = cleaned.replace('<div dir="auto"></div>', '')
                cleaned = cleaned.replace('<div></div>', '')
                return cleaned if cleaned else None
            except Exception as e:
                return None
 
        def has_attachments(payload):
            """Check if message has attachments"""
            if not payload:
                return False
               
            def check_part_for_attachment(part):
                if part.get("filename"):
                    return True
                if part.get("parts"):
                    for subpart in part["parts"]:
                        if check_part_for_attachment(subpart):
                            return True
                return False
                   
            # Check all parts recursively
            for part in payload.get("parts", []):
                if check_part_for_attachment(part):
                    return True
            return False
 
        def extract_from_parts(parts):
            """Recursively extract body from message parts"""
            if not parts:
                return None
           
            # First try to find text/plain parts
            for part in parts:
                if part.get("mimeType") == "text/plain":
                    if part.get("body", {}).get("data"):
                        content = decode_body(part["body"]["data"])
                        if content and content.strip():
                            return content
           
            # If no text/plain, try text/html
            for part in parts:
                if part.get("mimeType") == "text/html":
                    if part.get("body", {}).get("data"):
                        content = decode_body(part["body"]["data"])
                        if content and content.strip():
                            return content
               
                # Check nested parts
                if part.get("parts"):
                    nested_content = extract_from_parts(part["parts"])
                    if nested_content and nested_content.strip():
                        return nested_content
            return None
           
        payload = message["payload"]
        content = None
        has_attach = has_attachments(payload)
       
        # Try to get body directly from payload first
        if payload.get("body", {}).get("data"):
            content = decode_body(payload["body"]["data"])
       
        # If no content in direct payload, try parts
        if not content:
            content = extract_from_parts(payload.get("parts", []))
           
        # Clean up any remaining content and check if it's really empty
        if content:
            content = content.strip()
           
        if content and len(content) > 0:
            return {
                "is_body_extracted": True,
                "content": content,
                "error": None
            }
 
        # Handle empty email cases
        if has_attach:
            return {
                "is_body_extracted": False,
                "content": "",
                "error": "Email contains attachments but no message body"
            }
        else:
            return {
                "is_body_extracted": False,
                "content": "",
                "error": "Email has no message body and no attachments"
            }
           
    except Exception as e:
        return {
            "is_body_extracted": False,
            "content": "",
            "error": str(e)
        }
def get_latest_email():
    """Fetch the latest email from Gmail"""
    global last_check_time
   
    try:
        service = get_gmail_service()
       
        # Get the most recent message
        results = service.users().messages().list(userId="me", maxResults=1).execute()
        messages = results.get("messages", [])
       
        if not messages:
            return {
                "is_body_extracted": False,
                "content": "",
                "error": "No messages found"
            }
       
        # Get the message details
        msg = service.users().messages().get(
            userId="me",
            id=messages[0]["id"],
            format="full"
        ).execute()
       
        # Extract email body
        result = extract_email_body(service, msg)
       
        # Update last check time
        last_check_time = datetime.now()
       
        return {
            "body": result.get("content", ""),
            "is_email_extracted": result.get("is_body_extracted", False),
            "error": result.get("error")
        }
       
    except Exception as e:
        return {
            "body": "",
            "is_email_extracted": False,
            "error": str(e)
        }
 
async def check_for_new_emails():
    """Background task to check for new emails"""
    global last_check_time
   
    try:
        service = get_gmail_service()
       
        # Get messages since last check
        if last_check_time:
            query = f"after:{int(last_check_time.timestamp())}"
            results = service.users().messages().list(userId="me", q=query).execute()
            messages = results.get("messages", [])
           
            for message in messages:
                msg = service.users().messages().get(
                    userId="me",
                    id=message["id"],
                    format="full"
                ).execute()
               
                # Process new message
                result = extract_email_body(service, msg)
                print(f"New email received: {json.dumps(result, indent=2)}")
       
        # Update last check time
        last_check_time = datetime.now()
       
    except Exception as e:
        print(f"Error checking for new emails: {e}")
 
async def email_polling_loop():
    """Continuously poll for new emails"""
    global is_polling
    while is_polling:
        await check_for_new_emails()
        await asyncio.sleep(10)  # Poll every 10 seconds
 
async def start_email_polling():
    """Initialize email monitoring"""
    global is_polling
    is_polling = True
   
    # Initialize by getting the latest email
    result = get_latest_email()
    print(f"Initialized with latest email: {json.dumps(result, indent=2)}")
   
    # Start background task for polling
    asyncio.create_task(email_polling_loop())
 
async def stop_email_polling():
    """Stop email polling"""
    global is_polling
    is_polling = False
    print("Shutting down email monitoring...")
 
requirements.txt
 
langchain_openai==0.3.35
langchain-mcp-adapters==0.1.11
fastapi==0.118.3
fastapi_mcp==0.4.0
langgraph==0.6.10
python-dotenv
 
attachement_handler.py
 
import os, base64, json
from gmail_watch import get_gmail_service
 
SAVE_DIR = os.path.join("storage", "attachments")
STATE_FILE = os.path.join("storage", "history_state.json")
 
def ensure_storage():
    """Ensure storage directories exist"""
    os.makedirs(SAVE_DIR, exist_ok=True)
    os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)
 
# Initialize storage on module load
ensure_storage()
 
def load_state():
    try:
        with open(STATE_FILE, "r") as f:
            return json.load(f)
    except Exception:
        return {}
 
def save_state(state):
    with open(STATE_FILE, "w") as f:
        json.dump(state, f, indent=2)
 
def _walk_parts(parts):
    for p in parts or []:
        yield p
        for sub in p.get("parts", []) or []:
            yield from _walk_parts([sub])
 
def save_attachments_from_message(service, message):
    saved = []
    parts = message.get("payload", {}).get("parts", [])
    for part in _walk_parts(parts):
        filename = part.get("filename")
        body = part.get("body", {})
        att_id = body.get("attachmentId")
 
        if filename and att_id:
            att = service.users().messages().attachments().get(
                userId="me", messageId=message["id"], id=att_id
            ).execute()
            data = att.get("data")
            # Fix: Handle missing padding in base64 data
            padding_needed = 4 - (len(data) % 4)
            if padding_needed != 4:
                data += '=' * padding_needed
            file_data = base64.urlsafe_b64decode(data)
            path = os.path.join(SAVE_DIR, filename)
            with open(path, "wb") as f:
                f.write(file_data)
            saved.append(path)
            print("Saved attachment:", path)
    return saved
 
def save_email_body(service, message):
    """Save email body into a JSON file with status and content"""
    body_text = None
    error_message = None
    metadata = {}
   
    try:
        # Extract metadata headers
        headers = message.get("payload", {}).get("headers", [])
        for header in headers:
            name = header.get("name", "").lower()
            if name in ["subject", "from", "to", "date"]:
                metadata[name] = header.get("value", "")
       
        # Try to get body if full format available
        if "payload" in message:
            # Look for plain text part
            for part in _walk_parts(message.get("payload", {}).get("parts", [])):
                mime_type = part.get("mimeType")
                data = part.get("body", {}).get("data")
                if mime_type in ["text/plain", "text/html"] and data:
                    body_text = base64.urlsafe_b64decode(data).decode("utf-8", errors="ignore")
                    break
           
            # If no parts found, try getting body directly from payload
            if not body_text and message.get("payload", {}).get("body", {}).get("data"):
                body_text = base64.urlsafe_b64decode(
                    message["payload"]["body"]["data"]
                ).decode("utf-8", errors="ignore")
 
    except Exception as e:
        error_message = f"Error extracting email body: {str(e)}"
        print("❌", error_message)
 
    # Prepare JSON content
    email_data = {
        "is_body_extracted": bool(body_text),
        "content": body_text if body_text else "",
        "error": error_message if error_message else ""
    }
 
    # Get subject for filename
    subject = "no_subject"
    headers = message.get("payload", {}).get("headers", [])
    for h in headers:
        if h["name"].lower() == "subject":
            subject = h["value"].replace(" ", "_").replace("/", "_")
            break
 
    # Save as JSON
    file_path = os.path.join(SAVE_DIR, f"{subject}.json")
    with open(file_path, "w", encoding="utf-8") as f:
        json.dump(email_data, f, indent=2, ensure_ascii=False)
   
    print("Saved email body as JSON:", file_path)
    #print("Email Data: ", email_data)
    return email_data
 
def process_notification(email, history_id):
    service = get_gmail_service()
    state = load_state()
    last_hist = state.get(email)
 
    if not last_hist:
        # initialize and exit on first notification
        state[email] = history_id
        save_state(state)
        print("Initialized history id", history_id)
        return
 
    try:
        resp = service.users().history().list(
            userId="me", startHistoryId=str(last_hist), historyTypes="messageAdded"
        ).execute()
        histories = resp.get("history", [])
        message_ids = []
        for h in histories:
            for ma in h.get("messagesAdded", []):
                message_ids.append(ma["message"]["id"])
    except Exception as e:
        print("history.list failed:", e)
        # fallback → get recent messages
        res = service.users().messages().list(userId="me", q="newer_than:7d", maxResults=20).execute()
        message_ids = [m["id"] for m in res.get("messages", [])]
 
    for mid in message_ids:
        msg = service.users().messages().get(userId="me", id=mid, format="full").execute()
        print("Processing message id", mid, "snippet:", msg.get("snippet"))
 
        # Save email body
        save_email_body(service, msg)
 
        # Save attachments
        saved = save_attachments_from_message(service, msg)
        if saved:
            print("Attachments saved:", saved)
 
    state[email] = history_id
    save_state(state)
 
 
gmail_watch.py
 
import os
import pickle
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build
 
# Scopes (ONLY place where they are defined)
# gmail.readonly → fetch full messages + attachments
# gmail.metadata → required for Pub/Sub notifications
SCOPES = [
    'https://www.googleapis.com/auth/gmail.readonly'
]
 
def get_gmail_service(force_auth=False):
    creds = None
    token_path = "token.pickle"
    credentials_dir = "credentials"
    client_secret_path = os.path.join(credentials_dir, "client_secret.json")
 
    # Ensure credentials directory exists
    if not os.path.exists(credentials_dir):
        os.makedirs(credentials_dir)
        print(f"📁 Created credentials directory at {credentials_dir}")
 
    # Check for client_secret.json
    if not os.path.exists(client_secret_path):
        raise FileNotFoundError(
            f"❌ Error: {client_secret_path} not found. Please place your Google OAuth credentials file here."
        )
 
    # Load existing token if valid and not forcing new auth
    if not force_auth and os.path.exists(token_path):
        try:
            with open(token_path, "rb") as f:
                creds = pickle.load(f)
            if creds and creds.valid:
                print("✅ Loaded existing valid credentials")
                return build("gmail", "v1", credentials=creds, cache_discovery=False)
        except Exception as e:
            print(f"⚠️ Error loading existing token: {e}")
            creds = None
 
    # If we need new credentials, start the OAuth flow
    try:
        # Remove existing token if forcing new auth or token is invalid
        if os.path.exists(token_path):
            os.remove(token_path)
            print("🔑 Removed existing token to force new authentication")
       
        print("🔐 Starting Gmail OAuth authentication flow...")
        flow = InstalledAppFlow.from_client_secrets_file(
            client_secret_path,
            SCOPES
        )
       
        # Run the local server on a different port to avoid conflicts
        creds = flow.run_local_server(
            port=0,  # Let OS assign a free port
            prompt='consent',  # Always show the consent screen
            success_message="Gmail authentication successful! You can close this window.",
            open_browser=True  # Automatically open the browser
        )
       
        # Save the credentials for future use
        with open(token_path, "wb") as f:
            pickle.dump(creds, f)
        print("✅ Authentication successful - credentials saved")
    except Exception as e:
        print(f"Authentication error: {e}")
        raise
 
    service = build("gmail", "v1", credentials=creds, cache_discovery=False)
    return service
 
def create_watch(project_id, topic_full_name):
    service = get_gmail_service()
    body = {
        "labelIds": ["INBOX"],
        "topicName": topic_full_name
    }
    resp = service.users().watch(userId="me", body=body).execute()
    print("Watch created:", resp)
    return resp
 
if __name__ == "__main__":
    import sys
    if len(sys.argv) < 3:
        print("Usage: python gmail_watch.py <PROJECT_ID> <projects/PROJECT_ID/topics/TOPIC>")
        sys.exit(1)
 
    project = sys.argv[1]
    topic = sys.argv[2]
    create_watch(project, topic)




















Code 2


main.py

# main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn
import asyncio

# Import router and polling function
from routers import notification_router


# Shared CORS config
def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )


# Agent-specific sub-app creator
def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    app = FastAPI(title=title, description=description, version=version)
    apply_cors(app)
    return app


# Main app
app = FastAPI(title="MCP Email Server")
apply_cors(app)

email_app = create_sub_app(
    title="MCP Email Server",
    description="Gmail to Azure Blob duplicate attachment checker using MCP-style sub-app."
)

email_app.include_router(notification_router.router)

FastApiMCP(email_app, include_operations=["process_notification"]).mount_http()
app.mount("/api/v1/email", email_app)


@app.on_event("startup")
async def startup_main():
    """Start the email polling loop from the main app (since mounted app startup events don’t auto-run)."""
    print("📧 Starting email polling from main app...")
    asyncio.create_task(notification_router.email_polling_loop())


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8854)


gmail_watch.py
# gmail_watch.py
import os
import pickle
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build

# Scopes (ONLY place where they are defined)
# gmail.readonly → fetch full messages + attachments
# gmail.metadata → required for Pub/Sub notifications
SCOPES = [
    'https://www.googleapis.com/auth/gmail.readonly'
]

def get_gmail_service():
    """Authenticate with Gmail API and return the service object."""
    creds = None
    token_path = "token.pickle"
    
    # 🔄 Load existing token if valid
    if os.path.exists(token_path):
        with open(token_path, "rb") as f:
            creds = pickle.load(f)
    
    # 🔑 If no valid token → prompt user login
    if not creds or not creds.valid:
        flow = InstalledAppFlow.from_client_secrets_file("credentials/client_secret.json", SCOPES)
        creds = flow.run_local_server(port=0)
        with open(token_path, "wb") as f:
            pickle.dump(creds, f)
    
    # Build the Gmail service
    service = build("gmail", "v1", credentials=creds, cache_discovery=False)
    return service

def create_watch(project_id, topic_full_name):
    """Create a Gmail watch for Pub/Sub notifications."""
    service = get_gmail_service()
    body = {
        "labelIds": ["INBOX"],
        "topicName": topic_full_name
    }
    resp = service.users().watch(userId="me", body=body).execute()
    print("Watch created:", resp)
    return resp

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 3:
        print("Usage: python gmail_watch.py <PROJECT_ID> <projects/PROJECT_ID/topics/TOPIC>")
        sys.exit(1)
    
    project = sys.argv[1]
    topic = sys.argv[2]
    create_watch(project, topic)


attachment_handler.py

# attachment_handler.py
import os
import json
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv

load_dotenv()  # Load environment variables from .env if present

STATE_FILE = "history_state.json"
OUTPUT_DIR = "storage_output"
os.makedirs(OUTPUT_DIR, exist_ok=True)

# Azure Blob configuration via environment variables
AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
AZURE_BLOB_CONTAINER = os.environ.get("AZURE_BLOB_CONTAINER", "")

def _get_blob_service_client():
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING")
    return BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

def _ensure_container_exists(blob_service_client):
    if not AZURE_BLOB_CONTAINER:
        raise RuntimeError("Missing AZURE_BLOB_CONTAINER")
    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER)
    try:
        container_client.create_container()
    except Exception:
        pass  # Likely already exists
    return container_client

def get_blob_names_from_container():
    """Retrieve all blob names from the Azure Blob Storage container."""
    blob_names = []
    try:
        blob_service_client = _get_blob_service_client()
        container_client = _ensure_container_exists(blob_service_client)
        blobs = container_client.list_blobs()
        blob_names = [blob.name for blob in blobs]
    except Exception as e:
        print(f"Error retrieving blob names: {e}")
    return blob_names

def load_state():
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def save_state(state):
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, indent=2)

def _walk_parts(parts):
    for p in parts or []:
        yield p
        for sub in p.get("parts", []) or []:
            yield from _walk_parts([sub])

def save_attachments_from_message(service, message):
    """Extract attachment filenames from Gmail message and compare with Azure Blob Storage.

    Returns a dictionary with attachment names and comparison results, including a "is_duplicate" flag.
    """
    saved_attachments = []  # List to store attachment filenames
    parts = message.get("payload", {}).get("parts", [])
    
    for part in _walk_parts(parts):
        filename = part.get("filename") 
        if filename:
            saved_attachments.append(filename)  
    
    blob_service_client = _get_blob_service_client()
    container_client = _ensure_container_exists(blob_service_client)
    
    attachment_results = []  
    
    for filename in saved_attachments:
        is_duplicate = False  
        try:
            blob_client = container_client.get_blob_client(blob=filename)
            if blob_client.exists():
                is_duplicate = True
        except Exception as e:
            print(f"Error checking blob existence for {filename}: {e}")
        
        attachment_results.append({
            "filename": filename,
            "is_duplicate": is_duplicate
        })
    
    comparison_result = {
        "attachments": attachment_results
    }
    
    try:
        out_path = os.path.join(OUTPUT_DIR, f"{message.get('id')}_comparison.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(comparison_result, f, indent=2)
    except Exception as exc:
        print(f"Failed to write comparison result file: {exc}")
    
    return comparison_result




notification router.py
# routers/notification_router.py
import asyncio
from fastapi import APIRouter, BackgroundTasks
from pydantic import BaseModel
import uvicorn
from datetime import datetime
from gmail_watch import get_gmail_service
from attachment_handler import (
    save_attachments_from_message,
    load_state,
    save_state
)

router = APIRouter()

# Global variables for polling
running = True
last_check_time = None

class EmailNotification(BaseModel):
    emailAddress: str
    historyId: int

async def check_new_emails():
    """Check for new emails and process them."""
    global last_check_time
    service = get_gmail_service()
    
    try:
        # On first run: fetch the latest 5 emails. Afterwards: incremental by timestamp.
        if not last_check_time:
            results = service.users().messages().list(
                userId="me",
                maxResults=5
            ).execute()
        else:
            query = f"after:{int(last_check_time.timestamp())}"
            results = service.users().messages().list(userId="me", q=query).execute()
        messages = results.get("messages", [])
        
        for message in messages:
            msg = service.users().messages().get(
                userId="me",
                id=message["id"],
                format="full"
            ).execute()
            
            print(f"📩 Processing message ID: {message['id']}")
            
            # Save attachment filenames and compare with Azure Blob Storage
            comparison_result = save_attachments_from_message(service, msg)
            print(f"📎 Comparison result: {comparison_result}")
        
        last_check_time = datetime.now()
    
    except Exception as e:
        print(f"Error checking emails: {e}")

async def email_polling_loop():
    """Continuously poll for new emails."""
    while running:
        try:
            await check_new_emails()
        except Exception as e:
            print("Polling iteration error:", e)
        await asyncio.sleep(10)  # Poll every 10 seconds

# Startup and shutdown for this router (these will be wired into the sub-app startup)
# @router.on_event("startup")
# async def startup_event():
#     """Initialize and start email polling on startup."""
#     global running
#     try:
#         print("📧 Starting email polling...")
#         asyncio.create_task(email_polling_loop())
#     except Exception as e:
#         print("Startup non-fatal error:", e)

@router.on_event("shutdown")
async def shutdown_event():
    """Cleanup on shutdown."""
    global running
    running = False

@router.post("/process_notification", operation_id="process_notification")
async def process_notification(notification: EmailNotification, background_tasks: BackgroundTasks):
    """Process a Gmail notification and compare attachment filenames with Azure Blob Storage."""
    service = get_gmail_service()
    email = notification.emailAddress
    history_id = notification.historyId
    
    state = load_state()
    last_hist = state.get(email)
    
    if not last_hist:
        # Initialize state on first notification
        state[email] = history_id
        save_state(state)
        return {"message": f"Initialized history id {history_id}"}
    
    processed_messages = []
    try:
        # Get history since last check
        resp = service.users().history().list(
            userId="me",
            startHistoryId=str(last_hist),
            historyTypes="messageAdded"
        ).execute()
        
        histories = resp.get("history", [])
        message_ids = []
        for h in histories:
            for ma in h.get("messagesAdded", []):
                message_ids.append(ma["message"]["id"])
        
    except Exception as e:
        print("History list failed:", e)
        # Fallback: get recent messages
        res = service.users().messages().list(
            userId="me",
            q="newer_than:7d",
            maxResults=20
        ).execute()
        message_ids = [m["id"] for m in res.get("messages", [])]

    for mid in message_ids:
        msg = service.users().messages().get(
            userId="me",
            id=mid,
            format="full"
        ).execute()
        
        # Save attachment filenames and compare with Azure Blob Storage
        comparison_result = save_attachments_from_message(service, msg)
        processed_messages.append({
            "message_id": mid,
            "comparison_result": comparison_result
        })
    
    # Update state
    state[email] = history_id
    save_state(state)
    
    return {
        "message": f"Processed {len(processed_messages)} messages",
        "processed": processed_messages
    }
 
