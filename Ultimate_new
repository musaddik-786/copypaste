Great — I updated the whole project so the PDF keeps its original filename when downloaded (instead of always writing input.pdf). I also preserved the source_file field in the API response and kept the exact MCP names and routes you asked for.

Key changes
	•	When downloading the PDF from the blob URL, the handler now saves it locally using the blob’s basename (e.g. Acord_125_FullForm_Filled 1.pdf) inside the project root (same folder where you run the app).
	•	source_file is returned in the result (the file name).
	•	All other behavior is preserved: analysis logic unchanged, local JSON saved as output.json (temporary), JSON uploaded to output-results, MCP routes and operation IDs unchanged.

Drop these files into your project (replace previous versions).

⸻

router.py

from dotenv import load_dotenv
load_dotenv()

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator

from image_processor import analyze_file_async

router = APIRouter()


class DocumentExtractMCP(BaseModel):
    """Trigger document extraction for a PDF file located in blob storage."""
    AgentName: str = Field(
        default="DocumentExtractAgent",
        description="The unique agent name of the agent that is being called"
    )
    BlobUrl: str = Field(
        ...,
        description="Required: full blob URL (https://...) or container/blob path (container/path/to/file.pdf) to analyze"
    )

    @validator("BlobUrl")
    def bloburl_must_not_be_empty(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("BlobUrl must be a non-empty string pointing to the blob (full URL or container/blob path).")
        return v.strip()


@router.post("/document_extract_mcp", operation_id="document_extract_mcp")
async def document_extract_mcp(p_body: DocumentExtractMCP):
    """
    Analyze a PDF from Azure Blob storage using image_processor.analyze_file_async and
    return the analyzer's structured result.

    Body:

        AgentName (str): Agent name (optional)

        BlobUrl (str): Required full blob URL or container/blob path.
    """
    try:
        result = await analyze_file_async(bloburl=p_body.BlobUrl)

        return JSONResponse(content={
            "jsonrpc": "2.0",
            "id": 1,
            "result": result
        })

    except Exception as e:
        # Keep error behavior similar to your sample: return HTTP 500 with error message
        return JSONResponse(status_code=500, content={"error": f"Extraction failed: {e}"})


⸻

main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn
from contextlib import asynccontextmanager
import asyncio
import os

from router import router as document_router
from image_processor import process_input_folder_on_startup

async def _start_processing_task() -> None:
    """
    Create and store a background task that runs the processor once at startup.
    This returns the created task (already scheduled).
    """
    task = asyncio.create_task(process_input_folder_on_startup())
    return task

async def _stop_processing_task(task: asyncio.Task) -> None:
    """
    Cancel and await the background task if it's still running.
    """
    if task and not task.done():
        task.cancel()
        try:
            await task
        except Exception:
            # swallow exceptions on shutdown
            pass

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup: schedule the processing task and attach to app.state
    try:
        app.state.processing_task = await _start_processing_task()
    except Exception:
        app.state.processing_task = None

    yield

    # Shutdown: cancel the task if running
    task = getattr(app.state, "processing_task", None)
    if task:
        await _stop_processing_task(task)

app = FastAPI(lifespan=lifespan)

def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )

def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    sub_app = FastAPI(title=title, description=description, version=version, lifespan=lifespan)
    apply_cors(sub_app)
    return sub_app

apply_cors(app)

document_extractor_app = create_sub_app(
    title="document_extract_mcp",
    description="Analyze documents in blob storage using Azure Form Recognizer and return structured results"
)

document_extractor_app.include_router(document_router)
FastApiMCP(document_extractor_app, include_operations=["document_extract_mcp"]).mount_http()

# Mount path kept as in your template
app.mount("/api/v1/email_intent_agent", document_extractor_app)


if __name__ == "__main__":
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8502"))
    uvicorn.run(app, host=host, port=port)


⸻

image_processor.py

import os
import json
import uuid
from datetime import datetime
from typing import Optional

from dotenv import load_dotenv
load_dotenv()

from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence.aio import DocumentIntelligenceClient
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError

from service import _require_env, get_env_values, AZURE_STORAGE_CONNECTION_STRING, _parse_blob_url

# Local temporary output JSON filename (keeps same behavior)
LOCAL_OUTPUT_FILENAME = "output.json"


async def analyze_file_async(bloburl: str) -> dict:
    """
    1) Parse bloburl
    2) Download PDF blob to local file using the blob's basename (original filename)
    3) Call Azure Document Intelligence to analyze that local file (identical logic)
    4) Save JSON locally as ./output.json
    5) Upload JSON to output-results container and return output blob url + source_file name
    """
    # Validate env
    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    endpoint, key, model_id = get_env_values()

    # Parse blob url -> (container, blob_path)
    try:
        src_container, src_blob = _parse_blob_url(bloburl)
    except Exception as e:
        return {"status": False, "error": f"Invalid BlobUrl: {e}"}

    # Extract the basename of the blob (the PDF filename)
    source_file_name = os.path.basename(src_blob) if src_blob else "unknown.pdf"
    # sanitize filename to avoid weird paths (this keeps basename only)
    source_file_name = os.path.basename(source_file_name)

    # Download PDF bytes from blob storage
    if not AZURE_STORAGE_CONNECTION_STRING:
        return {"status": False, "error": "Missing AZURE_STORAGE_CONNECTION_STRING in .env"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(src_container)
            blob_client = container_client.get_blob_client(src_blob)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"PDF blob not found: container='{src_container}', blob='{src_blob}'"}

            stream = await blob_client.download_blob()
            pdf_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error downloading PDF from Blob Storage: {e}"}

    # Write to local file using the original filename
    input_path = os.path.join(os.getcwd(), source_file_name)
    try:
        with open(input_path, "wb") as wf:
            wf.write(pdf_bytes)
    except Exception as e:
        return {"status": False, "error": f"Failed to write local input file '{input_path}': {e}"}

    # Analyze using Azure Document Intelligence (async)
    try:
        client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    except Exception as e:
        return {"status": False, "error": f"Failed to create DocumentIntelligenceClient: {e}"}

    try:
        async with client:
            with open(input_path, "rb") as f:
                poller = await client.begin_analyze_document(model_id=model_id, body=f)
                result = await poller.result()
    except Exception as e:
        return {"status": False, "error": f"Error during document analysis: {e}"}

    # Save JSON locally as output.json (temporary)
    output_path = os.path.join(os.getcwd(), LOCAL_OUTPUT_FILENAME)
    json_data = result.as_dict()
    try:
        with open(output_path, "w", encoding="utf-8") as wf:
            json.dump(json_data, wf, indent=2, ensure_ascii=False)
    except Exception as e:
        return {"status": False, "error": f"Failed to save local output.json: {e}"}

    # Upload JSON to output-results container
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    unique = uuid.uuid4().hex
    target_blob_name = f"{timestamp}_{unique}_extracted_{timestamp}.json"
    output_container = "output-results"

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            out_container_client = blob_service.get_container_client(output_container)
            # Try to create container if missing (best-effort)
            try:
                await out_container_client.create_container()
            except Exception:
                pass

            out_blob_client = out_container_client.get_blob_client(target_blob_name)
            json_bytes = json.dumps(json_data, indent=2, ensure_ascii=False).encode("utf-8")
            await out_blob_client.upload_blob(json_bytes, overwrite=True)
    except Exception as e:
        return {"status": False, "error": f"Failed to upload JSON to output-results: {e}"}

    # Build full output blob URL using the same account domain as the incoming bloburl
    parsed = bloburl.split("://", 1)[-1]  # e.g. account.blob.core.windows.net/...
    account_and_rest = parsed.split("/", 1)[0]  # account.blob.core.windows.net
    output_blob_url = f"https://{account_and_rest}/{output_container}/{target_blob_name}"

    return {
        "status": True,
        "source_file": source_file_name,
        "output_blob_url": output_blob_url
    }


⸻

service.py

import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

ENDPOINT = os.getenv("ENDPOINT", "")
KEY = os.getenv("KEY", "")
MODEL_ID = os.getenv("MODEL_ID", "")
AZURE_STORAGE_CONNECTION_STRING = os.getenv("AZURE_STORAGE_CONNECTION_STRING", "")


def _require_env() -> None:
    """Ensure required environment variables exist."""
    if not (ENDPOINT and KEY and MODEL_ID):
        raise RuntimeError("Missing ENDPOINT, KEY, or MODEL_ID in .env file.")
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING in .env file.")


def get_env_values():
    """Return endpoint, key, model_id"""
    return ENDPOINT, KEY, MODEL_ID


def _parse_blob_url(url: str) -> tuple[str, str]:
    """
    Parse an Azure Blob URL and return (container_name, blob_path).
    Example URL:
      https://account.blob.core.windows.net/output-results/folder/name.json
    Returns:
      ("output-results", "folder/name.json")
    """
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("BlobUrl must be a valid http(s) URL to Azure Blob Storage.")
    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("BlobUrl must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)


⸻

test_client.py

# test_client.py
# Usage: python test_client.py
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient

async def get_tool_list(config_mcp_server):
    client = MultiServerMCPClient(config_mcp_server)
    tools_list = await client.get_tools()
    print("Tools fetched from MCP:", [tool.name for tool in tools_list])
    # some versions don't have aclose(); skip if missing
    if hasattr(client, "aclose"):
        await client.aclose()
    return tools_list

async def main():
    config_mcp_server = {
        "document_extract_mcp": {
            "url": "http://localhost:8502/api/v1/email_intent_agent/mcp",
            "transport": "streamable_http",
        }
    }
    tools = await get_tool_list(config_mcp_server=config_mcp_server)
    print("Total tools:", len(tools))

if __name__ == "__main__":
    asyncio.run(main())


⸻

test_scripts.py

# test_scripts.py
# Usage: python test_scripts.py
import requests
import json
import os

# Adjust host/port if you changed them in main.py .env
BASE_URL = os.getenv("DOC_EXTRACT_BASE_URL", "http://localhost:8502")
ENDPOINT = f"{BASE_URL}/api/v1/email_intent_agent/document_extract_mcp"

def run_test():
    payload = {
        "AgentName": "DocumentExtractAgent",
        # Replace with a real accessible PDF blob URL in your storage account
        "BlobUrl": "https://agenticai1.blob.core.windows.net/attachment-downloader/Acord_125_FullForm_Filled%201.pdf"
    }
    try:
        resp = requests.post(ENDPOINT, json=payload, timeout=180)
        print("HTTP Status:", resp.status_code)
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except json.JSONDecodeError:
            print("Response not JSON:", resp.text)
    except Exception as e:
        print("Error calling API:", str(e))

if __name__ == "__main__":
    print("Testing /document_extract_mcp ...")
    run_test()


⸻

Final notes & tips
	•	The downloaded PDF will be saved in the same directory where you run the app, named exactly as the blob filename. If you want them stored in a dedicated downloads/ folder instead, say so and I’ll update the path.
	•	If you expect many files or large PDFs, consider switching the local writes to aiofiles (non-blocking) and/or remove the local PDF after processing to save disk.
	•	If you want the response also to contain the container name in addition to source_file, I can add "source_container": "<container-name>".
	•	To test with curl (example):

curl -X POST "http://127.0.0.1:8502/api/v1/email_intent_agent/document_extract_mcp" \
  -H "Content-Type: application/json" \
  -d '{"AgentName": "DocumentExtractAgent", "BlobUrl": "https://agenticai1.blob.core.windows.net/attachment-downloader/Acord_125_FullForm_Filled%201.pdf"}'

If you want, I can:
	•	automatically delete the local PDF after analysis,
	•	save downloads into a dedicated folder,
	•	or change output.json temporary name to match the source PDF name (e.g., Acord_125_...json) — tell me which.
