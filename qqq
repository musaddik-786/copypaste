main.py



# main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn
from contextlib import asynccontextmanager

from routers.attachment_router import router as attachment_router
from attachment_service import start_email_polling, stop_email_polling

def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )

@asynccontextmanager
async def lifespan(app: FastAPI):
    # Startup
    await start_email_polling()
    yield
    # Shutdown
    await stop_email_polling()

def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    sub = FastAPI(title=title, description=description, version=version, lifespan=lifespan)
    apply_cors(sub)
    return sub

# Root app
app = FastAPI(lifespan=lifespan)
apply_cors(app)

# MCP-style sub-app
email_app = create_sub_app(
    title="attachment_checker_mcp",
    description="Reads latest emails and compares attachment filenames with Azure Blob names."
)
email_app.include_router(attachment_router)

# Expose MCP op
FastApiMCP(email_app, include_operations=["attachment_checker_mcp"]).mount_http()

# Mount like Code 1
app.mount("/mcp", email_app)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8502)









attachment_handler.py

import os
import json
import base64
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv

load_dotenv()

# Storage/state files (similar to your Code 2)
STATE_FILE = os.path.join("storage", "history_state.json")
OUTPUT_DIR = os.path.join("storage", "attachments")
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)

# Azure Blob config
AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
AZURE_BLOB_CONTAINER = os.environ.get("AZURE_BLOB_CONTAINER", "")

def _get_blob_service_client():
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING")
    return BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

def _ensure_container_exists(blob_service_client):
    if not AZURE_BLOB_CONTAINER:
        raise RuntimeError("Missing AZURE_BLOB_CONTAINER")
    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER)
    try:
        container_client.create_container()
    except Exception:
        pass
    return container_client

def load_state():
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def save_state(state):
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, indent=2, ensure_ascii=False)

def _walk_parts(parts):
    for p in parts or []:
        yield p
        for sub in p.get("parts", []) or []:
            yield from _walk_parts([sub])

def save_attachments_from_message(service, message):
    """
    Extract attachment filenames from Gmail message and compare with Azure Blob Storage.
    Returns: {"attachments": [{"filename": "...", "is_duplicate": bool}, ...]}
    """
    filenames = []
    parts = message.get("payload", {}).get("parts", [])
    for part in _walk_parts(parts):
        filename = part.get("filename")
        body = part.get("body", {}) or {}
        # Only care about filename presence for comparison purpose
        if filename:
            filenames.append(filename)

    # Compare with Azure container
    attachment_results = []
    try:
        blob_service_client = _get_blob_service_client()
        container_client = _ensure_container_exists(blob_service_client)

        for name in filenames:
            is_duplicate = False
            try:
                blob_client = container_client.get_blob_client(blob=name)
                if blob_client.exists():
                    is_duplicate = True
            except Exception as e:
                print(f"Error checking blob existence for {name}: {e}")

            attachment_results.append({
                "filename": name,
                "is_duplicate": is_duplicate
            })
    except Exception as e:
        # If Azure check fails entirely, still return filenames with error context
        print(f"Azure comparison error: {e}")
        attachment_results = [{"filename": n, "is_duplicate": False, "error": str(e)} for n in filenames]

    # Optional: persist a small artifact per message
    try:
        out_path = os.path.join(OUTPUT_DIR, f"{message.get('id', 'unknown')}_comparison.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump({"attachments": attachment_results}, f, indent=2, ensure_ascii=False)
    except Exception as exc:
        print(f"Failed to write comparison result file: {exc}")

    return {"attachments": attachment_results}



attachment_service.py

import asyncio
import json
from datetime import datetime
from typing import Dict, Any, List
from gmail_watch import get_gmail_service
from attachment_handler import (
    save_attachments_from_message,
    load_state,
    save_state
)

# Polling state
_last_check_time = None
_is_polling = True

def _extract_attachment_comparison(service, msg) -> Dict[str, Any]:
    """
    For a Gmail message, return comparison of attachment filenames vs Azure blobs.
    """
    comparison = save_attachments_from_message(service, msg)
    # Normalized result expected by router (jsonrpc 'result' payload)
    return {
        "message_id": msg.get("id", ""),
        "attachments": comparison.get("attachments", [])
    }

def get_latest_email_attachment_check() -> Dict[str, Any]:
    """
    Fetch the latest email and return comparison results.
    Mirrors Code 1's get_latest_email() pattern but for attachments.
    """
    try:
        service = get_gmail_service()
        results = service.users().messages().list(userId="me", maxResults=1).execute()
        messages = results.get("messages", [])

        if not messages:
            return {
                "attachments": [],
                "is_email_extracted": False,
                "error": "No messages found"
            }

        msg = service.users().messages().get(
            userId="me",
            id=messages[0]["id"],
            format="full"
        ).execute()

        comparison = _extract_attachment_comparison(service, msg)

        return {
            "attachments": comparison.get("attachments", []),
            "is_email_extracted": True,
            "error": None
        }
    except Exception as e:
        return {
            "attachments": [],
            "is_email_extracted": False,
            "error": str(e)
        }

async def _check_for_new_emails():
    """
    Background task step: check new emails since last check, compare attachments vs Azure blobs.
    """
    global _last_check_time
    try:
        service = get_gmail_service()

        # Initial bootstrap (few latest) then incremental by timestamp
        if not _last_check_time:
            results = service.users().messages().list(userId="me", maxResults=5).execute()
        else:
            query = f"after:{int(_last_check_time.timestamp())}"
            results = service.users().messages().list(userId="me", q=query).execute()

        messages = results.get("messages", [])
        for message in messages:
            msg = service.users().messages().get(
                userId="me",
                id=message["id"],
                format="full"
            ).execute()
            comparison = _extract_attachment_comparison(service, msg)
            print("ðŸ“Ž Attachment vs Blob comparison:", json.dumps(comparison, indent=2))

        _last_check_time = datetime.now()

    except Exception as e:
        print(f"Error checking for new emails: {e}")

async def _email_polling_loop():
    """
    Continuously poll for new emails.
    """
    global _is_polling
    print(" Email polling loop started.")
    while _is_polling:
        await _check_for_new_emails()
        await asyncio.sleep(10)
    print(" Email polling loop stopped.")

async def start_email_polling():
    """
    Initialize and start background polling on app startup (lifespan).
    """
    global _is_polling, _last_check_time
    _is_polling = True
    # Bootstrap by forcing a one-shot latest check for easier debugging
    bootstrap = get_latest_email_attachment_check()
    print("Initialized with latest attachment check:", json.dumps(bootstrap, indent=2))
    # Start background loop
    asyncio.create_task(_email_polling_loop())

async def stop_email_polling():
    """
    Stop background polling on app shutdown (lifespan).
    """
    global _is_polling
    _is_polling = False
    print(" Shutting down email monitoring...")




gmail_watch.py


import os
import pickle
from google_auth_oauthlib.flow import InstalledAppFlow
from googleapiclient.discovery import build

# Scopes (ONLY place where defined)
SCOPES = [
    'https://www.googleapis.com/auth/gmail.readonly'
]

def get_gmail_service(force_auth=False):
    creds = None
    token_path = "token.pickle"
    credentials_dir = "credentials"
    client_secret_path = os.path.join(credentials_dir, "client_secret.json")

    if not os.path.exists(credentials_dir):
        os.makedirs(credentials_dir, exist_ok=True)
        print(f" Created credentials directory at {credentials_dir}")

    if not os.path.exists(client_secret_path):
        raise FileNotFoundError(
            f" {client_secret_path} not found. Place your Google OAuth credentials file here."
        )

    if not force_auth and os.path.exists(token_path):
        try:
            with open(token_path, "rb") as f:
                creds = pickle.load(f)
            if creds and creds.valid:
                print(" Loaded existing valid credentials")
                return build("gmail", "v1", credentials=creds, cache_discovery=False)
        except Exception as e:
            print(f" Error loading existing token: {e}")
            creds = None

    # Fresh OAuth flow
    try:
        if os.path.exists(token_path):
            os.remove(token_path)
            print(" Removed existing token to force new authentication")

        print(" Starting Gmail OAuth authentication flow...")
        flow = InstalledAppFlow.from_client_secrets_file(client_secret_path, SCOPES)
        creds = flow.run_local_server(port=0, prompt='consent', success_message="Gmail authentication successful! You can close this window.", open_browser=True)

        with open(token_path, "wb") as f:
            pickle.dump(creds, f)
        print(" Authentication successful - credentials saved")
    except Exception as e:
        print(f"Authentication error: {e}")
        raise

    return build("gmail", "v1", credentials=creds, cache_discovery=False)

def create_watch(project_id, topic_full_name):
    service = get_gmail_service()
    body = {
        "labelIds": ["INBOX"],
        "topicName": topic_full_name
    }
    resp = service.users().watch(userId="me", body=body).execute()
    print("Watch created:", resp)
    return resp

if __name__ == "__main__":
    import sys
    if len(sys.argv) < 3:
        print("Usage: python gmail_watch.py <PROJECT_ID> <projects/PROJECT_ID/topics/TOPIC>")
        sys.exit(1)
    project = sys.argv[1]
    topic = sys.argv[2]
    create_watch(project, topic)






