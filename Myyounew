Nice — below are the complete, copy-paste ready files updated so that when a reference number from the remote blob Excel is appended into the local stub Excel (input/reference_stub.xlsx) its timestamp (IST) is written in Column B next to the reference in Column A.

I changed only what’s necessary (main flow unchanged). Key points:
	•	Timestamp uses India Standard Time (UTC+5:30) and is stored in ISO 8601 format (e.g. 2025-11-20T16:35:12+05:30).
	•	The stub file header is now: ["Reference Number", "Timestamp"].
	•	If the local stub already contains the same last reference, it will not append again (so no duplicate).
	•	If appending, the timestamp written is the current IST time at append moment.

⸻

file_router.py

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field

from handler import compare_name_with_sanctions, fetch_and_update_reference_number

router = APIRouter()


class CompareRequest(BaseModel):
    """
    This endpoint is invoked by the Data Extraction Agent.
    It passes the FULL Azure Blob URL of the extracted JSON in 'jsonfilepath'.
    """
    jsonfilepath: str = Field(..., description="Full Azure Blob URL to the extracted JSON.")


@router.post(
    "/Licensing_&_Sanction_Checker_MCP",
    operation_id="Licensing_&_Sanction_Checker_MCP",
    summary="Compare JSON (Owner/Insured/Contact/NAMED INSURED(S)) from blob URL with local sanctions CSV."
)
async def compare_files(request: CompareRequest):
    """
    Receives the JSON blob URL from the Data Extraction Agent and performs the comparison.
    Returns a JSON-RPC envelope for consistency with the duplicate attachment checker.
    """
    try:
        result = await compare_name_with_sanctions(json_file_url=request.jsonfilepath)
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": result
            },
            status_code=200
        )
    except Exception as e:
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": {"status": False, "error": f"Unexpected server error: {str(e)}"}
            },
            status_code=200
        )


@router.post(
    "/Fetch_And_Update_Reference",
    operation_id="Fetch_And_Update_Reference",
    summary="Fetch last reference number from Blob Excel and append to local stub Excel"
)
async def fetch_and_update_reference():
    """
    Download the Excel from blob, read last non-empty value in Column A,
    append that value and current IST timestamp to the local input/reference_stub.xlsx.
    """
    try:
        result = await fetch_and_update_reference_number()
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": result
            },
            status_code=200
        )
    except Exception as e:
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": {"status": False, "error": f"Unexpected server error: {str(e)}"}
            },
            status_code=200
        )


⸻

handler.py

import os
import json
from typing import Optional, Dict
import pandas as pd
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError
from dotenv import load_dotenv
from io import BytesIO
import openpyxl
from datetime import datetime, timezone, timedelta

from service import _require_env, _parse_blob_url

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
LOCAL_CSV_PATH = os.path.join("input", "sanctions.csv")

# URL of the remote Excel file on blob
BLOB_EXCEL_URL = "https://agenticai1.blob.core.windows.net/fetched-emails-data/Fetched_emails_data_new.xlsx"
# Local stub excel path (inside your project)
LOCAL_STUB_EXCEL = os.path.join("input", "reference_stub.xlsx")


async def compare_name_with_sanctions(json_file_url: str) -> Dict:
    """
    Flow:
      1) Parse blob URL -> (container, blob_name)
      2) Download JSON (async)
      3) Dynamically search for target labels in any nested structure
      4) Read local CSV input/sanctions.csv
      5) Case-insensitive exact match against entity_name
      6) Save outputs to ./output and return structured result (with 'status')
    """

    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    try:
        container_name, blob_name = _parse_blob_url(json_file_url)
    except Exception as e:
        return {"status": False, "error": f"Invalid jsonfilepath: {e}"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_name)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"JSON blob not found: container='{container_name}', blob='{blob_name}'"}

            stream = await blob_client.download_blob()
            raw_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error connecting to Azure Blob Storage: {e}"}

    # Python dict
    try:
        data = json.loads(raw_bytes.decode("utf-8"))
    except Exception as e:
        return {"status": False, "error": f"Failed to parse JSON: {e}"}

    # Target labels to search for dynamically
    target_labels = {"ownername", "insuredname", "contactname", "namedinsured(s)",
                     "applicant/firstnameinsured", "insured/applicant", "namedinsured",
                     "namedinsureds", "firstnameinsured", "applicantname"}
    extracted_name: Optional[str] = None

    def search_nested(data: dict) -> Optional[str]:
        """
        Recursively search for any field matching target labels in nested dictionaries or lists.
        """
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, dict) or isinstance(value, list):
                    result = search_nested(value)
                    if result:
                        return result
                elif isinstance(key, str):
                    normalized_key = key.lower().replace(" ", "").replace(":", "")
                    if normalized_key in target_labels:
                        if isinstance(value, str):
                            return value.split(",")[0].strip()
                        else:
                            return str(value)
        elif isinstance(data, list):
            for item in data:
                result = search_nested(item)
                if result:
                    return result
        return None

    extracted_name = search_nested(data)

    if not extracted_name:
        return {"status": False, "error": "No matching name field found in JSON."}

    if not os.path.exists(LOCAL_CSV_PATH):
        return {"status": False, "error": f"Local CSV not found at '{LOCAL_CSV_PATH}'"}

    try:
        csv_df = pd.read_csv(LOCAL_CSV_PATH)
    except Exception as e:
        return {"status": False, "error": f"Failed to read CSV '{LOCAL_CSV_PATH}': {e}"}

    if "entity_name" not in csv_df.columns:
        return {"status": False, "error": "CSV must contain 'entity_name' column"}

    def norm(s: str) -> str:
        return s.strip().lower() if isinstance(s, str) else ""

    entity_set = {norm(x) for x in csv_df["entity_name"].dropna() if isinstance(x, str) and x.strip()}
    is_sanctioned = norm(extracted_name) in entity_set

    results = [{"Extracted Name": extracted_name, "is_Sanctioned": is_sanctioned}]

    try:
        os.makedirs("./output", exist_ok=True)
        json_output = "./output/comparison_results.json"
        with open(json_output, "w", encoding="utf-8") as f:
            json.dump({"results": results}, f, indent=4, ensure_ascii=False)
    except Exception as e:
        return {"status": False, "error": f"Failed saving results: {e}"}

    return {"status": True, "results": results}


async def fetch_and_update_reference_number() -> Dict:
    """
    Steps:
    1. Download Excel from Blob (BLOB_EXCEL_URL)
    2. Read last non-empty value in Column A
    3. Append that value and current IST timestamp to local stub Excel in input/reference_stub.xlsx
    """

    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    # --- 1. Parse Blob URL ---
    try:
        container_name, blob_name = _parse_blob_url(BLOB_EXCEL_URL)
    except Exception as e:
        return {"status": False, "error": f"Invalid Blob Excel URL: {e}"}

    # --- 2. Download Excel ---
    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as svc:
            container_client = svc.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_name)

            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": "Blob Excel not found."}

            stream = await blob_client.download_blob()
            file_bytes = await stream.readall()

    except Exception as e:
        return {"status": False, "error": f"Failed to download Excel: {e}"}

    # --- 3. Load Excel from Bytes ---
    try:
        wb = openpyxl.load_workbook(filename=BytesIO(file_bytes), read_only=False, data_only=True)
        sheet = wb.active
    except Exception as e:
        return {"status": False, "error": f"Failed to read Excel from Blob: {e}"}

    # --- 4. Read last non-empty cell in Column A ---
    last_reference_number = None
    for row in range(sheet.max_row, 0, -1):
        cell_value = sheet.cell(row=row, column=1).value
        if cell_value not in (None, "", " "):
            last_reference_number = str(cell_value).strip()
            break

    if not last_reference_number:
        return {"status": False, "error": "Column A contains no valid values."}

    # --- 5. Ensure input directory and stub Excel exists ---
    try:
        os.makedirs("input", exist_ok=True)
        if not os.path.exists(LOCAL_STUB_EXCEL):
            wb_stub = openpyxl.Workbook()
            ws_stub = wb_stub.active
            # create a header for columns A and B.
            ws_stub.append(["Reference Number", "Timestamp"])
            wb_stub.save(LOCAL_STUB_EXCEL)
    except Exception as e:
        return {"status": False, "error": f"Failed to create stub excel: {e}"}

    # --- 6. Append last reference number and timestamp to stub Excel ---
    try:
        wb_stub = openpyxl.load_workbook(LOCAL_STUB_EXCEL)
        ws_stub = wb_stub.active

        # find last existing populated cell in column A
        last_local = None
        for row in range(ws_stub.max_row, 0, -1):
            v = ws_stub.cell(row=row, column=1).value
            if v not in (None, "", " "):
                last_local = str(v).strip()
                break

        # current IST timestamp
        IST = timezone(timedelta(hours=5, minutes=30))
        now_ist = datetime.now(IST).isoformat()

        if last_local != last_reference_number:
            ws_stub.append([last_reference_number, now_ist])
            wb_stub.save(LOCAL_STUB_EXCEL)
            appended = True
        else:
            appended = False
    except Exception as e:
        return {"status": False, "error": f"Failed to update stub excel: {e}"}

    return {
        "status": True,
        "last_reference_number": last_reference_number,
        "appended": appended,
        "timestamp": now_ist,
        "message": f"Reference processed. appended={appended}. local_file={LOCAL_STUB_EXCEL}"
    }


⸻

service.py

import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")


def _require_env() -> None:
    """Ensure required environment variables exist."""
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING in .env")


def _parse_blob_url(url: str) -> tuple[str, str]:
    """
    Parse an Azure Blob URL and return (container_name, blob_path).
    Example URL:
      https://account.blob.core.windows.net/output-results/folder/name.json
    Returns:
      ("output-results", "folder/name.json")
    """
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("jsonfilepath must be a valid http(s) URL to Azure Blob Storage.")

    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("jsonfilepath must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)


⸻

main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn

from file_router import router as file_router


def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )


def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    sub = FastAPI(title=title, description=description, version=version)
    apply_cors(sub)
    return sub


app = FastAPI()
apply_cors(app)

file_app = create_sub_app(
    title="Licensing & Sanction Checker MCP",
    description="Receives JSON blob URL from Data Extraction Agent and compares extracted name with local sanctions CSV."
)
file_app.include_router(file_router)

# Expose these operation ids via MCP HTTP
FastApiMCP(
    file_app,
    include_operations=[
        "Licensing_&_Sanction_Checker_MCP",
        "Fetch_And_Update_Reference"
    ]
).mount_http()

# Mount the sub-app under this prefix
app.mount("/api/v1/eligibility_agent", file_app)


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8601)


⸻

test_scripts.py

import requests
import json

BASE_COMPARE_URL = "http://localhost:8601/api/v1/eligibility_agent/Licensing_&_Sanction_Checker_MCP"
BASE_REF_URL = "http://localhost:8601/api/v1/eligibility_agent/Fetch_And_Update_Reference"


def run_compare_test():
    payload = {
        "jsonfilepath": "https://agenticai1.blob.core.windows.net/output-results/AASDEWQ213SF_attachment_Acord_125_extraction_20251149_1226666.json"
    }
    try:
        resp = requests.post(BASE_COMPARE_URL, json=payload, timeout=60)
        print("Compare HTTP Status:", resp.status_code)
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except json.JSONDecodeError:
            print("Response not JSON:", resp.text)
    except Exception as e:
        print("Error calling Compare API:", str(e))


def run_reference_test():
    try:
        resp = requests.post(BASE_REF_URL, timeout=60)
        print("Reference HTTP Status:", resp.status_code)
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except json.JSONDecodeError:
            print("Response not JSON:", resp.text)
    except Exception as e:
        print("Error calling Reference API:", str(e))


if __name__ == "__main__":
    print("Testing /Licensing_&_Sanction_Checker_MCP ...")
    run_compare_test()
    print("\nTesting /Fetch_And_Update_Reference ...")
    run_reference_test()


⸻

.env (example)

AZURE_STORAGE_CONNECTION_STRING=DefaultEndpointsProtocol=...your-connection-string...


⸻

requirements.txt (recommended)

fastapi
uvicorn
fastapi-mcp
python-dotenv
pandas
openpyxl
azure-storage-blob==12.24.0
azure-core
requests


⸻

Quick validations / tips
	1.	Ensure input/ folder is writable by the process. The code creates it if missing.
	2.	When you first run /Fetch_And_Update_Reference, it will create input/reference_stub.xlsx with header Reference Number | Timestamp and append the first found reference with timestamp.
	3.	Timestamp format example: 2025-11-20T16:35:12+05:30.
	4.	If you want the endpoint to accept a custom blob URL (instead of hard-coded BLOB_EXCEL_URL), tell me and I’ll modify file_router.py + handler.py accordingly.

If you want, I can also:
	•	add logging,
	•	add an endpoint variant that returns the updated stub file for download,
	•	or implement a safety check to allow only certain containers.

Which would you like next?
