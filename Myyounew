Thanks — that result: null output means the MCP endpoint returned a JSON-RPC envelope whose result field was null (i.e., None on the server) rather than a dict. I prepared a small, safe fix and additional debug info so the endpoint always returns a proper JSON object and logs helpful errors to STDOUT so you can see what went wrong.

Why this happens (short)
	•	Your endpoint code returns whatever compare_name_with_sanctions returns as result. If that function unexpectedly returns None or raises an exception that is swallowed, the response body will show "result": null.
	•	There may be an exception in the handler that wasn’t propagated (e.g., an error while talking to Azure, parsing Excel, or uploading). Because your router catches exceptions and returns a result that depends on the exception-handling code, it may still produce null.
	•	Also your test printout showed "jsonpc" (typo) — that suggests the response might be malformed at times or you printed raw text earlier. The fix below forces a well-formed JSON-RPC envelope and logs server-side errors.

What I changed
	1.	file_router.py — stronger guarding: if compare_name_with_sanctions returns None the endpoint replaces it with a clear error dict. Also logs the exception traceback when something truly unexpected happens.
	2.	handler.py — no large logic changes; I added a last-resort guard so the function never returns None (always returns a dict), and improved a couple of except branches to return explicit dict errors.
	3.	test_scripts.py — safer output: prints raw response text when JSON parsing fails so you can see server error HTML or tracebacks.

Replace your three files with the versions below and re-run. The server will print more useful messages to the console (where you run uvicorn), and test_scripts.py will show either the JSON or the raw body.

⸻

file_router.py (updated)

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import compare_name_with_sanctions
import traceback
import sys

router = APIRouter()


class CompareRequest(BaseModel):
    jsonfilepath: str = Field(..., description="Full Azure Blob URL to the extracted JSON.")


@router.post(
    "/Licensing_&_Sanction_Checker_MCP",
    operation_id="Licensing_&_Sanction_Checker_MCP",
    summary="Compare JSON (Owner/Insured/Contact/NAMED INSURED(S)) from blob URL with local sanctions CSV."
)
async def compare_files(request: CompareRequest):
    """
    Receives the JSON blob URL from the Data Extraction Agent and performs the comparison.
    Returns a JSON-RPC envelope for consistency with your duplicate attachment checker.
    This wrapper ensures 'result' is always a JSON object (not null) and logs unexpected errors.
    """
    try:
        result = await compare_name_with_sanctions(json_file_url=request.jsonfilepath)
        # Guard: ensure result is a dict (JSON object). If not, convert to error dict.
        if result is None or not isinstance(result, dict):
            result = {"status": False, "error": "Internal: handler returned no result (None or non-dict)."}
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": result
            },
            status_code=200
        )
    except Exception as e:
        # Log traceback server-side for debugging
        tb = traceback.format_exc()
        print("Unexpected error in compare_files:", file=sys.stderr)
        print(tb, file=sys.stderr)
        # Return JSON-RPC with an error object (still in result so client keeps consistent shape)
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": {"status": False, "error": f"Unexpected server error: {str(e)}", "trace": tb}
            },
            status_code=200
        )


⸻

handler.py (minor defensive updates + ensure dict return)

Replace your current handler.py with this (I kept the logic you had previously but made sure all code paths return a dict and added additional error returns where appropriate):

import os
import json
from typing import Optional, Dict, Any
import pandas as pd
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError
from dotenv import load_dotenv
from io import BytesIO
import openpyxl
from datetime import datetime
from zoneinfo import ZoneInfo
import traceback
import sys

from service import _require_env, _parse_blob_url

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
LOCAL_CSV_PATH = os.path.join("input", "sanctions.csv")

FETCHED_EMAILS_EXCEL_BLOB_URL = "https://agenticai1.blob.core.windows.net/fetched-emails-data/Fetched_emails_data_new.xlsx"
TARGET_BLOB_EXCEL_URL = "https://agenticail.blob.core.windows.net/eligibility-results/dummyeligibility.xlsx"
LOCAL_STUBBED_EXCEL_PATH = os.path.join("input", "stubbed.xlsx")


async def compare_name_with_sanctions(json_file_url: str) -> Dict[str, Any]:
    """
    Main flow. Always returns a dict with at least 'status' key.
    """
    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    # Step 1: parse blob url
    try:
        container_name, blob_name = _parse_blob_url(json_file_url)
    except Exception as e:
        return {"status": False, "error": f"Invalid jsonfilepath: {e}"}

    # Step 2: download json blob (async)
    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_name)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"JSON blob not found: container='{container_name}', blob='{blob_name}'"}
            stream = await blob_client.download_blob()
            raw_bytes = await stream.readall()
    except Exception as e:
        tb = traceback.format_exc()
        print("Error connecting to Azure Blob Storage:", file=sys.stderr)
        print(tb, file=sys.stderr)
        return {"status": False, "error": f"Error connecting to Azure Blob Storage: {e}", "trace": tb}

    # Step 3: parse JSON
    try:
        data = json.loads(raw_bytes.decode("utf-8"))
    except Exception as e:
        tb = traceback.format_exc()
        print("Failed to parse JSON:", file=sys.stderr)
        print(tb, file=sys.stderr)
        return {"status": False, "error": f"Failed to parse JSON: {e}", "trace": tb}

    # Step 4: search for name
    target_labels = {"ownername", "insuredname", "contactname", "namedinsured(s)",
                     "applicant/firstnameinsured", "insured/applicant", "namedinsured",
                     "namedinsureds", "firstnameinsured", "applicantname"}
    extracted_name: Optional[str] = None

    def search_nested(data_obj: Any) -> Optional[str]:
        if isinstance(data_obj, dict):
            for key, value in data_obj.items():
                if isinstance(value, dict):
                    res = search_nested(value)
                    if res:
                        return res
                elif isinstance(value, list):
                    for elem in value:
                        if isinstance(elem, dict):
                            res = search_nested(elem)
                            if res:
                                return res
                elif isinstance(key, str):
                    normalized_key = key.lower().replace(" ", "").replace(":", "")
                    if normalized_key in target_labels:
                        val = str(value)
                        return val.split(",")[0].strip()
        return None

    try:
        extracted_name = search_nested(data)
    except Exception as e:
        tb = traceback.format_exc()
        print("Error while searching nested JSON:", file=sys.stderr)
        print(tb, file=sys.stderr)
        return {"status": False, "error": f"Error while searching nested JSON: {e}", "trace": tb}

    # If not found, still attempt excel append (consistent with your earlier design)
    if not extracted_name:
        try:
            excel_status = await _append_and_copy_columns_to_target_blob(extracted_flag=None)
        except Exception as e:
            tb = traceback.format_exc()
            print("Error in excel append after missing name:", file=sys.stderr)
            print(tb, file=sys.stderr)
            excel_status = {"status": False, "error": f"Excel append failed: {e}", "trace": tb}
        return {"status": False, "error": "No matching name field found in JSON.", "excel_update": excel_status}

    # Step 5: read and compare with local CSV
    if not os.path.exists(LOCAL_CSV_PATH):
        try:
            excel_status = await _append_and_copy_columns_to_target_blob(extracted_flag=None)
        except Exception as e:
            tb = traceback.format_exc()
            excel_status = {"status": False, "error": f"Excel append failed: {e}", "trace": tb}
        return {"status": False, "error": f"Local CSV not found at '{LOCAL_CSV_PATH}'", "excel_update": excel_status}

    try:
        csv_df = pd.read_csv(LOCAL_CSV_PATH)
    except Exception as e:
        tb = traceback.format_exc()
        print("Failed to read CSV:", file=sys.stderr)
        print(tb, file=sys.stderr)
        excel_status = await _append_and_copy_columns_to_target_blob(extracted_flag=None)
        return {"status": False, "error": f"Failed to read CSV '{LOCAL_CSV_PATH}': {e}", "trace": tb, "excel_update": excel_status}

    if "entity_name" not in csv_df.columns:
        excel_status = await _append_and_copy_columns_to_target_blob(extracted_flag=None)
        return {"status": False, "error": "CSV must contain 'entity_name' column", "excel_update": excel_status}

    def norm(s: str) -> str:
        return s.strip().lower() if isinstance(s, str) else ""

    entity_set = {norm(x) for x in csv_df["entity_name"].dropna() if isinstance(x, str) and x.strip()}
    is_unique = norm(extracted_name) in entity_set

    results = [{"Extracted Name": extracted_name, "is_Sanctioned": is_unique}]

    # save comparison result (best-effort)
    try:
        os.makedirs("./output", exist_ok=True)
        json_output = "./output/comparison_results.json"
        with open(json_output, "w", encoding="utf-8") as f:
            json.dump({"results": results}, f, indent=4, ensure_ascii=False)
    except Exception as e:
        tb = traceback.format_exc()
        print("Failed saving results:", file=sys.stderr)
        print(tb, file=sys.stderr)
        excel_status = await _append_and_copy_columns_to_target_blob(extracted_flag=is_unique)
        return {"status": False, "error": f"Failed saving results: {e}", "trace": tb, "excel_update": excel_status}

    # final step: append/copy to target blob excel
    try:
        excel_status = await _append_and_copy_columns_to_target_blob(extracted_flag=is_unique)
    except Exception as e:
        tb = traceback.format_exc()
        print("Error in final excel append:", file=sys.stderr)
        print(tb, file=sys.stderr)
        excel_status = {"status": False, "error": f"Excel append failed: {e}", "trace": tb}

    return {"status": True, "results": results, "excel_update": excel_status}


# (The _append_and_copy_columns_to_target_blob implementation should be identical to the one you previously used;
# include the same function body here — to keep this response concise I assume you keep the version that performs:
#  - fetch source excel, get last ref
#  - fetch target blob excel, append ref & timestamp, copy C->J from local stubbed based on boolean
#  - upload back to blob
# Make sure that function also returns dict and catches exceptions and returns a dict.)
# If you want, I can paste that function again verbatim, but it was in your previous handler.py version.

Note: I intentionally left a comment at the end saying include your existing _append_and_copy_columns_to_target_blob function unchanged, because earlier you had that full function and it already did the Excel operations. If you prefer, I can paste the full handler including that function integrated again — say “paste full handler” and I’ll send the complete copy with the function re-inserted.

⸻

test_scripts.py (safer printing)

Replace your test_scripts.py with this: it prints raw response text when resp.json() fails so you’ll see server tracebacks or HTML.

import requests
import json

BASE_URL = "http://localhost:8601/api/v1/eligibility_agent/Licensing_&_Sanction_Checker_MCP"

def run_test():
    payload = {
        "jsonfilepath": "https://agenticai1.blob.core.windows.net/output-results/AASDEWQ213SF_attachment_Acord_125_extraction_20251149_1226666.json"
    }
    try:
        resp = requests.post(BASE_URL, json=payload, timeout=180)
        print("HTTP Status:", resp.status_code)
        # First try to parse JSON; if it fails, print raw text for debugging
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except Exception:
            print("Response not valid JSON. Raw response text below:")
            print(resp.text)
    except Exception as e:
        print("Error calling API:", str(e))

if __name__ == "__main__":
    print("Testing /Licensing_&_Sanction_Checker_MCP ...")
    run_test()


⸻

Next steps for you (quick)
	1.	Replace file_router.py, drop the defensive handler.py in place (or tell me to paste the full handler + helper function again).
	2.	Restart the uvicorn server.
	3.	Run python3 testscripts.py.
	4.	If you still get "result": null — look at the server console (where uvicorn runs). The server should now print a traceback or message explaining the failure. Copy that trace if you want me to inspect it.

If you want, I can now paste the entire handler.py including the _append_and_copy_columns_to_target_blob function exactly as you had it previously but with the defensive changes applied (so you have one complete file to drop in). Say “paste full handler” and I’ll send it.
