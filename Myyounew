Got it — I implemented the bottom-up behavior you asked for: the code now finds the last non-empty cell in column A by scanning upward and writes the new reference one row below it. That guarantees:
	•	If you manually put a value in A1 and A2.. are empty, the new reference will go to A2.
	•	If there are earlier values down the column, the new reference will always be appended directly after the last non-empty cell.
	•	It ignores formatting/borders because it only looks at cell.value.

I updated the project files — only handler.py changed (logic for reading/writing the stubbed Excel). Below are the complete files again (copy/paste). If you already have the rest, you only need to replace handler.py.

⸻

file_router.py

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import compare_name_with_sanctions

router = APIRouter()


class CompareRequest(BaseModel):
    """
    This endpoint is invoked by the Data Extraction Agent.
    It passes the FULL Azure Blob URL of the extracted JSON in 'jsonfilepath'.

    Example payload:
    {
      "jsonfilepath": "https://<account>.blob.core.windows.net/<container>/<path>/file.json"
    }

    The service will:
    - Download that JSON from Azure Blob using AZURE_STORAGE_CONNECTION_STRING,
    - Extract the name from Owner/Insured/Contact/NAMED INSURED(S),
    - Compare it against local 'input/sanctions.csv',
    - Return the comparison result in a JSON-RPC envelope (consistent with your duplicate attachment checker).
    """
    jsonfilepath: str = Field(..., description="Full Azure Blob URL to the extracted JSON.")


@router.post(
    "/Licensing_&_Sanction_Checker_MCP",
    operation_id="Licensing_&_Sanction_Checker_MCP",
    summary="Compare JSON (Owner/Insured/Contact/NAMED INSURED(S)) from blob URL with local sanctions CSV."
)
async def compare_files(request: CompareRequest):
    """
    Receives the JSON blob URL from the Data Extraction Agent and performs the comparison.
    Returns a JSON-RPC envelope for consistency with the duplicate attachment checker.
    """
    try:
        result = await compare_name_with_sanctions(json_file_url=request.jsonfilepath)
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": result
            },
            status_code=200
        )
    except Exception as e:
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": {"status": False, "error": f"Unexpected server error: {str(e)}"}
            },
            status_code=200
        )


⸻

handler.py  (UPDATED — bottom-up append behavior)

import os
import json
from typing import Optional, Dict
import pandas as pd
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError
from dotenv import load_dotenv
from io import BytesIO
import openpyxl

from service import _require_env, _parse_blob_url

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
LOCAL_CSV_PATH = os.path.join("input", "sanctions.csv")

# The blob Excel URL to pull the reference numbers from (as requested by you).
FETCHED_EMAILS_EXCEL_BLOB_URL = "https://agenticai1.blob.core.windows.net/fetched-emails-data/Fetched_emails_data_new.xlsx"

# Local stubbed excel path where we append the reference numbers
LOCAL_STUBBED_EXCEL_PATH = os.path.join("input", "stubbed.xlsx")


async def compare_name_with_sanctions(json_file_url: str) -> Dict:
    """
    Flow:
      1) Parse blob URL -> (container, blob_name)
      2) Download JSON (async)
      3) Dynamically search for target labels in any nested structure
      4) Read local CSV input/sanctions.csv
      5) Case-insensitive exact match against entity_name
      6) Save outputs to ./output and return structured result (with 'status')
      7) Additionally: download fetched-emails Excel from blob, read last value in column A,
         append that reference number to the local stubbed excel (input/stubbed.xlsx).
    """

    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    # -------------------------
    # 1) Download and parse JSON
    # -------------------------
    try:
        container_name, blob_name = _parse_blob_url(json_file_url)
    except Exception as e:
        return {"status": False, "error": f"Invalid jsonfilepath: {e}"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_name)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"JSON blob not found: container='{container_name}', blob='{blob_name}'"}

            stream = await blob_client.download_blob()
            raw_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error connecting to Azure Blob Storage: {e}"}

    # Python dict
    try:
        data = json.loads(raw_bytes.decode("utf-8"))
    except Exception as e:
        return {"status": False, "error": f"Failed to parse JSON: {e}"}

    # Target labels to search for dynamically
    target_labels = {"ownername", "insuredname", "contactname", "namedinsured(s)",
                     "applicant/firstnameinsured", "insured/applicant", "namedinsured",
                     "namedinsureds", "firstnameinsured", "applicantname"}
    extracted_name: Optional[str] = None

    def search_nested(data: dict) -> Optional[str]:
        """
        Recursively search for any field matching target labels in nested dictionaries.
        """
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, dict):  # If the value is a nested dictionary, recurse
                    result = search_nested(value)
                    if result:
                        return result
                elif isinstance(value, list):
                    # If a list, check each element if it's a dict
                    for elem in value:
                        if isinstance(elem, dict):
                            result = search_nested(elem)
                            if result:
                                return result
                elif isinstance(key, str):
                    normalized_key = key.lower().replace(" ", "").replace(":", "")
                    if normalized_key in target_labels:
                        # If value is not string, convert; if contains comma (confidence), pick first part
                        val = str(value)
                        return val.split(",")[0].strip()
        return None

    extracted_name = search_nested(data)

    if not extracted_name:
        # Even if no extracted name, we will still attempt the excel update (user asked both flows run),
        # but we'll return the error for the sanction portion.
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": "No matching name field found in JSON.", "excel_update": excel_status}

    # -------------------------
    # 2) Read local CSV and compare
    # -------------------------
    if not os.path.exists(LOCAL_CSV_PATH):
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": f"Local CSV not found at '{LOCAL_CSV_PATH}'", "excel_update": excel_status}

    try:
        csv_df = pd.read_csv(LOCAL_CSV_PATH)
    except Exception as e:
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": f"Failed to read CSV '{LOCAL_CSV_PATH}': {e}", "excel_update": excel_status}

    if "entity_name" not in csv_df.columns:
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": "CSV must contain 'entity_name' column", "excel_update": excel_status}

    def norm(s: str) -> str:
        return s.strip().lower() if isinstance(s, str) else ""

    entity_set = {norm(x) for x in csv_df["entity_name"].dropna() if isinstance(x, str) and x.strip()}
    is_unique = norm(extracted_name) in entity_set

    results = [{"Extracted Name": extracted_name, "is_Sanctioned": is_unique}]

    try:
        os.makedirs("./output", exist_ok=True)
        json_output = "./output/comparison_results.json"
        with open(json_output, "w", encoding="utf-8") as f:
            json.dump({"results": results}, f, indent=4, ensure_ascii=False)
    except Exception as e:
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": f"Failed saving results: {e}", "excel_update": excel_status}

    # -------------------------
    # 3) Excel append operation (download fetched-emails excel from blob and append last value of column A
    # -------------------------
    excel_status = await _append_reference_from_blob_excel()

    # Return combined status (sanctions + excel update)
    return {"status": True, "results": results, "excel_update": excel_status}


async def _append_reference_from_blob_excel() -> Dict:
    """
    Downloads the predefined fetched-emails Excel from blob, extracts the last non-empty
    value in column A (first worksheet), and appends it into a local stubbed Excel
    located at input/stubbed.xlsx (creating if it doesn't exist).

    Behavior: finds last non-empty cell by scanning UPWARDS (from ws.max_row down to 1),
    and writes new reference to last_non_empty_row + 1. If there are no non-empty cells,
    writes to row 2 (so A1 can be reserved for a header you might have).
    """

    # If Azure connection string not set, return an error
    if not AZURE_STORAGE_CONNECTION_STRING:
        return {"status": False, "error": "Missing AZURE_STORAGE_CONNECTION_STRING for Excel blob download."}

    # Try to parse the blob URL
    try:
        container_name, blob_path = _parse_blob_url(FETCHED_EMAILS_EXCEL_BLOB_URL)
    except Exception as e:
        return {"status": False, "error": f"Invalid fetched emails excel blob URL: {e}"}

    # Download the Excel blob (async)
    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_path)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"Fetched emails Excel blob not found: container='{container_name}', blob='{blob_path}'"}

            stream = await blob_client.download_blob()
            excel_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error downloading fetched emails Excel from blob: {e}"}

    # Read the downloaded excel bytes using openpyxl (via BytesIO)
    try:
        bio = BytesIO(excel_bytes)
        wb = openpyxl.load_workbook(filename=bio, read_only=True, data_only=True)
        ws = wb[wb.sheetnames[0]]  # first worksheet
        # iterate column A from bottom up to find last non-empty cell
        last_value = None
        max_row = ws.max_row or 0
        for row in range(max_row, 0, -1):
            cell = ws.cell(row=row, column=1)
            val = cell.value
            if val is not None and str(val).strip() != "":
                last_value = str(val).strip()
                break
        wb.close()
        if last_value is None:
            return {"status": False, "error": "No non-empty values found in column A of fetched emails Excel."}
    except Exception as e:
        return {"status": False, "error": f"Failed to parse downloaded fetched emails Excel: {e}"}

    # Append the last_value into local stubbed excel using openpyxl to preserve layout
    try:
        # Ensure input directory exists
        os.makedirs(os.path.dirname(LOCAL_STUBBED_EXCEL_PATH), exist_ok=True)

        if os.path.exists(LOCAL_STUBBED_EXCEL_PATH):
            # Open existing workbook (read/write)
            wb_local = openpyxl.load_workbook(LOCAL_STUBBED_EXCEL_PATH)
            ws_local = wb_local[wb_local.sheetnames[0]]

            # Find last non-empty row in column A by scanning upwards (ignores formatting)
            last_non_empty = 0
            # Use ws_local.max_row for upper bound, but scan downwards to 1
            local_max = ws_local.max_row or 0
            for r in range(local_max, 0, -1):
                v = ws_local.cell(row=r, column=1).value
                if v is not None and (not (isinstance(v, str) and v.strip() == "")):
                    last_non_empty = r
                    break

            # Decide write_row:
            # - If found a last_non_empty (>=1), write to last_non_empty + 1
            # - If none found (column A empty), write to row 2 (reserve A1 for header)
            if last_non_empty >= 1:
                write_row = last_non_empty + 1
            else:
                write_row = 2

            ws_local.cell(row=write_row, column=1, value=last_value)
            wb_local.save(LOCAL_STUBBED_EXCEL_PATH)
            wb_local.close()
            return {"status": True, "message": f"Appended reference '{last_value}' to stubbed excel at '{LOCAL_STUBBED_EXCEL_PATH}' (row {write_row})."}
        else:
            # Create new workbook and write the value into A2 (leave A1 empty)
            wb_local = openpyxl.Workbook()
            ws_local = wb_local.active
            ws_local.cell(row=2, column=1, value=last_value)
            wb_local.save(LOCAL_STUBBED_EXCEL_PATH)
            wb_local.close()
            return {"status": True, "message": f"Created new stubbed excel and wrote reference '{last_value}' to '{LOCAL_STUBBED_EXCEL_PATH}' (row 2)."}
    except Exception as e:
        return {"status": False, "error": f"Failed to update local stubbed excel '{LOCAL_STUBBED_EXCEL_PATH}': {e}"}


⸻

main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn

from file_router import router as file_router


def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )


def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    sub = FastAPI(title=title, description=description, version=version)
    apply_cors(sub)
    return sub


app = FastAPI()
apply_cors(app)

file_app = create_sub_app(
    title="Licensing & Sanction Checker MCP",
    description="Receives JSON blob URL from Data Extraction Agent and compares extracted name with local sanctions CSV."
)
file_app.include_router(file_router)

# Expose ONLY this operation id via MCP HTTP (client will use transport='http')
FastApiMCP(file_app, include_operations=["Licensing_&_Sanction_Checker_MCP"]).mount_http()

# Mount the sub-app under this prefix (keep as-is per your request)
app.mount("/api/v1/eligibility_agent", file_app)


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8601)


⸻

service.py

import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")


def _require_env() -> None:
    """Ensure required environment variables exist."""
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING in .env")


def _parse_blob_url(url: str) -> tuple[str, str]:
    """
    Parse an Azure Blob URL and return (container_name, blob_path).
    Example URL:
      https://account.blob.core.windows.net/output-results/folder/name.json
    Returns:
      ("output-results", "folder/name.json")
    """
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("jsonfilepath must be a valid http(s) URL to Azure Blob Storage.")

    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("jsonfilepath must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)


⸻

test_scripts.py

import requests
import json

BASE_URL = "http://localhost:8601/api/v1/eligibility_agent/Licensing_&_Sanction_Checker_MCP"

def run_test():
    payload = {
          "jsonfilepath": "https://agenticai1.blob.core.windows.net/output-results/AASDEWQ213SF_attachment_Acord_125_extraction_20251149_1226666.json"
    }
    try:
        resp = requests.post(BASE_URL, json=payload, timeout=180)
        print("HTTP Status:", resp.status_code)
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except json.JSONDecodeError:
            print("Response not JSON:", resp.text)
    except Exception as e:
        print("Error calling API:", str(e))

if __name__ == "__main__":
    print("Testing /Licensing_&_Sanction_Checker_MCP ...")
    run_test()


⸻

Short summary of the change
	•	For the local stubbed workbook, we now find the last non-empty row in column A by scanning upwards (from max_row down to 1) and write to last_non_empty + 1. If none exist, we write to row 2 (so A1 stays as your header or manually populated value).
	•	For the fetched-excel (source) we still find the last non-empty cell by scanning upward (so we pick the latest reference).

If you want me to also add a dedupe check (skip appending if that reference already exists in stubbed.xlsx) or to log the row number in ./output JSON for easier debugging, tell me and I’ll add it.
