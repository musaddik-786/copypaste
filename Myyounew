Nice — I updated your project to add the requested Excel flow (download the fetched-emails Excel from blob, read the last value in column A, and append that value into a local stubbed Excel in input/). I placed the new logic inside the same MCP endpoint (no new endpoint), and I kept the existing sanction-checking flow intact. I used openpyxl for safe Excel append operations and azure.storage.blob.aio for async blob downloads.

Below you’ll find the complete updated code for all files you provided. Important notes after the code blocks.

⸻

file_router.py

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import compare_name_with_sanctions

router = APIRouter()


class CompareRequest(BaseModel):
    """
    This endpoint is invoked by the Data Extraction Agent.
    It passes the FULL Azure Blob URL of the extracted JSON in 'jsonfilepath'.

    Example payload:
    {
      "jsonfilepath": "https://<account>.blob.core.windows.net/<container>/<path>/file.json"
    }

    The service will:
    - Download that JSON from Azure Blob using AZURE_STORAGE_CONNECTION_STRING,
    - Extract the name from Owner/Insured/Contact/NAMED INSURED(S),
    - Compare it against local 'input/sanctions.csv',
    - Return the comparison result in a JSON-RPC envelope (consistent with your duplicate attachment checker).
    """
    jsonfilepath: str = Field(..., description="Full Azure Blob URL to the extracted JSON.")


@router.post(
    "/Licensing_&_Sanction_Checker_MCP",
    operation_id="Licensing_&_Sanction_Checker_MCP",
    summary="Compare JSON (Owner/Insured/Contact/NAMED INSURED(S)) from blob URL with local sanctions CSV."
)
async def compare_files(request: CompareRequest):
    """
    Receives the JSON blob URL from the Data Extraction Agent and performs the comparison.
    Returns a JSON-RPC envelope for consistency with the duplicate attachment checker.
    """
    try:
        result = await compare_name_with_sanctions(json_file_url=request.jsonfilepath)
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": result
            },
            status_code=200
        )
    except Exception as e:
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": {"status": False, "error": f"Unexpected server error: {str(e)}"}
            },
            status_code=200
        )


⸻

handler.py

import os
import json
from typing import Optional, Dict
import pandas as pd
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError
from dotenv import load_dotenv
from io import BytesIO
import openpyxl

from service import _require_env, _parse_blob_url

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
LOCAL_CSV_PATH = os.path.join("input", "sanctions.csv")

# The blob Excel URL to pull the reference numbers from (as requested by you).
# You can also move this to .env if you prefer.
FETCHED_EMAILS_EXCEL_BLOB_URL = "https://agenticai1.blob.core.windows.net/fetched-emails-data/Fetched_emails_data_new.xlsx"

# Local stubbed excel path where we append the reference numbers
LOCAL_STUBBED_EXCEL_PATH = os.path.join("input", "stubbed.xlsx")


async def compare_name_with_sanctions(json_file_url: str) -> Dict:
    """
    Flow:
      1) Parse blob URL -> (container, blob_name)
      2) Download JSON (async)
      3) Dynamically search for target labels in any nested structure
      4) Read local CSV input/sanctions.csv
      5) Case-insensitive exact match against entity_name
      6) Save outputs to ./output and return structured result (with 'status')
      7) Additionally: download fetched-emails Excel from blob, read last value in column A,
         append that reference number to the local stubbed excel (input/stubbed.xlsx).
    """

    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    # -------------------------
    # 1) Download and parse JSON
    # -------------------------
    try:
        container_name, blob_name = _parse_blob_url(json_file_url)
    except Exception as e:
        return {"status": False, "error": f"Invalid jsonfilepath: {e}"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_name)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"JSON blob not found: container='{container_name}', blob='{blob_name}'"}

            stream = await blob_client.download_blob()
            raw_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error connecting to Azure Blob Storage: {e}"}

    # Python dict
    try:
        data = json.loads(raw_bytes.decode("utf-8"))
    except Exception as e:
        return {"status": False, "error": f"Failed to parse JSON: {e}"}

    # Target labels to search for dynamically
    target_labels = {"ownername", "insuredname", "contactname", "namedinsured(s)",
                     "applicant/firstnameinsured", "insured/applicant", "namedinsured",
                     "namedinsureds", "firstnameinsured", "applicantname"}
    extracted_name: Optional[str] = None

    def search_nested(data: dict) -> Optional[str]:
        """
        Recursively search for any field matching target labels in nested dictionaries.
        """
        if isinstance(data, dict):
            for key, value in data.items():
                if isinstance(value, dict):  # If the value is a nested dictionary, recurse
                    result = search_nested(value)
                    if result:
                        return result
                elif isinstance(value, list):
                    # If a list, check each element if it's a dict
                    for elem in value:
                        if isinstance(elem, dict):
                            result = search_nested(elem)
                            if result:
                                return result
                elif isinstance(key, str):
                    normalized_key = key.lower().replace(" ", "").replace(":", "")
                    if normalized_key in target_labels:
                        # If value is not string, convert; if contains comma (confidence), pick first part
                        val = str(value)
                        return val.split(",")[0].strip()
        return None

    extracted_name = search_nested(data)

    if not extracted_name:
        # Even if no extracted name, we will still attempt the excel update (user asked both flows run),
        # but we'll return the error for the sanction portion.
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": "No matching name field found in JSON.", "excel_update": excel_status}

    # -------------------------
    # 2) Read local CSV and compare
    # -------------------------
    if not os.path.exists(LOCAL_CSV_PATH):
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": f"Local CSV not found at '{LOCAL_CSV_PATH}'", "excel_update": excel_status}

    try:
        csv_df = pd.read_csv(LOCAL_CSV_PATH)
    except Exception as e:
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": f"Failed to read CSV '{LOCAL_CSV_PATH}': {e}", "excel_update": excel_status}

    if "entity_name" not in csv_df.columns:
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": "CSV must contain 'entity_name' column", "excel_update": excel_status}

    def norm(s: str) -> str:
        return s.strip().lower() if isinstance(s, str) else ""

    entity_set = {norm(x) for x in csv_df["entity_name"].dropna() if isinstance(x, str) and x.strip()}
    is_unique = norm(extracted_name) in entity_set

    results = [{"Extracted Name": extracted_name, "is_Sanctioned": is_unique}]

    try:
        os.makedirs("./output", exist_ok=True)
        json_output = "./output/comparison_results.json"
        with open(json_output, "w", encoding="utf-8") as f:
            json.dump({"results": results}, f, indent=4, ensure_ascii=False)
    except Exception as e:
        excel_status = await _append_reference_from_blob_excel()
        return {"status": False, "error": f"Failed saving results: {e}", "excel_update": excel_status}

    # -------------------------
    # 3) Excel append operation (download fetched-emails excel from blob and append last value of column A
    # -------------------------
    excel_status = await _append_reference_from_blob_excel()

    # Return combined status (sanctions + excel update)
    return {"status": True, "results": results, "excel_update": excel_status}


async def _append_reference_from_blob_excel() -> Dict:
    """
    Downloads the predefined fetched-emails Excel from blob, extracts the last non-empty
    value in column A (first worksheet), and appends it into a local stubbed Excel
    located at input/stubbed.xlsx (creating if it doesn't exist).

    Returns dict with status and message (for inclusion in main response).
    """

    # If Azure connection string not set, return an error
    if not AZURE_STORAGE_CONNECTION_STRING:
        return {"status": False, "error": "Missing AZURE_STORAGE_CONNECTION_STRING for Excel blob download."}

    # Try to parse the blob URL
    try:
        container_name, blob_path = _parse_blob_url(FETCHED_EMAILS_EXCEL_BLOB_URL)
    except Exception as e:
        return {"status": False, "error": f"Invalid fetched emails excel blob URL: {e}"}

    # Download the Excel blob (async)
    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_path)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"Fetched emails Excel blob not found: container='{container_name}', blob='{blob_path}'"}

            stream = await blob_client.download_blob()
            excel_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error downloading fetched emails Excel from blob: {e}"}

    # Read the downloaded excel bytes using openpyxl (via BytesIO)
    try:
        bio = BytesIO(excel_bytes)
        wb = openpyxl.load_workbook(filename=bio, read_only=True, data_only=True)
        ws = wb[wb.sheetnames[0]]  # first worksheet
        # iterate column A from bottom up to find last non-empty cell
        last_value = None
        # Efficient scan: find max row and walk backward
        max_row = ws.max_row or 0
        for row in range(max_row, 0, -1):
            cell = ws.cell(row=row, column=1)
            val = cell.value
            if val is not None and str(val).strip() != "":
                last_value = str(val).strip()
                break
        wb.close()
        if last_value is None:
            return {"status": False, "error": "No non-empty values found in column A of fetched emails Excel."}
    except Exception as e:
        return {"status": False, "error": f"Failed to parse downloaded fetched emails Excel: {e}"}

    # Append the last_value into local stubbed excel using openpyxl to preserve layout
    try:
        # Ensure input directory exists
        os.makedirs(os.path.dirname(LOCAL_STUBBED_EXCEL_PATH), exist_ok=True)

        if os.path.exists(LOCAL_STUBBED_EXCEL_PATH):
            # Open existing workbook (read/write)
            wb_local = openpyxl.load_workbook(LOCAL_STUBBED_EXCEL_PATH)
            ws_local = wb_local[wb_local.sheetnames[0]]
            # Find the next row to append
            next_row = ws_local.max_row + 1
            # If the sheet is empty (max_row == 1 but cell A1 might be empty), adjust
            if next_row == 1 and (ws_local.cell(row=1, column=1).value is None):
                next_row = 1
            ws_local.cell(row=next_row, column=1, value=last_value)
            wb_local.save(LOCAL_STUBBED_EXCEL_PATH)
            wb_local.close()
            return {"status": True, "message": f"Appended reference '{last_value}' to existing stubbed excel at '{LOCAL_STUBBED_EXCEL_PATH}'."}
        else:
            # Create new workbook and write header-less value in A1 (user wanted column A populated)
            wb_local = openpyxl.Workbook()
            ws_local = wb_local.active
            ws_local.cell(row=1, column=1, value=last_value)
            wb_local.save(LOCAL_STUBBED_EXCEL_PATH)
            wb_local.close()
            return {"status": True, "message": f"Created new stubbed excel and wrote reference '{last_value}' to '{LOCAL_STUBBED_EXCEL_PATH}'."}
    except Exception as e:
        return {"status": False, "error": f"Failed to update local stubbed excel '{LOCAL_STUBBED_EXCEL_PATH}': {e}"}


⸻

main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn

from file_router import router as file_router


def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )


def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    sub = FastAPI(title=title, description=description, version=version)
    apply_cors(sub)
    return sub


app = FastAPI()
apply_cors(app)

file_app = create_sub_app(
    title="Licensing & Sanction Checker MCP",
    description="Receives JSON blob URL from Data Extraction Agent and compares extracted name with local sanctions CSV."
)
file_app.include_router(file_router)

# Expose ONLY this operation id via MCP HTTP (client will use transport='http')
FastApiMCP(file_app, include_operations=["Licensing_&_Sanction_Checker_MCP"]).mount_http()

# Mount the sub-app under this prefix (keep as-is per your request)
app.mount("/api/v1/eligibility_agent", file_app)


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8601)


⸻

service.py

import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")


def _require_env() -> None:
    """Ensure required environment variables exist."""
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING in .env")


def _parse_blob_url(url: str) -> tuple[str, str]:
    """
    Parse an Azure Blob URL and return (container_name, blob_path).
    Example URL:
      https://account.blob.core.windows.net/output-results/folder/name.json
    Returns:
      ("output-results", "folder/name.json")
    """
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("jsonfilepath must be a valid http(s) URL to Azure Blob Storage.")

    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("jsonfilepath must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)


⸻

test_scripts.py

import requests
import json

BASE_URL = "http://localhost:8601/api/v1/eligibility_agent/Licensing_&_Sanction_Checker_MCP"

def run_test():
    payload = {
          "jsonfilepath": "https://agenticai1.blob.core.windows.net/output-results/AASDEWQ213SF_attachment_Acord_125_extraction_20251149_1226666.json"
    }
    try:
        resp = requests.post(BASE_URL, json=payload, timeout=120)
        print("HTTP Status:", resp.status_code)
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except json.JSONDecodeError:
            print("Response not JSON:", resp.text)
    except Exception as e:
        print("Error calling API:", str(e))

if __name__ == "__main__":
    print("Testing /Licensing_&_Sanction_Checker_MCP ...")
    run_test()


⸻

Important notes & assumptions
	1.	Local stubbed Excel file name & location: I used input/stubbed.xlsx as the local stub file. If you prefer a different filename (for example input/stubbed_input.xlsx or input/stubbed_emails.xlsx), change the LOCAL_STUBBED_EXCEL_PATH variable in handler.py.
	2.	Fetched Excel blob URL: I used the exact blob URL you provided:

https://agenticai1.blob.core.windows.net/fetched-emails-data/Fetched_emails_data_new.xlsx

If you’d rather read the Excel from a dynamic blob URL sent in the request payload, we can change the code to accept that — but you explicitly asked not to add an endpoint, so I left it as a fixed URL constant. If you want it moved to .env, tell me and I’ll show code to read it from FETCHED_EMAILS_EXCEL_BLOB_URL environment variable.

	3.	Pip packages:
	•	openpyxl (for Excel read/write)
	•	pandas
	•	azure-storage-blob (the async package; typically azure-storage-blob>=12.x)
	•	python-dotenv
	•	fastapi, uvicorn, fastapi_mcp, etc.
Make sure your environment has these installed:

pip install openpyxl pandas azure-storage-blob python-dotenv fastapi uvicorn fastapi-mcp


	4.	Behavior details:
	•	The code finds the last non-empty cell in Column A of the fetched-emails Excel and writes that value into the first sheet of the local stubbed excel at the next empty row in Column A.
	•	If the local stubbed excel does not exist, the handler creates it and writes the value into cell A1.
	•	The sanction-check result and the excel update result are both included in the JSON returned by the endpoint under result. Example:

{
  "status": true,
  "results": [ ... ],
  "excel_update": { "status": true, "message": "Appended reference 'REF123' ..." }
}


	•	If the sanction flow fails but the excel update runs, the response includes the sanction error and the excel update status.

	5.	Async vs sync caveat:
	•	Excel file operations with openpyxl are synchronous (blocking). They are executed inside the async function (this is common and acceptable for short operations). If your workload needs high throughput you may want to run the Excel work in a threadpool (e.g., run_in_executor) — I can add that if you want.
	6.	Edge cases:
	•	If the fetched Excel has only empty cells in column A, the excel update returns an error and no append is made.
	•	Values from column A are coerced to str when appended (to avoid type confusion).

⸻

If you’d like, I can:
	•	Change the fetched-excel blob URL to be read from .env.
	•	Make the local stub file name configurable via .env.
	•	Add deduplication logic (skip appending if the same reference already exists in the stubbed file).
	•	Run the Excel update in a background thread to avoid blocking the async loop.

Tell me which of the above (if any) you want and I’ll modify the code accordingly.
