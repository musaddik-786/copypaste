# analyze_local_singlefile.py
"""
Single-file demo:
- Reads a local document file (path from .env)
- Sends it to Azure Document Intelligence custom model
- Converts result into JSON-serializable dict and saves to disk (OUTPUT_JSON from .env)
- Has detailed exception handling and helpful error messages
"""

import os
import sys
import json
import traceback
from pathlib import Path
from typing import Any, Dict, List, IO
from io import BytesIO

from dotenv import load_dotenv

# Azure SDK imports
try:
    from azure.core.credentials import AzureKeyCredential
    from azure.ai.documentintelligence import DocumentIntelligenceClient
except Exception as e:
    # If SDK import fails, show a friendly message and re-raise
    print("ERROR: Failed to import Azure SDK modules required by this script.")
    print("Make sure you installed the required packages (see requirements.txt).")
    print("Run: pip install -r requirements.txt")
    print("\nDetailed import error:")
    traceback.print_exc()
    raise

# -----------------------
# Helper: environment
# -----------------------
def load_configuration() -> Dict[str, str]:
    load_dotenv()
    config = {
        "FORM_RECOGNIZER_ENDPOINT": os.getenv("FORM_RECOGNIZER_ENDPOINT", "").strip(),
        "FORM_RECOGNIZER_KEY": os.getenv("FORM_RECOGNIZER_KEY", "").strip(),
        "MODEL_ID": os.getenv("MODEL_ID", "").strip(),
        "LOCAL_DOCUMENT": os.getenv("LOCAL_DOCUMENT", "").strip(),
        "OUTPUT_JSON": os.getenv("OUTPUT_JSON", "./output/analysis_result.json").strip(),
        "PRINT_TO_CONSOLE": os.getenv("PRINT_TO_CONSOLE", "true").strip().lower(),
    }
    return config

# -----------------------
# Helper: local file functions
# -----------------------
def get_local_file_path(path_str: str) -> Path:
    if not path_str:
        raise ValueError("LOCAL_DOCUMENT path is required but empty. Set LOCAL_DOCUMENT in .env.")
    p = Path(path_str).expanduser().resolve()
    return p

def open_local_file_stream(path: Path) -> IO[bytes]:
    if not path.exists():
        raise FileNotFoundError(f"File not found: {path}")
    if not path.is_file():
        raise FileNotFoundError(f"Not a file: {path}")
    return open(path, "rb")

# -----------------------
# Azure Document Intelligence client & analyze
# -----------------------
def create_client(endpoint: str, key: str) -> DocumentIntelligenceClient:
    if not endpoint or not key:
        raise ValueError("Both FORM_RECOGNIZER_ENDPOINT and FORM_RECOGNIZER_KEY are required.")
    client = DocumentIntelligenceClient(endpoint=endpoint, credential=AzureKeyCredential(key))
    return client

def analyze_local_file(client: DocumentIntelligenceClient, model_id: str, file_stream: IO[bytes]):
    if not model_id:
        raise ValueError("MODEL_ID is required (set MODEL_ID in .env).")
    poller = client.begin_analyze_document(model_id, document=file_stream)
    result = poller.result()
    return result

# -----------------------
# Convert SDK AnalyzeResult to plain dict (JSON serializable)
# -----------------------
def analyze_result_to_dict(result) -> Dict[str, Any]:
    """
    Convert the SDK AnalyzeResult object into a JSON-serializable dictionary.
    We avoid importing type names like AnalyzeResult to prevent import errors.
    """
    out: Dict[str, Any] = {}
    out["model_id"] = getattr(result, "model_id", None)

    # Documents / fields
    docs: List[Dict[str, Any]] = []
    for doc in getattr(result, "documents", []) or []:
        docd: Dict[str, Any] = {
            "doc_type": getattr(doc, "doc_type", None),
            "confidence": getattr(doc, "confidence", None),
            "fields": {}
        }
        for name, field in (getattr(doc, "fields", {}) or {}).items():
            docd["fields"][name] = {
                "content": getattr(field, "content", None),
                "confidence": getattr(field, "confidence", None),
                "type": getattr(field, "type", None)
            }
        docs.append(docd)
    out["documents"] = docs

    # Pages
    pages_list: List[Dict[str, Any]] = []
    for page in getattr(result, "pages", []) or []:
        pd = {
            "page_number": getattr(page, "page_number", None),
            "width": getattr(page, "width", None),
            "height": getattr(page, "height", None),
            "unit": getattr(page, "unit", None),
            "lines": [getattr(line, "content", None) for line in (getattr(page, "lines", []) or [])],
            "words": [
                {"content": getattr(w, "content", None), "confidence": getattr(w, "confidence", None)}
                for w in (getattr(page, "words", []) or [])
            ],
        }
        if getattr(page, "selection_marks", None):
            pd["selection_marks"] = [
                {"state": getattr(sm, "state", None), "confidence": getattr(sm, "confidence", None)}
                for sm in (getattr(page, "selection_marks", []) or [])
            ]
        pages_list.append(pd)
    out["pages"] = pages_list

    # Tables
    tables_out: List[Dict[str, Any]] = []
    for table in (getattr(result, "tables", []) or []):
        t = {
            "row_count": getattr(table, "row_count", None),
            "column_count": getattr(table, "column_count", None),
            "bounding_regions": [getattr(r, "page_number", None) for r in (getattr(table, "bounding_regions", []) or [])],
            "cells": []
        }
        for cell in (getattr(table, "cells", []) or []):
            t["cells"].append({
                "row_index": getattr(cell, "row_index", None),
                "column_index": getattr(cell, "column_index", None),
                "content": getattr(cell, "content", None),
                "row_span": getattr(cell, "row_span", None),
                "column_span": getattr(cell, "column_span", None)
            })
        tables_out.append(t)
    out["tables"] = tables_out

    # Raw text
    if getattr(result, "content", None):
        out["raw_text"] = getattr(result, "content", None)

    return out

# -----------------------
# Save JSON to path
# -----------------------
def save_json_to_path(data: Dict[str, Any], path_str: str) -> Path:
    p = Path(path_str).expanduser().resolve()
    if not p.parent.exists():
        p.parent.mkdir(parents=True, exist_ok=True)
    with p.open("w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)
    return p

# -----------------------
# Optional console printer
# -----------------------
def pretty_print_result(data: Dict[str, Any]) -> None:
    print("\n=== Summary ===")
    print(f"Model ID: {data.get('model_id')}")
    docs = data.get("documents", [])
    for i, doc in enumerate(docs, start=1):
        print(f"\nDocument #{i}: type={doc.get('doc_type')} conf={doc.get('confidence')}")
        fields = doc.get("fields", {})
        if fields:
            print(" Fields:")
            for name, info in fields.items():
                print(f"  - {name}: {info.get('content')} (conf: {info.get('confidence')})")
        else:
            print(" No fields found.")
    pages = data.get("pages", [])
    print(f"\nPages found: {len(pages)}")
    if pages:
        first_page = pages[0]
        lines = first_page.get("lines", [])
        if lines:
            print("\nFirst page lines (first 10):")
            for line in lines[:10]:
                print("  " + (line or ""))
    print("\n=== End Summary ===\n")

# -----------------------
# Main
# -----------------------
def main():
    try:
        cfg = load_configuration()

        # Validate
        required = ["FORM_RECOGNIZER_ENDPOINT", "FORM_RECOGNIZER_KEY", "MODEL_ID", "LOCAL_DOCUMENT"]
        missing = [k for k in required if not cfg.get(k)]
        if missing:
            print(f"ERROR: Missing required env vars: {', '.join(missing)}")
            print("Copy .env.example to .env and fill values, or set environment variables.")
            sys.exit(1)

        local_path = get_local_file_path(cfg["LOCAL_DOCUMENT"])
        print(f"Local document path resolved to: {local_path}")

        # open file and send to AI
        with open_local_file_stream(local_path) as f:
            client = create_client(cfg["FORM_RECOGNIZER_ENDPOINT"], cfg["FORM_RECOGNIZER_KEY"])
            print("Submitting document for analysis (this may take a few seconds)...")
            result = analyze_local_file(client, cfg["MODEL_ID"], f)

        # convert and save
        result_dict = analyze_result_to_dict(result)
        saved_path = save_json_to_path(result_dict, cfg["OUTPUT_JSON"])
        print(f"Analysis saved to JSON at: {saved_path}")

        # optional print
        if cfg["PRINT_TO_CONSOLE"] in ("1", "true", "yes"):
            pretty_print_result(result_dict)

    except Exception as ex:
        print("\nAN ERROR OCCURRED:")
        # full traceback
        traceback.print_exc()
        # friendly hints for common problems
        err_msg = str(ex).lower()
        if "authentication failed" in err_msg or "unauthorized" in err_msg or "403" in err_msg:
            print("\nHINT: Authentication failed. Check FORM_RECOGNIZER_ENDPOINT and FORM_RECOGNIZER_KEY in your .env.")
        if "model_id" in err_msg or "model" in err_msg:
            print("\nHINT: Validate MODEL_ID — make sure the custom model exists & model id is correct.")
        if "file not found" in err_msg or "no such file" in err_msg:
            print("\nHINT: Check LOCAL_DOCUMENT path and that the file exists.")
        if "unsupported media type" in err_msg or "415" in err_msg:
            print("\nHINT: The file may be an unsupported format — Azure Document Intelligence accepts PDF, TIFF, JPEG, PNG typically.")
        sys.exit(2)

if __name__ == "__main__":
    main()
