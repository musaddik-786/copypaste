image_processor.py



import os
import json
import uuid
from datetime import datetime
from dotenv import load_dotenv

from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError

from service import _require_env, get_env_values, AZURE_STORAGE_CONNECTION_STRING, _parse_blob_url

load_dotenv()


async def analyze_file_async(bloburl: str) -> dict:
    """
    Download the PDF from bloburl, analyze it with Azure Document Intelligence,
    save the JSON locally inside ./output/output.json,
    and also upload the same JSON to Azure Blob Storage (output-results container).
    """

    
    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    endpoint, key, model_id = get_env_values()

   
    try:
        src_container, src_blob = _parse_blob_url(bloburl)
    except Exception as e:
        return {"status": False, "error": f"Invalid BlobUrl: {e}"}

    source_file_name = os.path.basename(src_blob) if src_blob else "unknown.pdf"
    source_file_name = os.path.basename(source_file_name)

  
    if not AZURE_STORAGE_CONNECTION_STRING:
        return {"status": False, "error": "Missing AZURE_STORAGE_CONNECTION_STRING in .env"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(src_container)
            blob_client = container_client.get_blob_client(src_blob)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"PDF blob not found: container='{src_container}', blob='{src_blob}'"}

            stream = await blob_client.download_blob()
            pdf_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error downloading PDF from Blob Storage: {e}"}

  
    # input_path = os.path.join(os.getcwd(), source_file_name)
    # try:
    #     with open(input_path, "wb") as wf:
    #         wf.write(pdf_bytes)
    # except Exception as e:
    #     return {"status": False, "error": f"Failed to write local input file '{input_path}': {e}"}
    input_dir = os.path.join(os.getcwd(), "input")
    os.makedirs(input_dir, exist_ok=True)

    input_path = os.path.join(input_dir, source_file_name)

    try:
        with open(input_path, "wb") as wf:
            wf.write(pdf_bytes)
    except Exception as e:
        return {"status": False, "error": f"Failed to write local input file '{input_path}': {e}"}

   
    print(" Connecting to Azure Document Intelligence service...")
    try:
        client = DocumentIntelligenceClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(key)
        )
    except Exception as e:
        return {"status": False, "error": f"Failed to create DocumentIntelligenceClient: {e}"}

    print(f"Analyzing '{source_file_name}' using model '{model_id}'...")
    try:
        with open(input_path, "rb") as f:
            poller = client.begin_analyze_document(model_id=model_id, body=f)
            result = poller.result()
    except Exception as e:
        return {"status": False, "error": f"Error during document analysis: {e}"}

    output_dir = os.path.join(os.getcwd(), "output")
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "output.json")

    json_data = result.as_dict()
    try:
        with open(output_path, "w", encoding="utf-8") as wf:
            json.dump(json_data, wf, indent=2, ensure_ascii=False)
    except Exception as e:
        return {"status": False, "error": f"Failed to save local output.json: {e}"}

    
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    unique = uuid.uuid4().hex
    output_container = "output-results"
    target_blob_name = f"{timestamp}_{unique}_extracted.json"

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            out_container_client = blob_service.get_container_client(output_container)
            try:
                await out_container_client.create_container()
            except Exception:
                pass 

            out_blob_client = out_container_client.get_blob_client(target_blob_name)

            with open(output_path, "rb") as data:
                await out_blob_client.upload_blob(data, overwrite=True)
    except Exception as e:
        return {"status": False, "error": f"Failed to upload JSON to output-results: {e}"}

    parsed = bloburl.split("://", 1)[-1]
    account_and_rest = parsed.split("/", 1)[0]
    output_blob_url = f"https://{account_and_rest}/{output_container}/{target_blob_name}"

    print(f"JSON saved locally and uploaded to: {output_blob_url}")


    return {
        # "status": True,
        "source_file": source_file_name,
        # "local_output_json": output_path,
        "output_blob_url": output_blob_url
    }


async def process_input_folder_on_startup() -> None:
    """Background startup function — checks ./input for any PDFs and logs them."""
    try:
        input_dir = os.path.join(os.getcwd(), "input")
        if not os.path.exists(input_dir):
            return

        files = [f for f in os.listdir(input_dir) if f.lower().endswith(".pdf")]
        if not files:
            return

        for pdf in files:
            print(f"[startup] Found local PDF in ./input: {pdf} — no automatic processing.")
    except Exception as e:
        print(f"[startup] process_input_folder_on_startup error: {e}")
        return



router.py


from dotenv import load_dotenv
load_dotenv()

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field, validator

from image_processor import analyze_file_async

router = APIRouter()


class DocumentExtractMCP(BaseModel):
    """Trigger document extraction for a PDF file located in blob storage."""
    AgentName: str = Field(
        default="DocumentExtractAgent",
        description="The unique agent name of the agent that is being called"
    )
    BlobUrl: str = Field(
        ...,
        description="Required: full blob URL (https://...) or container/blob path (container/path/to/file.pdf) to analyze"
    )

    @validator("BlobUrl")
    def bloburl_must_not_be_empty(cls, v: str) -> str:
        if not v or not v.strip():
            raise ValueError("BlobUrl must be a non-empty string pointing to the blob (full URL or container/blob path).")
        return v.strip()


@router.post("/document_extract_mcp", operation_id="document_extract_mcp")
async def document_extract_mcp(p_body: DocumentExtractMCP):
    """
    Analyze a PDF from Azure Blob storage using image_processor.analyze_file_async and
    return the analyzer's structured result.

    Body:

        AgentName (str): Agent name (optional)

        BlobUrl (str): Required full blob URL or container/blob path.
    """
    try:
        result = await analyze_file_async(bloburl=p_body.BlobUrl)

        return JSONResponse(content={
            "jsonrpc": "2.0",
            "id": 1,
            "result": result
        })

    except Exception as e:
        return JSONResponse(status_code=500, content={"error": f"Extraction failed: {e}"})



service.py



import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

ENDPOINT = os.getenv("ENDPOINT", "")
KEY = os.getenv("KEY", "")
MODEL_ID = os.getenv("MODEL_ID", "")
AZURE_STORAGE_CONNECTION_STRING = os.getenv("AZURE_STORAGE_CONNECTION_STRING", "")


def _require_env() -> None:
    """Ensure required environment variables exist."""
    if not (ENDPOINT and KEY and MODEL_ID):
        raise RuntimeError("Missing ENDPOINT, KEY, or MODEL_ID in .env file.")
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING in .env file.")


def get_env_values():
    """Return endpoint, key, model_id"""
    return ENDPOINT, KEY, MODEL_ID


def _parse_blob_url(url: str) -> tuple[str, str]:
    """
    Parse an Azure Blob URL and return (container_name, blob_path).
    Example URL:
      https://account.blob.core.windows.net/output-results/folder/name.json
    Returns:
      ("output-results", "folder/name.json")
    """
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("BlobUrl must be a valid http(s) URL to Azure Blob Storage.")
    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("BlobUrl must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)



main.py



from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn
from contextlib import asynccontextmanager
import asyncio
import os

from router import router as document_router
from image_processor import process_input_folder_on_startup

async def _start_processing_task() -> None:
    """
    Create and store a background task that runs the processor once at startup.
    This returns the created task (already scheduled).
    """
    task = asyncio.create_task(process_input_folder_on_startup())
    return task

async def _stop_processing_task(task: asyncio.Task) -> None:
    """
    Cancel and await the background task if it's still running.
    """
    if task and not task.done():
        task.cancel()
        try:
            await task
        except Exception:
            pass

@asynccontextmanager
async def lifespan(app: FastAPI):
    try:
        app.state.processing_task = await _start_processing_task()
    except Exception:
        app.state.processing_task = None

    yield

    task = getattr(app.state, "processing_task", None)
    if task:
        await _stop_processing_task(task)

app = FastAPI(lifespan=lifespan)

def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )

def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    sub_app = FastAPI(title=title, description=description, version=version, lifespan=lifespan)
    apply_cors(sub_app)
    return sub_app

apply_cors(app)

document_extractor_app = create_sub_app(
    title="document_extract_mcp",
    description="Analyze documents in blob storage using Azure Form Recognizer and return structured results"
)

document_extractor_app.include_router(document_router)
FastApiMCP(document_extractor_app, include_operations=["document_extract_mcp"]).mount_http()

app.mount("/api/v1/email_intent_agent", document_extractor_app)


if __name__ == "__main__":
    host = os.getenv("HOST", "0.0.0.0")
    port = int(os.getenv("PORT", "8102"))
    uvicorn.run(app, host=host, port=port)


below is my code that i have to integrate basically the json that is getting generated in above code that json has to be taken as input in this below code so as the above code is storing the json in blob storage
that path should be used in the below codes input 
and this code that it will generate the final output.josn this json should be stored locally in an output folder instead of that earlier json that was getting stored,.
but make sure the json that got generated earlier and stored on blob should be taken as input 

this below code you can have in final_processor.py.
below is the code that has to be integrated in above code

import os
import json
from dotenv import load_dotenv
from openai import AzureOpenAI
 
# ------------------ CONFIG ------------------
load_dotenv()
 
client = AzureOpenAI(
    azure_endpoint=os.getenv("AZURE_OPENAI_ENDPOINT"),
    api_key=os.getenv("AZURE_OPENAI_API_KEY"),
    api_version="2024-05-01-preview"
)
 
MODEL = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")
CONFIDENCE_THRESHOLD = 0.5
MAX_CHARS = 12000
OUTPUT_FILE = "final_output.json"


BASE_DIR = os.path.dirname(os.path.abspath(__file__))
LOCAL_CSV_PATH = os.path.join(BASE_DIR,"input", "sanctions.csv")

 
# ------------------ HELPERS ------------------
def chunk_text(text, max_chars=MAX_CHARS):
    """Split long text into safe chunks for OpenAI input."""
    chunks, current, length = [], [], 0
    for line in text.splitlines():
        if length + len(line) > max_chars:
            chunks.append("\n".join(current))
            current, length = [], 0
        current.append(line)
        length += len(line)
    if current:
        chunks.append("\n".join(current))
    return chunks
 
 
def normalize_value(val):
    """Join token lists like ['6:00', 'AM'] → '6:00 AM'."""
    if isinstance(val, list):
        val = " ".join(str(v) for v in val if v)
    elif isinstance(val, dict):
        val = val.get("valueString") or val.get("content") or str(val)
    return str(val).strip()
 
 
def simplify_fields(raw_json):
    """Simplify Azure Doc Intelligence output."""
    simplified = {}
    docs = raw_json.get("documents", [])
    if not docs:
        return simplified
 
    for doc in docs:
        for key, value in doc.get("fields", {}).items():
            if "valueString" in value:
                val = normalize_value(value["valueString"])
            elif "valueBoolean" in value:
                val = value["valueBoolean"]
            elif "content" in value:
                val = normalize_value(value["content"])
            else:
                val = None
 
            conf = value.get("confidence", None)
            if val in [None, ""]:
                continue
 
            entry = {"value": val, "confidence": conf}
            entry["low_confidence"] = conf is not None and conf < CONFIDENCE_THRESHOLD
            simplified[key] = entry
    return simplified
 
 
def smart_group_fields(fields):
    """Group related fields (LOB, Policy, Broker, etc.)."""
    groups = {}
    for key, info in fields.items():
        lname = key.lower()
        if "lob" in lname or "lineofbusiness" in lname:
            group = "Line of Business"
        elif "policy" in lname:
            group = "Policy"
        elif "insured" in lname:
            group = "Insured Details"
        elif "broker" in lname:
            group = "Broker Details"
        elif "contact" in lname:
            group = "Contact Details"
        elif "address" in lname:
            group = "Address"
        else:
            group = "General"
        groups.setdefault(group, {})[key] = info
    return groups
 
 
def build_prompt(cleaned_data):
    """Structured prompt for GPT."""
    return f"""
You are a JSON data formatter. The following JSON is the output from Azure Document Intelligence for an ACORD insurance form.
 
Return only a valid JSON (no commentary, no markdown, no explanations).
 
Requirements:
1. Extract clean key–value pairs grouped logically (Line of Business, Policy, Broker, etc.).
2. Each field must show its value and confidence like:
   "Field Name": "Value, confidence - 0.87"
   if field is repeated extract both under its relevant umbrella section.
   Use semantic context, proximity and and field coordinates to determine grouping in the document to assign each field to its most relevant umbrella category.
3. Combine split tokens like ["6:00", "AM"] → "6:00 AM".
4. Add "total_confidence" (avg for group).
5. Add "overall_total_confidence" at end.
 
Cleaned JSON input:
{json.dumps(cleaned_data, indent=2)}
"""
 
 
def process_chunk(chunk):
    """Send one chunk to GPT and ensure JSON-only output."""
    response = client.chat.completions.create(
        model=MODEL,
        temperature=0,
        messages=[
            {"role": "system", "content": "You are a precise document understanding assistant that outputs only JSON."},
            {"role": "user", "content": build_prompt(chunk)},
        ]
    )
 
    text = response.choices[0].message.content.strip()
    # Try to load as JSON safely
    try:
        start = text.find("{")
        end = text.rfind("}") + 1
        json_text = text[start:end]
        return json.loads(json_text)
    except Exception:
        print("⚠️ Warning: GPT output not clean JSON. Attempting recovery.")
        return {"error": "Invalid JSON returned", "raw_output": text}
 
 
def process_azure_output(json_data):
    """Process entire Doc Intelligence JSON with chunking + GPT."""
    simplified = simplify_fields(json_data)
    grouped = smart_group_fields(simplified)
    text_data = json.dumps(grouped, indent=2)
 
    chunks = chunk_text(text_data)
    final_combined = {}
 
    for chunk in chunks:
        result = process_chunk(chunk)
        if isinstance(result, dict):
            for k, v in result.items():
                if k in final_combined and isinstance(v, dict):
                    final_combined[k].update(v)
                else:
                    final_combined[k] = v
 
    # Save to JSON file
    with open(OUTPUT_FILE, "w", encoding="utf-8") as f:
        json.dump(final_combined, f, indent=2, ensure_ascii=False)
 
    print(f"✅ Clean JSON written to {OUTPUT_FILE}")
    return final_combined
 
 
# ------------------ MAIN ------------------
if __name__ == "__main__":
    with open("myfile.json", "r", encoding="utf-8") as f:
        raw_json = json.load(f)
 
    process_azure_output(raw_json)






