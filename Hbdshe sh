
below is the code you have to refer

file router.py
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import compare_name_with_sanctions

router = APIRouter()


class CompareRequest(BaseModel):
    """
    This endpoint is invoked by the Data Extraction Agent.
    It passes the FULL Azure Blob URL of the extracted JSON in 'jsonfilepath'.

    Example payload:
    {
      "jsonfilepath": "https://<account>.blob.core.windows.net/<container>/<path>/file.json"
    }

    The service will:
    - Download that JSON from Azure Blob using AZURE_STORAGE_CONNECTION_STRING,
    - Extract the name from Owner/Insured/Contact/NAMED INSURED(S),
    - Compare it against local 'input/sanctions.csv',
    - Return the comparison result in a JSON-RPC envelope (consistent with your duplicate attachment checker).
    """
    jsonfilepath: str = Field(..., description="Full Azure Blob URL to the extracted JSON.")


@router.post(
    "/Licensing_&_Sanction_Checker_MCP",
    operation_id="Licensing_&_Sanction_Checker_MCP",
    summary="Compare JSON (Owner/Insured/Contact/NAMED INSURED(S)) from blob URL with local sanctions CSV."
)
async def compare_files(request: CompareRequest):
    """
    Receives the JSON blob URL from the Data Extraction Agent and performs the comparison.
    Returns a JSON-RPC envelope for consistency with the duplicate attachment checker.
    """
    try:
        result = await compare_name_with_sanctions(json_file_url=request.jsonfilepath)
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": result
            },
            status_code=200
        )
    except Exception as e:
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": {"status": False, "error": f"Unexpected server error: {str(e)}"}
            },
            status_code=200
        )



handler.py
import os
import json
from typing import Optional, Dict
import pandas as pd
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError
from dotenv import load_dotenv

from service import _require_env, _parse_blob_url

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
LOCAL_CSV_PATH = os.path.join("input", "sanctions.csv")


async def compare_name_with_sanctions(json_file_url: str) -> Dict:
    """
    Flow:
      1) Parse blob URL -> (container, blob_name)
      2) Download JSON (async)
      3) Extract FIRST matching field:
         Owner Name / Insured Name / Contact Name / NAMED INSURED(S)
      4) Read local CSV input/sanctions.csv
      5) Case-insensitive exact match against entity_name
      6) Save outputs to ./output and return structured result (with 'status')
    """

    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}


    try:
        container_name, blob_name = _parse_blob_url(json_file_url)
    except Exception as e:
        return {"status": False, "error": f"Invalid jsonfilepath: {e}"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(container_name)
            blob_client = container_client.get_blob_client(blob_name)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"JSON blob not found: container='{container_name}', blob='{blob_name}'"}

            stream = await blob_client.download_blob()
            raw_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error connecting to Azure Blob Storage: {e}"}

    #python dict
    try:
        data = json.loads(raw_bytes.decode("utf-8"))
    except Exception as e:
        return {"status": False, "error": f"Failed to parse JSON: {e}"}

    
    target_labels = {"ownername", "insuredname", "contactname", "namedinsured(s)", "applicant/firstnameinsured","insured/applicant","namedinsured","firstnameinsured","applicantname"}
    extracted_name: Optional[str] = None
    fields = data.get("extracted_fields", [])

    if isinstance(fields, list):
        for item in fields:
            if isinstance(item, dict):
                label_raw = item.get("Field", "")
                if isinstance(label_raw, str):
                    normalized = label_raw.lower().replace(" ", "").replace(":", "")
                    if normalized in target_labels:
                        value = item.get("Value", "").strip()
                        if value:
                            extracted_name = value
                            break

    if not extracted_name:
        return {"status": False, "error": "No matching name field found in JSON."}

    if not os.path.exists(LOCAL_CSV_PATH):
        return {"status": False, "error": f"Local CSV not found at '{LOCAL_CSV_PATH}'"}

    try:
        csv_df = pd.read_csv(LOCAL_CSV_PATH)
    except Exception as e:
        return {"status": False, "error": f"Failed to read CSV '{LOCAL_CSV_PATH}': {e}"}

    if "entity_name" not in csv_df.columns:
        return {"status": False, "error": "CSV must contain 'entity_name' column"}
    def norm(s: str) -> str:
        return s.strip().lower() if isinstance(s, str) else ""

    entity_set = {norm(x) for x in csv_df["entity_name"].dropna() if isinstance(x, str) and x.strip()}
    # is_unique = norm(extracted_name) not in entity_set
    is_unique = norm(extracted_name) in entity_set

    results = [{"Extracted Name": extracted_name, "is_Sanctioned": is_unique}]

    try:
        os.makedirs("./output", exist_ok=True)
        json_output = "./output/comparison_results.json"
        with open(json_output, "w", encoding="utf-8") as f:
            json.dump({"results": results}, f, indent=4, ensure_ascii=False)
    except Exception as e:
        return {"status": False, "error": f"Failed saving results: {e}"}

    return {"status": True, "results": results}




main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
from fastapi_mcp import FastApiMCP
import uvicorn

from file_router import router as file_router


def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )


def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    sub = FastAPI(title=title, description=description, version=version)
    apply_cors(sub)
    return sub


app = FastAPI()
apply_cors(app)

file_app = create_sub_app(
    title="Licensing & Sanction Checker MCP",
    description="Receives JSON blob URL from Data Extraction Agent and compares extracted name with local sanctions CSV."
)
file_app.include_router(file_router)

# Expose ONLY this operation id via MCP HTTP (client will use transport='http')
FastApiMCP(file_app, include_operations=["Licensing_&_Sanction_Checker_MCP"]).mount_http()

# Mount the sub-app under this prefix (keep as-is per your request)
app.mount("/api/v1/eligibility_agent", file_app)


if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8601)



service.py

import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")


def _require_env() -> None:
    """Ensure required environment variables exist."""
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING in .env")


def _parse_blob_url(url: str) -> tuple[str, str]:
    """
    Parse an Azure Blob URL and return (container_name, blob_path).
    Example URL:
      https://account.blob.core.windows.net/output-results/folder/name.json
    Returns:
      ("output-results", "folder/name.json")
    """
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("jsonfilepath must be a valid http(s) URL to Azure Blob Storage.")

    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("jsonfilepath must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)




test_client.py
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient


async def get_tool_list(config_mcp_server):
    client = MultiServerMCPClient(config_mcp_server)
    tools_list = await client.get_tools()
    print("Tools fetched from MCP:", [tool.name for tool in tools_list])
    return tools_list
     
    # await client.aclose()


async def main():
    config_mcp_server = {
        "sanction_checker_mcp": {
            "url": "http://localhost:8601/api/v1/eligibility_agent/mcp",
             "transport":"streamable_http", 
        }
    }
    tools = await get_tool_list(config_mcp_server=config_mcp_server)
    print(tools)


if __name__ == "__main__":
    asyncio.run(main())


testscripts.py
import requests
import json

BASE_URL = "http://localhost:8601/api/v1/eligibility_agent/Licensing_&_Sanction_Checker_MCP"

def run_test():
    payload = {
        "jsonfilepath": "https://agenticai1.blob.core.windows.net/output-results/20251027_101657_8b635bd693b247899be4f0295a02807c_extracted_20251027_101657.json"
    }
    try:
        resp = requests.post(BASE_URL, json=payload, timeout=60)
        print("HTTP Status:", resp.status_code)
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except json.JSONDecodeError:
            print("Response not JSON:", resp.text)
    except Exception as e:
        print("Error calling API:", str(e))

if __name__ == "__main__":
    print("Testing /Licensing_&_Sanction_Checker_MCP ...")
    run_test()


below is my current code

handler.py


from service import extract_address, check_earthquake_risk, read_json_blob, extract_container_and_file_name
 
async def process_blob_file(blob_url: str, providers: dict) -> dict:
    try:
        container_name, file_name = extract_container_and_file_name(blob_url)
        json_data = read_json_blob(file_name)
 
        fields = json_data.get("extracted_fields", [])
        address_value = None
 
        for field in fields:
            if "MAILING ADDRESS" in field.get("Field", ""):
                address_value = field.get("Value")
                break
 
        if not address_value:
            return {"error": "No mailing address found in extracted_fields."}
 
        address = extract_address(address_value)
        print(address)
        coordinates = providers["geocode"](address)
 
        if "latitude" not in coordinates or "longitude" not in coordinates:
            return {
                "coordinates": coordinates,
                "earthquake_prone": "Unknown (geocoding failed)",
                "construction_type": "Unknown",
                "distance_to_fire_hydrant": "Unknown",
                "distance_to_fire_station": "Unknown",
                "year_built": "Unknown"
            }
 
        latitude = coordinates["latitude"]
        longitude = coordinates["longitude"]
 
        earthquake_risk = providers["earthquake"](latitude, longitude)
 
        return {
            "coordinates": coordinates,
            "earthquake_count": earthquake_risk,
            "construction_type": "Cement",
            "distance_to_fire_hydrant": "20 FT",
            "distance_to_fire_station": "3 MI",
            "year_built": 2018
        }
 
    except Exception as e:
        return {"error": f"Failed to process blob file: {str(e)}"}


main.py

from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from router import router  

app = FastAPI()

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

app.include_router(router, prefix="/api/v1/hazard")
 
if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8000, reload=True)


router.py

from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import process_blob_file
from service import get_coordinates_from_openweather, check_earthquake_risk
import os

API_KEY = os.getenv("OPENWEATHER_API_KEY")

router = APIRouter()

class BlobRequest(BaseModel):
    file_path: str = Field(..., description="Full blob URL to the JSON file in Azure Blob Storage.")

providers = {
    "geocode": get_coordinates_from_openweather,
    "earthquake": check_earthquake_risk,
    "geocoder_name": "OpenWeather"
}

@router.post("/get-coordinates", operation_id="hazard_profile_checker")
async def get_coordinates_from_blob(p_body: BlobRequest):
    result = await process_blob_file(p_body.file_path, providers)
    return JSONResponse(content=result, status_code=200)


service.py

import json
import os
from urllib.parse import urlparse
import requests
from dotenv import load_dotenv
from azure.storage.blob import BlobServiceClient
import re
from datetime import datetime, timedelta

load_dotenv()
API_KEY = os.getenv("OPENWEATHER_API_KEY")
USGS_API_URL = os.getenv("USGS_EARTHQUAKE_API")
AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
AZURE_BLOB_CONTAINER_INPUT = os.environ.get("AZURE_BLOB_CONTAINER_INPUT", "")
AZURE_BLOB_CONTAINER_OUTPUT = os.environ.get("AZURE_BLOB_CONTAINER_OUTPUT", "")

def _get_blob_service_client():
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING")
    return BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

def _ensure_input_container_exists(blob_service_client):
    if not AZURE_BLOB_CONTAINER_INPUT:
        raise RuntimeError("Missing input AZURE_BLOB_CONTAINER")
    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER_INPUT)
    try:
        container_client.create_container()
    except Exception:
        pass
    return container_client

def _ensure_output_container_exists(blob_service_client):
    if not AZURE_BLOB_CONTAINER_OUTPUT:
        raise RuntimeError("Missing output AZURE_BLOB_CONTAINER")
    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER_OUTPUT)
    try:
        container_client.create_container()
    except Exception:
        pass
    return container_client

def read_json_blob(filename: str) -> dict:
    """
    Downloads a JSON file from Azure Blob Storage and returns it as a Python dictionary.
    """
    blob_service_client = _get_blob_service_client()
    container_client = _ensure_input_container_exists(blob_service_client)
    blob_client = container_client.get_blob_client(blob=filename)

    if not blob_client.exists():
        raise FileNotFoundError(f"Blob '{filename}' not found in container '{AZURE_BLOB_CONTAINER_INPUT}'")

    blob_data = blob_client.download_blob().readall()
    return json.loads(blob_data.decode("utf-8"))

def extract_container_and_file_name(url: str) -> tuple[str, str]:
    parsed = urlparse(url)
    path_parts = parsed.path.lstrip("/").split("/", 1)
    if len(path_parts) != 2:
        raise ValueError("Invalid file URL format")
    container_name, file_name = path_parts
    return container_name, file_name

def write_json_to_blob(filename: str, data: dict):
    """
    Uploads a JSON-serializable dictionary to Azure Blob Storage as a .json file.
    """
    blob_service_client = _get_blob_service_client()
    container_client = _ensure_output_container_exists(blob_service_client)
    blob_client = container_client.get_blob_client(blob=filename)

    json_bytes = json.dumps(data, indent=4).encode("utf-8")
    blob_client.upload_blob(json_bytes, overwrite=True)

# extracts everything after street name
def extract_address(value: str) -> str:
    # Match any street-like pattern ending in "Street", "Road", "Lane", "Avenue", etc.
    pattern = r"\b(?:Street|Road|Lane|Avenue|Boulevard|Drive|Way|Circle|Court|Place|Terrace|Ln|St)\b,?\s*(.*)"
    match = re.search(pattern, value, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    return ""

# extracts last 4 components of the adddress
# def extract_address(value: str) -> str:
#     parts = value.split(",")
#     if len(parts) >= 4:
#         return ", ".join([parts[-4].strip(), parts[-3].strip(), parts[-2].strip()])
#     return value.strip()

# extracts the complete address
# def extract_address(value: str) -> str:
#     parts = value.split(",", 1)
#     if len(parts) == 2:
#         return parts[1].strip()
#     return value.strip()

#latitude and longitude
def get_coordinates_from_openweather(address: str) -> dict:
    print(f"Querying OpenWeather with: {address}") 

    url = "http://api.openweathermap.org/geo/1.0/direct"
    params = {
        "q": address,
        "limit": 1,
        "appid": API_KEY
    }
    response = requests.get(url, params=params)

    if response.status_code == 404:
        return {"error": f"Location not found for '{address}'."}

    response.raise_for_status()
    data = response.json()

    if not data:
        return {"error": f"No location found for '{address}'."}

    location = data[0]
    return {
        "latitude": location.get("lat"),
        "longitude": location.get("lon"),
    }

# earthquack zone
def check_earthquake_risk(lat: float, lon: float, radius_km: int = 100, years: int = 5) -> int:
    end = datetime.utcnow()
    start = end - timedelta(days=365 * years)

    params = {
        "format": "geojson",
        "latitude": lat,
        "longitude": lon,
        "maxradiuskm": radius_km,
        "starttime": start.strftime("%Y-%m-%d"),
        "endtime": end.strftime("%Y-%m-%d")
    }

    response = requests.get(USGS_API_URL, params=params)
    response.raise_for_status()
    data = response.json()
    print(data)

    return data.get("count", 0)



test.py

import requests

blob_url = "https://yourstorage.blob.core.windows.net/output-results/20251027_101657_8b635bd693b247899be4f0295a02807c_extracted_20251027_101657.json"

endpoint = "http://localhost:8000/api/v1/hazard/get-coordinates"

payload = {
    "file_path": blob_url
}

response = requests.post(endpoint, json=payload)

print("Status Code:", response.status_code)
print("Response JSON:", response.json())
