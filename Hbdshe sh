Nice — you want a single clear reason the LLM used to pick structured or unstructured, printed to the terminal so you can inspect what it considered. Below I give:
	1.	A short explanation of what the LLM is actually getting now (and therefore what it can base its decision on).
	2.	A small, safe code-change that adds an explicit, human-readable reason (and simple features) to the prompt and prints that reason in your terminal.
	3.	a tiny fix (one \n bug I noticed in load_document) and an example of expected terminal output.

⸻

1) What the LLM is currently receiving (and why it decides)

Right now your classify_document function sends the LLM two things:
	•	text — a (string) concatenation of document lines extracted by Form Recognizer (you currently limit the prompt to the first ~2000 chars).
	•	tables — whatever you extracted and put into document_content["tables"] (or the string "No tables detected").

So the LLM’s decision is based on what you give it. Typical signals it will (implicitly) use:
	•	Presence of tables (non-empty tables) is a strong signal for structured (forms, spreadsheets, ACORD).
	•	Table-like content inside text (repeated columns, delimiters) can also push toward structured.
	•	Consistent short lines / label:value patterns (e.g., Name: John) → structured.
	•	Long paragraphs / free-flow sentences → unstructured.
	•	Keywords such as “Invoice”, “Total”, “Policy No”, “ACORD” or “Quote”, “Dear”, “Thank you” etc.
	•	Layout regularity, e.g., many rows with same number of columns or repeated field names.
	•	Number of lines / avg words per line — short lines favor structured; long sentences favor unstructured.

However — important — the LLM only sees what you send in the prompt. If you want a robust explanation, it’s best to compute a few simple features locally (presence/number of tables, avg words per line, presence of “form-like” keywords) and include them in the prompt. That both makes the LLM’s output more consistent and lets you print the reason.

⸻

2) Minimal code changes (prints the reason to terminal)

Changes:
	•	Update classifier.py to compute simple features and ask the LLM to return both the label and a one-line reason (or a JSON).
	•	Slight fix in document_loader.py to join lines with "\n" (instead of "n").

⸻

Updated classifier.py (replace your function with this)

# classifier.py
from dotenv import load_dotenv
import os
import openai
import json

load_dotenv()
openai.api_type = "azure"
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")
openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")

deployment_name = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")

# simple list of form/document keywords you can expand
FORM_KEYWORDS = [
    "policy", "policy no", "acord", "invoice", "amount", "claim", "insured",
    "coverage", "address", "phone", "date of birth", "total", "qty", "item"
]
QUOTE_KEYWORDS = ["quote", "proposal", "thanks", "dear", "regards", "sincerely"]

def _compute_features(document_content):
    text = document_content.get("text", "") or ""
    tables = document_content.get("tables", [])
    # feature: whether tables exist
    tables_exist = bool(tables and tables != "No tables detected")
    num_tables = len(tables) if isinstance(tables, list) else 0

    # basic text features
    lines = [l for l in text.splitlines() if l.strip()]
    num_lines = len(lines)
    total_words = sum(len(l.split()) for l in lines) if lines else 0
    avg_words_per_line = (total_words / num_lines) if num_lines else 0

    # keyword checks (lowercase)
    lower_text = text.lower()
    form_kw_found = [kw for kw in FORM_KEYWORDS if kw in lower_text]
    quote_kw_found = [kw for kw in QUOTE_KEYWORDS if kw in lower_text]

    features = {
        "tables_exist": tables_exist,
        "num_tables": num_tables,
        "num_lines": num_lines,
        "avg_words_per_line": round(avg_words_per_line, 2),
        "form_keywords_found": form_kw_found,
        "quote_keywords_found": quote_kw_found
    }
    return features

def classify_document(document_content):
    """
    Returns (label, reason, features) where label is 'structured' or 'unstructured',
    reason is a short human-readable explanation from the LLM, and features is the local features dict.
    """
    # compute features locally and include them in the prompt
    features = _compute_features(document_content)
    text = document_content.get("text", "")
    tables = document_content.get("tables", "No tables detected")

    prompt = f"""
You are an assistant that must classify a document layout as exactly one of:
  - structured
  - unstructured

Use the provided features and the document text to decide. Be concise.

Local features (computed by code):
{json.dumps(features, indent=2)}

Tables extracted (showing first 1000 chars):
{str(tables)[:1000]}

Document text (first 2000 chars):
{text[:2000]}

Reply in JSON ONLY, with keys:
{{"label": "<structured|unstructured>", "reason": "<one-sentence reason mentioning which feature(s) influenced the decision>"}}
Example:
{{"label":"structured","reason":"Tables detected (num_tables=1) and repeated short label:value lines indicate a form-like layout."}}
"""

    response = openai.ChatCompletion.create(
        engine=deployment_name,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=120,
        temperature=0.0
    )
    content = response['choices'][0]['message']['content'].strip()

    # Try to parse JSON; if LLM didn't return JSON, fall back to the raw label
    try:
        parsed = json.loads(content)
        label = parsed.get("label", "").strip()
        reason = parsed.get("reason", "").strip()
    except Exception:
        # best-effort fallback: take last word or the content
        label = content.splitlines()[0].strip() if content else "unknown"
        reason = content

    return label, reason, features


⸻

Small fix in document_loader.py (line join bug)

Change this:

document_content = {
    "text": "n".join(lines),
    "tables": tables if tables else "No tables detected"
}

to:

document_content = {
    "text": "\n".join(lines),
    "tables": tables if tables else "No tables detected"
}


⸻

Minimal main.py change to print reason

Replace how you call classify_document:

from document_loader import load_document
from classifier import classify_document

def main():
    print(" Document Layout Classifier")
    file_path = input("Enter the file path: ").strip()

    try:
        document_content = load_document(file_path)
        if not document_content["text"]:
            raise Exception("No readable content found.")

        layout_type, reason, features = classify_document(document_content)
        print(f"\n Document Layout: {layout_type}")
        print(f"Reason: {reason}")
        print(f"Features used: {features}")

    except Exception as e:
        print("Error:", e)

if __name__ == "__main__":
    main()


⸻

3) Example terminal output (what you will now see)

After running the updated code you will get something like:

 Document Layout Classifier
Enter the file path: C:\...\Acord_125 (1).pdf

 Document Layout: structured
Reason: Tables detected (num_tables=1) and repeated short label:value lines indicate a form-like layout.
Features used: {'tables_exist': True, 'num_tables': 1, 'num_lines': 42, 'avg_words_per_line': 3.4, 'form_keywords_found': ['acord','policy'], 'quote_keywords_found': []}

Or for a quote document:

 Document Layout: unstructured
Reason: No tables found and long paragraph sentences with average words/line=18 indicate free-form text.
Features used: {'tables_exist': False, 'num_tables': 0, 'num_lines': 12, 'avg_words_per_line': 18.2, 'form_keywords_found': [], 'quote_keywords_found': ['quote']}


⸻

Notes & suggestions
	•	I kept the LLM deterministic (temperature=0.0) so the explanation is stable.
	•	You can add more features (number of short lines, repeated colon : occurrences, bounding box consistency from result.pages if you want spatial signals) and include them in the prompt — that will make the reason even more explicit and robust.
	•	If you want only a single textual reason (no JSON parsing), you can simplify the prompt to ask for exactly: "<structured|unstructured> — <one-sentence reason>" and parse accordingly. But JSON is safer.

⸻

If you want, I can:
	•	add num_short_lines, num_label_colon_patterns (counts of \w+:), or
	•	show how to extract bounding-box-based features from result.pages[*].lines[*].spans to detect consistent column x-coordinates (more reliable for tables/forms).

Want me to add one of those extra features now?
