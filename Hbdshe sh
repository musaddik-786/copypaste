Got it — removed the legacy exact-match check. Now the code only treats a blob as matching if its name contains the _attachment_<filename> token (the new format). I did not change anything else in your file.

Copy-paste this full file over your existing one:

import os
import json
import base64
import re
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv

load_dotenv()

STATE_FILE = os.path.join("storage", "history_state.json")
OUTPUT_DIR = os.path.join("storage", "attachments")
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
AZURE_BLOB_CONTAINER = os.environ.get("AZURE_BLOB_CONTAINER", "")

def _get_blob_service_client():
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING")
    return BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

def _ensure_container_exists(blob_service_client):
    if not AZURE_BLOB_CONTAINER:
        raise RuntimeError("Missing AZURE_BLOB_CONTAINER")
    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER)
    try:
        container_client.create_container()
    except Exception:
        pass
    return container_client

def load_state():
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def save_state(state):
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, indent=2, ensure_ascii=False)

def _walk_parts(parts):
    for p in parts or []:
        yield p
        for sub in p.get("parts", []) or []:
            yield from _walk_parts([sub])

def _blob_exists_for_attachment(container_client, attachment_name):
    """
    Returns True if any blob corresponds to the given attachment_name using the new naming rule:
      - New format: <referenceid>_attachment_<filename>
    We no longer check for exact blob name == attachment_name.
    """
    search_token = f"_attachment_{attachment_name}"
    try:
        # list_blobs may be expensive for very large containers; this directly scans blob names.
        for blob in container_client.list_blobs():
            # match if the search token appears in the blob name (handles folder prefixes)
            if search_token in blob.name:
                return True
    except Exception as e:
        print(f"Error while listing blobs for pattern search '{search_token}': {e}")

    return False

def save_attachments_from_message(service, message):
    """
    Extract attachment filenames from Gmail message and compare with Azure Blob Storage.
    Success: returns {"attachments": [{"filename": "...", "is_duplicate": bool}, ...]}
    No attachments: returns {"attachments": []}
    Azure/Gmail errors: raise exception → caller returns status:false + error
    """
    filenames = []
    parts = message.get("payload", {}).get("parts", [])
    for part in _walk_parts(parts):
        filename = part.get("filename")
        _ = part.get("body", {}) or {}
        if filename:
            filenames.append(filename)

    if not filenames:
        try:
            out_path = os.path.join(OUTPUT_DIR, f"{message.get('id', 'unknown')}_comparison.json")
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump({"attachments": []}, f, indent=2, ensure_ascii=False)
        except Exception as exc:
            print(f"Failed to write comparison result file: {exc}")
        return {"attachments": []}

    # --- NEW: use regex to find attachments containing the word "Accord"
    # If any filenames match, only those will be checked against blob storage.
    accord_pattern = re.compile(r"acord", re.IGNORECASE)
    accord_matches = [fn for fn in filenames if accord_pattern.search(fn)]

    if accord_matches:
        filenames_to_check = accord_matches
    else:
        # fallback to original behaviour (check all attachments)
        filenames_to_check = filenames
    # --- END NEW

    blob_service_client = _get_blob_service_client()
    container_client = _ensure_container_exists(blob_service_client)

    attachment_results = []
    for name in filenames_to_check:
        try:
            # New behavior: only pattern-based match against blobs named like <ref>_attachment_<filename>
            is_duplicate = _blob_exists_for_attachment(container_client, name)
        except Exception as e:
            # if blob check fails for any reason, report as not duplicate and log the error
            print(f"Error checking blob existence for '{name}': {e}")
            is_duplicate = False

        attachment_results.append({
            "filename": name,
            "is_duplicate": is_duplicate
        })

    try:
        out_path = os.path.join(OUTPUT_DIR, f"{message.get('id', 'unknown')}_comparison.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump({"attachments": attachment_results}, f, indent=2, ensure_ascii=False)
    except Exception as exc:
        print(f"Failed to write comparison result file: {exc}")

    return {"attachments": attachment_results}

If you want the pattern search to be case-insensitive (e.g., _attachment_Acord 3.pdf vs _attachment_acord 3.pdf) or to handle small filename normalization (trimming whitespace, URL-encoding differences), I can add that — but I left it strict to match exactly _attachment_<filename> as you described.
