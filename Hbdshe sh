Nice — this is a great question. Short answer first: the LLM only “sees” whatever you send it in the prompt. It does not inspect the PDF directly — your Form Recognizer output (text, tables, metadata) is what you provide as input, and the model decides structured vs unstructured based on that prompt content.

Below I’ll:
	1.	Explain what your current pipeline is already sending to the model.
	2.	Show why the model gave structured for ACORD and unstructured for the quote PDF.
	3.	Give a concrete, minimal code change that prints the features (what the LLM is being given) to the terminal and also sends richer metadata to the model so you can see exactly what influences the decision.
	4.	Offer a simple rule-based fallback you can use (recommended) so you don’t entirely rely on the LLM for this single binary decision.

⸻

1) What the model actually receives now

In your classifier.classify_document you build a prompt using:
	•	text = document_content["text"] — the document text (you pass first 2000 chars in prompt).
	•	tables = document_content["tables"] — either a list of table cell text or "No tables detected".

So the model’s decision is based only on the textual snippet and the tables content you included. If tables shows table-like structures or you include an ACORD-like text snippet, the model will likely say structured. If the tables value is "No tables detected" and the text is free-flowing, it returns unstructured.

Important: the LLM cannot know about other page/layout signals (page count, words, column consistency, bounding boxes) unless you extract them and include them in the prompt.

⸻

2) Why you saw structured for ACORD and unstructured for the quote
	•	ACORD forms are typically parsed by Form Recognizer as tables or many small labelled fields. Your load_document extracts tables — if one or more tables were present, tables in the prompt would include them and the LLM sees evidence of structured layout → returns structured.
	•	The quote PDF likely had no page.tables detected, so you passed "No tables detected" and lots of paragraph text → model returns unstructured.

⸻

3) Code: print features and send richer metadata to the model

Replace / update your document_loader.py and classifier.py with the versions below. The loader now returns metadata (page_count, word_count, lines_count, num_tables, table_summaries). The classifier prints those features to terminal before calling the LLM and includes them in the prompt.

document_loader.py (updated)

# document_loader.py
from dotenv import load_dotenv
import os
from azure.ai.formrecognizer import DocumentAnalysisClient
from azure.core.credentials import AzureKeyCredential

load_dotenv()

endpoint = os.getenv("AZURE_FORMRECOG_ENDPOINT")
key = os.getenv("AZURE_FORMRECOG_KEY")
if not endpoint:
    raise ValueError("AZURE_FORMRECOG_ENDPOINT is not set. Check your .env file.")
if not key:
    raise ValueError("AZURE_FORMRECOG_KEY is not set. Check your .env file.")

client = DocumentAnalysisClient(endpoint=endpoint, credential=AzureKeyCredential(key))

def load_document(file_path):
    """
    Extracts text and layout information using Form Recognizer's prebuilt-layout model.
    Returns both content and metadata so we can inspect what the classifier sees.
    """
    with open(file_path, "rb") as f:
        poller = client.begin_analyze_document("prebuilt-layout", document=f)
        result = poller.result()

    # Lines and words
    lines = [line.content for page in result.pages for line in page.lines]
    words_count = 0
    for page in result.pages:
        if hasattr(page, "words"):
            words_count += len(page.words)
        else:
            # fallback: approximate by splitting lines
            words_count += sum(len(line.content.split()) for line in page.lines)

    # Debugging: Print available attributes in DocumentPage (keeps your old debug)
    if len(result.pages) > 0:
        print("Available attributes in DocumentPage:")
        print(dir(result.pages[0]))

    # Extract table information if available and build summaries
    tables = []
    table_summaries = []
    num_tables = 0

    for p_idx, page in enumerate(result.pages):
        if hasattr(page, "tables") and page.tables:
            for t_idx, table in enumerate(page.tables):
                num_tables += 1
                # try to get row_count and column_count if SDK provides
                row_count = getattr(table, "row_count", None)
                column_count = getattr(table, "column_count", None)

                # fallback: attempt to infer from cell attributes if available
                cells = getattr(table, "cells", None)
                if cells:
                    # approximate counts if explicit not present
                    try:
                        max_row = max(getattr(c, "row_index", 0) for c in cells)
                        max_col = max(getattr(c, "column_index", 0) for c in cells)
                        inferred_rows = max_row + 1
                        inferred_cols = max_col + 1
                    except Exception:
                        inferred_rows = None
                        inferred_cols = None
                else:
                    inferred_rows = None
                    inferred_cols = None

                # small preview of table content (first few cells as text)
                preview = []
                if cells:
                    for c in cells[:10]:
                        preview.append(getattr(c, "content", str(c)))
                else:
                    # fallback if table.cells not available
                    preview = ["(cells not available)"]

                table_info = {
                    "page": p_idx + 1,
                    "row_count": row_count or inferred_rows,
                    "column_count": column_count or inferred_cols,
                    "num_cells": len(cells) if cells else None,
                    "preview_cells": preview
                }
                table_summaries.append(table_info)

                # also build a simple table representation for the LLM (list of cell texts)
                table_texts = [getattr(c, "content", str(c)) for c in cells] if cells else []
                tables.append(table_texts)

    document_content = {
        "text": "\n".join(lines),
        "tables": tables if tables else None,
        "meta": {
            "page_count": len(result.pages),
            "line_count": len(lines),
            "word_count": words_count,
            "num_tables": num_tables,
            "table_summaries": table_summaries
        }
    }
    return document_content

classifier.py (updated)

# classifier.py
from dotenv import load_dotenv
import os
import openai
load_dotenv()

openai.api_type = "azure"
openai.api_base = os.getenv("AZURE_OPENAI_ENDPOINT")
openai.api_version = os.getenv("AZURE_OPENAI_API_VERSION")
openai.api_key = os.getenv("AZURE_OPENAI_API_KEY")
deployment_name = os.getenv("AZURE_OPENAI_CHAT_DEPLOYMENT")

def classify_document(document_content):
    """
    Classifies the document layout as 'structured' or 'unstructured'.
    Prints the exact features passed to the LLM so you can see what influences the decision.
    """
    text = document_content.get("text", "")
    tables = document_content.get("tables")
    meta = document_content.get("meta", {})

    # Print features (so you'll see in terminal what LLM is being given)
    print("\n--- Document features passed to classifier ---")
    print(f"Pages: {meta.get('page_count')}")
    print(f"Lines: {meta.get('line_count')}")
    print(f"Words (approx): {meta.get('word_count')}")
    print(f"Num tables detected: {meta.get('num_tables')}")
    if meta.get("table_summaries"):
        for i, t in enumerate(meta["table_summaries"], start=1):
            print(f" Table {i}: page={t['page']}, rows={t['row_count']}, cols={t['column_count']}, num_cells={t['num_cells']}")
            # show a short preview
            print("  preview cells:", t["preview_cells"][:5])
    print("--- end features ---\n")

    # Compose richer prompt that includes metadata - gives LLM more context
    prompt = f"""
You are an AI assistant. Classify the following document layout as either 'structured' or 'unstructured'.

Hints:
- Structured documents often contain forms, tables, consistent columns or repeated key:value pairs and consistent field layouts.
- Unstructured documents are mainly paragraphs, long sentences, and no repeated tabular structure.

Metadata:
Pages: {meta.get('page_count')}
Lines: {meta.get('line_count')}
Words: {meta.get('word_count')}
Num tables detected: {meta.get('num_tables')}
Table summaries: {meta.get('table_summaries') if meta.get('table_summaries') else 'No table summaries'}

Document Text (first 2000 chars):
{text[:2000]}

Tables (first 2000 chars if present):
{tables if tables else "No tables detected"}

Reply with only one word: structured or unstructured.
"""
    # Call the model deterministically
    response = openai.ChatCompletion.create(
        engine=deployment_name,
        messages=[{"role": "user", "content": prompt}],
        max_tokens=4,
        temperature=0.0
    )
    return response['choices'][0]['message']['content'].strip()

main.py note: no changes required beyond what you already have — this loader + classifier will print the features and the model output.

⸻

4) Simple rule-based fallback (recommended)

Because this is a binary structural check, you may prefer a deterministic rule first and only call the LLM if the rule is ambiguous. Example heuristic:
	•	If num_tables >= 1 → structured.
	•	Else if avg_words_per_line < 6 and line_count > 20 → likely forms or lists → structured.
	•	Else if text contains many : separated short key names (like Name:, Address:) repeated → structured.
	•	Else → unstructured.

You can implement that cheaply before contacting the LLM (faster, cheaper, reproducible).

Example snippet to add into classifier.classify_document before calling LLM:

# simple heuristic before LLM
if meta.get("num_tables", 0) >= 1:
    return "structured"
avg_words_per_line = meta["word_count"] / max(1, meta["line_count"])
if avg_words_per_line < 6 and meta["line_count"] > 10:
    return "structured"
# if uncertain, fall back to LLM for final decision


⸻

5) Final notes / debugging tips
	•	Print the prompt to the terminal if you want to absolutely know what the LLM sees (be careful about very long text). I included printed metadata so you won’t need to dump the whole prompt every time.
	•	The model’s answer is only as good as the extraction. If Form Recognizer misses tables, the LLM cannot “magically” see the PDF’s layout. Improve the extraction (experiment with other prebuilt models or pass the PDF through multiple detectors) if you get false negatives.
	•	Keep temperature=0.0 and max_tokens=4 for deterministic single-word classification.
	•	If you want transparency, add a reason request in the prompt (e.g., “Explain in 1-2 lines why you classified it so”) — but that costs tokens.

⸻

If you want, I can:
	•	Add the heuristic-first approach into your code (I already showed the snippet).
	•	Or run through an example output given the metadata you posted and show exactly what prompt would be sent (I can mock it here).

Which would you prefer?
