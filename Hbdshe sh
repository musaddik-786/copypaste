Perfect question ðŸ‘ â€” right now, with the last version I gave you, your JSON is only being saved locally (inside the ./output/ folder).

To also upload that same JSON file to Azure Blob Storage (just like before), weâ€™ll simply add back the upload logic â€” while keeping your local storage unchanged.

Letâ€™s fix that cleanly ðŸ‘‡

â¸»

âœ… What youâ€™ll get after this update

After the PDF is analyzed:
	1.	The JSON is saved locally at:
./output/output.json
	2.	The same JSON file is uploaded to Azure Blob Storage, in the container
output-results (same as before).
	3.	The API response includes both:

{
  "status": true,
  "source_file": "Acord_125_FullForm_Filled 1.pdf",
  "local_output_json": "./output/output.json",
  "output_blob_url": "https://agenticai1.blob.core.windows.net/output-results/20251027_101657_abc123_extracted.json"
}



â¸»

ðŸ§  How it works (in short)
	â€¢	After saving output/output.json, we connect to Azure Blob Storage using your existing
AZURE_STORAGE_CONNECTION_STRING (already loaded from .env).
	â€¢	We create (if needed) the container output-results.
	â€¢	We upload the JSON file with a unique name using timestamp and UUID (same as before).
	â€¢	Then we return both file paths in the response.

â¸»

âœ¨ Final Updated image_processor.py

(copy-paste the entire file below â€” it combines both local and blob saving)

# image_processor.py

import os
import json
import uuid
from datetime import datetime
from dotenv import load_dotenv

from azure.core.credentials import AzureKeyCredential
from azure.ai.documentintelligence import DocumentIntelligenceClient
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError

from service import _require_env, get_env_values, AZURE_STORAGE_CONNECTION_STRING, _parse_blob_url

load_dotenv()


async def analyze_file_async(bloburl: str) -> dict:
    """
    Download the PDF from bloburl, analyze it with Azure Document Intelligence,
    save the JSON locally inside ./output/output.json,
    and also upload the same JSON to Azure Blob Storage (output-results container).
    """

    # 1ï¸âƒ£ Validate environment variables
    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": str(e)}

    endpoint, key, model_id = get_env_values()

    # 2ï¸âƒ£ Parse the blob URL (container, blob path)
    try:
        src_container, src_blob = _parse_blob_url(bloburl)
    except Exception as e:
        return {"status": False, "error": f"Invalid BlobUrl: {e}"}

    source_file_name = os.path.basename(src_blob) if src_blob else "unknown.pdf"
    source_file_name = os.path.basename(source_file_name)

    # 3ï¸âƒ£ Download PDF from Azure Blob Storage
    if not AZURE_STORAGE_CONNECTION_STRING:
        return {"status": False, "error": "Missing AZURE_STORAGE_CONNECTION_STRING in .env"}

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            container_client = blob_service.get_container_client(src_container)
            blob_client = container_client.get_blob_client(src_blob)
            try:
                await blob_client.get_blob_properties()
            except ResourceNotFoundError:
                return {"status": False, "error": f"PDF blob not found: container='{src_container}', blob='{src_blob}'"}

            stream = await blob_client.download_blob()
            pdf_bytes = await stream.readall()
    except Exception as e:
        return {"status": False, "error": f"Error downloading PDF from Blob Storage: {e}"}

    # 4ï¸âƒ£ Save the PDF locally (keep name)
    input_path = os.path.join(os.getcwd(), source_file_name)
    try:
        with open(input_path, "wb") as wf:
            wf.write(pdf_bytes)
    except Exception as e:
        return {"status": False, "error": f"Failed to write local input file '{input_path}': {e}"}

    # 5ï¸âƒ£ Analyze the PDF using Azure Document Intelligence
    print("ðŸ”— Connecting to Azure Document Intelligence service...")
    try:
        client = DocumentIntelligenceClient(
            endpoint=endpoint,
            credential=AzureKeyCredential(key)
        )
    except Exception as e:
        return {"status": False, "error": f"Failed to create DocumentIntelligenceClient: {e}"}

    print(f"ðŸ“„ Analyzing '{source_file_name}' using model '{model_id}'...")
    try:
        with open(input_path, "rb") as f:
            poller = client.begin_analyze_document(model_id=model_id, body=f)
            result = poller.result()
    except Exception as e:
        return {"status": False, "error": f"Error during document analysis: {e}"}

    # 6ï¸âƒ£ Save the JSON output inside ./output/ folder
    print("ðŸ’¾ Saving results in './output/output.json'...")
    output_dir = os.path.join(os.getcwd(), "output")
    os.makedirs(output_dir, exist_ok=True)
    output_path = os.path.join(output_dir, "output.json")

    json_data = result.as_dict()
    try:
        with open(output_path, "w", encoding="utf-8") as wf:
            json.dump(json_data, wf, indent=2, ensure_ascii=False)
    except Exception as e:
        return {"status": False, "error": f"Failed to save local output.json: {e}"}

    # 7ï¸âƒ£ Also upload the same JSON to Azure Blob Storage (output-results)
    timestamp = datetime.utcnow().strftime("%Y%m%d_%H%M%S")
    unique = uuid.uuid4().hex
    output_container = "output-results"
    target_blob_name = f"{timestamp}_{unique}_extracted.json"

    try:
        async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
            out_container_client = blob_service.get_container_client(output_container)
            try:
                await out_container_client.create_container()
            except Exception:
                pass  # container already exists

            out_blob_client = out_container_client.get_blob_client(target_blob_name)

            # Upload the same file you saved locally
            with open(output_path, "rb") as data:
                await out_blob_client.upload_blob(data, overwrite=True)
    except Exception as e:
        return {"status": False, "error": f"Failed to upload JSON to output-results: {e}"}

    # Build output blob URL for the response
    parsed = bloburl.split("://", 1)[-1]
    account_and_rest = parsed.split("/", 1)[0]
    output_blob_url = f"https://{account_and_rest}/{output_container}/{target_blob_name}"

    print(f"âœ… JSON saved locally and uploaded to: {output_blob_url}")

    # 8ï¸âƒ£ Return final response
    return {
        "status": True,
        "source_file": source_file_name,
        "local_output_json": output_path,
        "output_blob_url": output_blob_url
    }


async def process_input_folder_on_startup() -> None:
    """Background startup function â€” checks ./input for any PDFs and logs them."""
    try:
        input_dir = os.path.join(os.getcwd(), "input")
        if not os.path.exists(input_dir):
            return

        files = [f for f in os.listdir(input_dir) if f.lower().endswith(".pdf")]
        if not files:
            return

        for pdf in files:
            print(f"[startup] Found local PDF in ./input: {pdf} â€” no automatic processing.")
    except Exception as e:
        print(f"[startup] process_input_folder_on_startup error: {e}")
        return


â¸»

ðŸ“¦ Example output (when tested)

{
  "status": true,
  "source_file": "Acord_125_FullForm_Filled 1.pdf",
  "local_output_json": "/home/jarvis/Musaddique/FINALAZUREDATAEXTRACTION/output/output.json",
  "output_blob_url": "https://agenticai1.blob.core.windows.net/output-results/20251027_101657_abcd1234_extracted.json"
}

âœ… You now have both:
	â€¢	A local copy in ./output/output.json
	â€¢	An uploaded copy in your Azure Blob container output-results

â¸»

Would you like the JSON uploaded to keep the same filename as the PDF (e.g., Acord_125_FullForm_Filled 1.json), or keep it with the timestamp + UUID format (as above)?
