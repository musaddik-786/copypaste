Great — understood. I removed OCR, kept only the files you explicitly asked for, and implemented a clean pipeline that:
	•	downloads the PDF from the provided Azure Blob URL to input/
	•	renders the first page of the PDF to a PNG
	•	encodes the PNG as base64 and embeds it in the LLM prompt (you said you want the image passed via prompt)
	•	sends the prompt + image to your Azure OpenAI deployment (vision-capable model assumed) and asks for a strict JSON: { layout, confidence, evidence, notes }
	•	writes a trace file into output/ and returns the JSON result via the FastAPI endpoint

Files included (exact names you asked for):
	•	file_router.py
	•	handler.py
	•	main.py
	•	service.py
	•	test_scripts.py
	•	test_client.py

Make sure you have these system deps installed: poppler (for pdf2image). Python deps: fastapi uvicorn python-dotenv azure-storage-blob pdf2image pillow openai requests langchain_mcp_adapters (install as needed).

Put credentials in .env:

AZURE_STORAGE_CONNECTION_STRING=...
AZURE_OPENAI_API_KEY=...
AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com/
AZURE_OPENAI_DEPLOYMENT_NAME=<your-deployment-name>


⸻

file_router.py

# file_router.py
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import process_blob_for_layout

router = APIRouter()

class LayoutDetectRequest(BaseModel):
    blob_url: str = Field(..., description="Full Azure Blob URL to the PDF file")

@router.post("/layout_detect", summary="Detect document layout (structured/unstructured)")
async def layout_detect(req: LayoutDetectRequest):
    try:
        out = await process_blob_for_layout(req.blob_url)
        return JSONResponse(status_code=200, content=out)
    except Exception as e:
        return JSONResponse(status_code=500, content={"status": False, "error": str(e)})


⸻

handler.py

# handler.py
import os
import json
import base64
import asyncio
from typing import Dict, Any
from dotenv import load_dotenv
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError
from pdf2image import convert_from_path
from PIL import Image
import openai

from service import _require_env, _parse_blob_url, AZURE_STORAGE_CONNECTION_STRING, AZURE_OPENAI_API_KEY, AZURE_OPENAI_ENDPOINT, AZURE_OPENAI_DEPLOYMENT_NAME

load_dotenv()

# directories
BASE_DIR = os.getcwd()
INPUT_DIR = os.path.join(BASE_DIR, "input")
OUTPUT_DIR = os.path.join(BASE_DIR, "output")
os.makedirs(INPUT_DIR, exist_ok=True)
os.makedirs(OUTPUT_DIR, exist_ok=True)

# configure openai for azure (synchronous)
openai.api_type = "azure"
openai.api_base = AZURE_OPENAI_ENDPOINT
# set api_version if needed; you can set openai.api_version = "2024-10-01" etc in .env if required
openai.api_key = AZURE_OPENAI_API_KEY

# helper: async download blob
async def _download_blob_to_local(container: str, blob_path: str, dest_path: str) -> None:
    async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as blob_service:
        container_client = blob_service.get_container_client(container)
        blob_client = container_client.get_blob_client(blob_path)
        try:
            await blob_client.get_blob_properties()
        except ResourceNotFoundError:
            raise FileNotFoundError(f"Blob not found: container={container}, blob={blob_path}")

        stream = await blob_client.download_blob()
        data = await stream.readall()
        with open(dest_path, "wb") as f:
            f.write(data)

def _render_first_page_to_png(pdf_path: str, png_out: str, dpi: int = 300) -> str:
    pages = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)
    if not pages:
        raise RuntimeError("Failed to render the first page of PDF.")
    pages[0].save(png_out, format="PNG")
    return png_out

def _build_prompt_with_image(image_b64: str) -> str:
    # prompt instructs model to return strict JSON only
    return f"""
You are a document layout classifier. You will be given IMAGE_BASE64 containing the first page of a document.
Do NOT ask for anything else. DO NOT return any explanation. Return STRICT JSON only with these keys:
{{"layout": "structured"|"unstructured", "confidence": 0.00, "evidence": ["key1","key2"], "notes": ""}}

Use the image to decide if the document is structured (templated form, labeled fields, tables, repeated layout — e.g., ACORD, invoice, form) or unstructured (free-form quote, letter, narrative).
Return "confidence" as a float 0.0-1.0. "evidence" should be up to 3 short words/phrases found visually that support your decision.

IMAGE_BASE64:
{image_b64}
""".strip()

def _call_llm_with_image(image_b64: str, timeout: int = 30) -> Dict[str, Any]:
    """
    Synchronous call to Azure OpenAI ChatCompletion. This implementation
    embeds the base64 image inside the user message (as you requested).
    Replace this with your SDK's native image attachment mechanism if available.
    """
    prompt = _build_prompt_with_image(image_b64)

    # Use your Azure deployment name as 'engine' for openai.ChatCompletion.create
    try:
        response = openai.ChatCompletion.create(
            engine=AZURE_OPENAI_DEPLOYMENT_NAME,
            messages=[
                {"role": "system", "content": "You are a strict JSON-producing assistant for layout classification."},
                {"role": "user", "content": prompt},
            ],
            temperature=0.0,
            max_tokens=256
        )
    except Exception as e:
        return {"layout": "unknown", "confidence": 0.0, "evidence": [], "notes": f"LLM request failed: {e}"}

    try:
        assistant_text = response["choices"][0]["message"]["content"]
    except Exception:
        return {"layout": "unknown", "confidence": 0.0, "evidence": [], "notes": "No assistant content returned"}

    # parse strict JSON from model output
    try:
        parsed = json.loads(assistant_text)
        # normalize fields
        layout = parsed.get("layout", "unknown")
        confidence = float(parsed.get("confidence", 0.0))
        evidence = parsed.get("evidence", [])
        notes = parsed.get("notes", "")
        return {"layout": layout, "confidence": confidence, "evidence": evidence, "notes": notes, "raw": assistant_text}
    except Exception:
        return {"layout": "unknown", "confidence": 0.0, "evidence": [], "notes": "Failed to parse LLM JSON", "raw": assistant_text}

async def process_blob_for_layout(blob_url: str) -> Dict[str, Any]:
    """
    Main flow:
      - parse blob url
      - download pdf to input/
      - render first page to PNG
      - base64-encode PNG and send to LLM with prompt (no OCR)
      - return LLM JSON and save trace
    """
    _require_env()
    container, blob_path = _parse_blob_url(blob_url)

    filename = os.path.basename(blob_path)
    local_pdf = os.path.join(INPUT_DIR, filename)
    png_name = f"{os.path.splitext(filename)[0]}_page1.png"
    local_png = os.path.join(INPUT_DIR, png_name)

    # download blob async
    await _download_blob_to_local(container, blob_path, local_pdf)

    # render to png
    _render_first_page_to_png(local_pdf, local_png, dpi=300)

    # base64 encode image
    with open(local_png, "rb") as f:
        img_b64 = base64.b64encode(f.read()).decode("utf-8")

    # call LLM (sync)
    llm_result = _call_llm_with_image(img_b64)

    # assemble output
    out = {
        "status": True,
        "source_blob": blob_url,
        "local_pdf": local_pdf,
        "local_png": local_png,
        "llm_result": llm_result
    }

    # save trace
    out_file = os.path.join(OUTPUT_DIR, f"layout_result_{os.path.splitext(filename)[0]}.json")
    with open(out_file, "w", encoding="utf-8") as wf:
        json.dump(out, wf, indent=2)

    return out


⸻

service.py

# service.py
import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
AZURE_OPENAI_API_KEY = os.environ.get("AZURE_OPENAI_API_KEY", "")
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT", "")
AZURE_OPENAI_DEPLOYMENT_NAME = os.environ.get("AZURE_OPENAI_DEPLOYMENT_NAME", "")

def _require_env() -> None:
    missing = []
    if not AZURE_STORAGE_CONNECTION_STRING:
        missing.append("AZURE_STORAGE_CONNECTION_STRING")
    if not AZURE_OPENAI_API_KEY:
        missing.append("AZURE_OPENAI_API_KEY")
    if not AZURE_OPENAI_ENDPOINT:
        missing.append("AZURE_OPENAI_ENDPOINT")
    if not AZURE_OPENAI_DEPLOYMENT_NAME:
        missing.append("AZURE_OPENAI_DEPLOYMENT_NAME")
    if missing:
        raise RuntimeError(f"Missing environment vars: {', '.join(missing)}")

def _parse_blob_url(url: str) -> tuple[str, str]:
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("blob_url must be a valid http(s) URL to Azure Blob Storage.")
    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("blob_url must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)


⸻

main.py

# main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn

from file_router import router as file_router

def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )

app = FastAPI(title="Layout Detection MCP")
apply_cors(app)

app.include_router(file_router, prefix="/api/v1/layout", tags=["layout"])

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8602, reload=True)


⸻

test_scripts.py

# test_scripts.py
import requests
import json

BASE = "http://localhost:8602/api/v1/layout/layout_detect"

def run_test():
    payload = {
        "blob_url": "https://<account>.blob.core.windows.net/<container>/<path>/sample.pdf"
    }
    resp = requests.post(BASE, json=payload, timeout=180)
    print("Status:", resp.status_code)
    try:
        print(json.dumps(resp.json(), indent=2))
    except Exception:
        print(resp.text)

if __name__ == "__main__":
    run_test()


⸻

test_client.py

# test_client.py
import asyncio
from langchain_mcp_adapters.client import MultiServerMCPClient

async def get_tool_list(config_mcp_server):
    client = MultiServerMCPClient(config_mcp_server)
    tools_list = await client.get_tools()
    print("Tools fetched from MCP:", [tool.name for tool in tools_list])
    await client.aclose()
    return tools_list

async def main():
    config_mcp_server = {
        "sanction_checker_mcp": {
            "url": "http://localhost:8602/api/v1/layout/mcp",
            "transport":"streamable_http",
        }
    }
    tools = await get_tool_list(config_mcp_server=config_mcp_server)
    print(tools)

if __name__ == "__main__":
    asyncio.run(main())


⸻

Notes, constraints & recommended follow-ups
	1.	The code intentionally does not run OCR — it sends the image only (embedded as base64 in the prompt) to the model as you requested. If your deployment supports native image attachments via the SDK (better), replace _call_llm_with_image with the native method — embedding base64 in the prompt can be heavy for large images.
	2.	The ChatCompletion call uses the openai Python package configured for Azure. If your environment or SDK differs (azure-ai-openai), let me know and I’ll adapt the call.
	3.	For testing locally: install poppler (apt install poppler-utils on Debian/Ubuntu) and pdf2image + Pillow.
	4.	If you want the shorter flow (no async blob client), I can convert the blob download to synchronous azure.storage.blob.BlobServiceClient — but async is non-blocking and recommended in production.
	5.	If the LLM returns malformed JSON, the endpoint will return the parsing error inside "llm_result.notes". You can add more strictness (e.g., function calling or JSON-schema validation) later.
	6.	I left out the embedding snippet you posted (the AzureOpenAIEmbeddings init) because it’s not needed for layout detection; if you need embeddings elsewhere, tell me where and I’ll add it.

⸻

If you want, I can now:
	•	adapt _call_llm_with_image to use the Azure OpenAI REST / azure-ai-openai SDK native image handling (recommended), or
	•	change the blob download to synchronous code (if you prefer not to run async), or
	•	add a simple threshold check (e.g., if confidence >= 0.75 then route to your extractor — I can wire that into handler.process_blob_for_layout directly.

Which one should I do next?
