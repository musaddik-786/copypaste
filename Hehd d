Nice — I’ll give you a complete, ready-to-drop-in codebase that:
	•	Downloads the PDF from the blob URL into the root input/ folder (preserves filename).
	•	Converts the first page of the PDF to an image file (using pdf2image) and saves that image in input/.
	•	Does not upload the PNG to blob storage.
	•	Calls your Azure OpenAI deployment and passes the image to the model (best-effort; see notes below) — the model is asked to return only one token: structured or unstructured.
	•	To avoid the token / context explosion you saw earlier, the image is downscaled before sending (so the data URI is smaller).
	•	Returns only the verdict (layout_type) in the response payload (and a small llm_raw debug string you can remove).

Important caveats (please read — they matter)
	1.	You asked to send the image to the LLM without uploading to blob. The OpenAI/Azure APIs generally expect images either by URL or as embedded content. Embedding an image still sends the image bytes (as a data URI / base64) to the API. That can be large — I mitigate this by downscaling the image aggressively (default width 800px) and encoding as PNG, which typically keeps the base64 size low enough to avoid the previous token-limit error. This is the pragmatic approach when you don’t want to upload image to blob. If your model is truly vision-capable and your Azure OpenAI deployment accepts binary image attachments (some SDKs do), you can switch to that method; if not, the data-URI approach works in many deployments provided the image is small enough.
	2.	The new OpenAI Python client (>=1.0) has several variants of API shapes (responses, chat.completions, etc.). I implement a best-effort call using the new OpenAI client with responses.create(...) style that supports multimodal inputs. If your deployed client version or Azure API shape differs, you may need to adjust the call slightly (I keep fallbacks in code and descriptive error messages).
	3.	pdf2image requires poppler installed on your machine. I include instructions below.
	4.	I keep llm_raw in results for debugging. Remove it if you want only the verdict.

⸻

Files to drop into your project root. I tried to keep file names exactly as you requested or used previously.

⸻

requirements.txt

fastapi
uvicorn[standard]
python-dotenv
azure-storage-blob>=12.14.1
pdf2image
Pillow
openai
requests
aiofiles

Install poppler on your OS (pdf2image requirement):
	•	Windows: install Poppler for Windows and add poppler\bin to PATH (e.g., from https://github.com/oschwartz10612/poppler-windows/releases).
	•	macOS: brew install poppler
	•	Debian/Ubuntu: sudo apt-get install poppler-utils

⸻

.env (example)

AZURE_STORAGE_CONNECTION_STRING=<your-azure-storage-connection-string>
AZURE_OPENAI_API_KEY=<your-azure-openai-key>
AZURE_OPENAI_ENDPOINT=https://<your-resource>.openai.azure.com/
AZURE_OPENAI_API_VERSION=<your-azure-api-version>   # example: 2024-06-01-preview
AZURE_OPENAI_CHAT_DEPLOYMENT=<your-deployment-name>


⸻

service.py

# service.py
import os
from urllib.parse import urlparse, unquote
from dotenv import load_dotenv

load_dotenv()

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")

def _require_env() -> None:
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING in environment/.env")

def _parse_blob_url(url: str) -> tuple[str, str]:
    parsed = urlparse(url)
    if not parsed.scheme.startswith("http"):
        raise ValueError("url must be a valid http(s) Azure Blob URL.")
    path = parsed.path.lstrip("/")
    if "/" not in path:
        raise ValueError("url must include both container and blob path.")
    parts = path.split("/", 1)
    container = parts[0]
    blob_path = parts[1]
    return unquote(container), unquote(blob_path)


⸻

storage_utils.py

# storage_utils.py
import os
import aiofiles
from azure.storage.blob.aio import BlobServiceClient
from azure.core.exceptions import ResourceNotFoundError
from service import AZURE_STORAGE_CONNECTION_STRING, _parse_blob_url

async def download_blob_to_input_folder(blob_url: str, input_folder: str = "input") -> str:
    """
    Downloads the blob at blob_url into input_folder (root-level) preserving filename.
    Returns the local path to the downloaded file.
    """
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("AZURE_STORAGE_CONNECTION_STRING not set")

    container, blob_path = _parse_blob_url(blob_url)
    filename = os.path.basename(blob_path)
    os.makedirs(input_folder, exist_ok=True)
    local_path = os.path.join(input_folder, filename)

    async with BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING) as service:
        container_client = service.get_container_client(container)
        blob_client = container_client.get_blob_client(blob_path)
        try:
            await blob_client.get_blob_properties()
        except ResourceNotFoundError:
            raise FileNotFoundError(f"Blob not found: container={container} blob={blob_path}")
        stream = await blob_client.download_blob()
        data = await stream.readall()

    # write file async
    async with aiofiles.open(local_path, "wb") as f:
        await f.write(data)

    return local_path


⸻

pdf_utils.py

# pdf_utils.py
import os
from typing import Tuple
from pdf2image import convert_from_path
from PIL import Image
import io
import base64

def convert_first_page_to_png_file(pdf_path: str, output_image_path: str = None, dpi: int = 150, max_width: int = 800) -> Tuple[str, bytes]:
    """
    Convert first page of pdf_path to PNG and save to output_image_path (if provided).
    Returns (local_image_path_or_None, image_bytes).
    The image is downscaled to max_width to reduce size before sending to LLM.
    """
    if not os.path.exists(pdf_path):
        raise FileNotFoundError(f"PDF not found: {pdf_path}")

    # convert first page only
    pages = convert_from_path(pdf_path, dpi=dpi, first_page=1, last_page=1)
    if not pages:
        raise ValueError("PDF has no pages or pdf2image failed to convert")

    pil_img: Image.Image = pages[0]

    # downscale to max_width while keeping aspect ratio
    w, h = pil_img.size
    if w > max_width:
        new_h = int(h * (max_width / w))
        pil_img = pil_img.resize((max_width, new_h), Image.LANCZOS)

    # ensure RGB
    if pil_img.mode != "RGB":
        pil_img = pil_img.convert("RGB")

    buf = io.BytesIO()
    pil_img.save(buf, format="PNG", optimize=True)
    image_bytes = buf.getvalue()

    local_image_path = None
    if output_image_path:
        os.makedirs(os.path.dirname(output_image_path), exist_ok=True)
        with open(output_image_path, "wb") as f:
            f.write(image_bytes)
        local_image_path = output_image_path

    return local_image_path, image_bytes

def image_bytes_to_data_uri(image_bytes: bytes, mime: str = "image/png") -> str:
    b64 = base64.b64encode(image_bytes).decode("utf-8")
    return f"data:{mime};base64,{b64}"


⸻

llm_client.py

# llm_client.py
import os
from openai import OpenAI
from dotenv import load_dotenv
from typing import Dict, Any

load_dotenv()

AZURE_OPENAI_API_KEY = os.environ.get("AZURE_OPENAI_API_KEY")
AZURE_OPENAI_ENDPOINT = os.environ.get("AZURE_OPENAI_ENDPOINT")
AZURE_OPENAI_API_VERSION = os.environ.get("AZURE_OPENAI_API_VERSION")
AZURE_OPENAI_CHAT_DEPLOYMENT = os.environ.get("AZURE_OPENAI_CHAT_DEPLOYMENT")

def _require_azure_openai_env():
    missing = []
    if not AZURE_OPENAI_API_KEY: missing.append("AZURE_OPENAI_API_KEY")
    if not AZURE_OPENAI_ENDPOINT: missing.append("AZURE_OPENAI_ENDPOINT")
    if not AZURE_OPENAI_API_VERSION: missing.append("AZURE_OPENAI_API_VERSION")
    if not AZURE_OPENAI_CHAT_DEPLOYMENT: missing.append("AZURE_OPENAI_CHAT_DEPLOYMENT")
    if missing:
        raise RuntimeError("Missing env vars for Azure OpenAI: " + ", ".join(missing))

def build_client() -> OpenAI:
    _require_azure_openai_env()
    client = OpenAI(
        api_key=AZURE_OPENAI_API_KEY,
        api_base=AZURE_OPENAI_ENDPOINT.rstrip("/"),
        api_type="azure",
        api_version=AZURE_OPENAI_API_VERSION,
    )
    return client

def classify_layout_from_image_bytes(image_bytes: bytes, extra_context: str = "") -> Dict[str, Any]:
    """
    Send the image bytes (as small data URI) in a minimal prompt to the vision-capable model.
    Returns {'raw': raw_text, 'layout_type': 'structured'|'unstructured'}.
    NOTE: This uses a best-effort 'responses' API call for multimodal input.
    If your SDK or deployment expects a slightly different shape, tweak accordingly.
    """
    client = build_client()
    data_uri = None
    try:
        from pdf_utils import image_bytes_to_data_uri
        data_uri = image_bytes_to_data_uri(image_bytes)
    except Exception:
        import base64
        b64 = base64.b64encode(image_bytes).decode("utf-8")
        data_uri = f"data:image/png;base64,{b64}"

    system_text = (
        "You are a document layout classifier. You will be given an image (first page of a document). "
        "Return EXACTLY one lowercase word only, with NO punctuation and NO extra text: 'structured' or 'unstructured'. "
        "If unsure, return 'unstructured'."
    )

    user_text = f"{extra_context}\n\nImage provided below. Return one word: structured OR unstructured."

    # Best-effort: use the Responses API to send multimodal input (image as data URI).
    try:
        resp = client.responses.create(
            model=AZURE_OPENAI_CHAT_DEPLOYMENT,
            input=[
                {"role": "system", "content": system_text},
                {"role": "user", "content": [
                    {"type": "input_text", "text": user_text},
                    {"type": "input_image", "image_url": data_uri}
                ]}
            ],
            max_output_tokens=16,
            temperature=0.0,
        )
    except Exception as e:
        # If the above shape isn't supported by your SDK/runtime, fall back to a chat completion style call
        # embedding the image as a small data URI in the message body (still may work).
        try:
            resp = client.chat.completions.create(
                deployment_id=AZURE_OPENAI_CHAT_DEPLOYMENT,
                messages=[
                    {"role": "system", "content": system_text},
                    {"role": "user", "content": user_text + f"\n\nImage (data URI):\n{data_uri}"}
                ],
                max_tokens=8,
                temperature=0.0,
            )
        except Exception as e2:
            raise RuntimeError(f"LLM request failed: {e} | fallback failed: {e2}")

    # Extract text from response (best-effort for different shapes)
    llm_text = ""
    try:
        # responses API shape
        if hasattr(resp, "output") and resp.output:
            # find first text content
            for item in resp.output:
                if isinstance(item, dict):
                    # older/newer shapes vary; handle multiple possibilities
                    if item.get("type") == "message":
                        # content might be list: item["content"][0]["text"]
                        cont = item.get("content", [])
                        if isinstance(cont, list):
                            for c in cont:
                                if c.get("type") == "output_text" and "text" in c:
                                    llm_text = c["text"]
                                    break
                        elif isinstance(cont, str):
                            llm_text = cont
                    elif item.get("type") == "output_text":
                        llm_text = item.get("text", "")
                    if llm_text:
                        break
        # fallback: chat completions shape
        if not llm_text and hasattr(resp, "choices") and resp.choices:
            try:
                llm_text = resp.choices[0].message["content"]
            except Exception:
                llm_text = getattr(resp.choices[0], "text", "")
    except Exception:
        llm_text = ""

    text_clean = (llm_text or "").strip().lower()
    if text_clean == "structured":
        layout = "structured"
    elif text_clean == "unstructured":
        layout = "unstructured"
    else:
        if "structured" in text_clean:
            layout = "structured"
        elif "unstructured" in text_clean:
            layout = "unstructured"
        else:
            layout = "unstructured"

    return {"raw": llm_text, "layout_type": layout}


⸻

handler.py

# handler.py
import os
import json
from typing import Dict, Any
from service import _require_env
from storage_utils import download_blob_to_input_folder
from pdf_utils import convert_first_page_to_png_file
from llm_client import classify_layout_from_image_bytes

INPUT_FOLDER = "input"
OUTPUT_FOLDER = "output"
TMP_IMAGE_FOLDER = os.path.join(INPUT_FOLDER, "rendered_images")

async def classify_blob_pdf_layout(blob_url: str) -> Dict[str, Any]:
    """
    - download pdf into input/
    - render first page to png saved in input/
    - call LLM with local image bytes (no upload)
    - return minimal verdict
    """
    try:
        _require_env()
    except Exception as e:
        return {"status": False, "error": f"Environment misconfigured: {e}"}

    # 1) download pdf
    try:
        local_pdf = await download_blob_to_input_folder(blob_url, input_folder=INPUT_FOLDER)
    except Exception as e:
        return {"status": False, "error": f"Failed to download blob: {e}"}

    # 2) render first page to PNG and save locally
    try:
        os.makedirs(TMP_IMAGE_FOLDER, exist_ok=True)
        base_name = os.path.splitext(os.path.basename(local_pdf))[0]
        local_png = os.path.join(TMP_IMAGE_FOLDER, f"{base_name}.png")
        _, image_bytes = convert_first_page_to_png_file(local_pdf, output_image_path=local_png, dpi=150, max_width=800)
    except Exception as e:
        return {"status": False, "error": f"Failed converting PDF to image: {e}"}

    # 3) call LLM with image bytes (no blob upload)
    try:
        llm_result = classify_layout_from_image_bytes(image_bytes)
    except Exception as e:
        return {"status": False, "error": f"LLM classification failed: {e}"}

    layout_type = llm_result.get("layout_type")
    llm_raw = llm_result.get("raw", "")

    result = {"status": True, "layout_type": layout_type, "llm_raw": llm_raw}

    # 4) Save minimal output to ./output
    try:
        os.makedirs(OUTPUT_FOLDER, exist_ok=True)
        out_path = os.path.join(OUTPUT_FOLDER, "layout_classification_result.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump(result, f, indent=2, ensure_ascii=False)
    except Exception as e:
        result["save_error"] = str(e)

    return result


⸻

file_router.py

# file_router.py
from fastapi import APIRouter
from fastapi.responses import JSONResponse
from pydantic import BaseModel, Field
from handler import classify_blob_pdf_layout

router = APIRouter()

class LayoutDetectRequest(BaseModel):
    attachment_url: str = Field(..., description="Full Azure Blob URL to the PDF attachment.")

@router.post("/layout_detection_mcp", operation_id="layout_detection_mcp",
             summary="Download PDF from blob, render first page to image and ask LLM to classify layout")
async def detect_layout(request: LayoutDetectRequest):
    try:
        result = await classify_blob_pdf_layout(request.attachment_url)
        return JSONResponse(content={"jsonrpc": "2.0", "id": 1, "result": result}, status_code=200)
    except Exception as e:
        return JSONResponse(content={"jsonrpc": "2.0", "id": 1,
                                     "result": {"status": False, "error": str(e)}}, status_code=200)


⸻

main.py

# main.py
from fastapi import FastAPI
from fastapi.middleware.cors import CORSMiddleware
import uvicorn
from file_router import router as file_router

def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )

app = FastAPI(title="Layout Detection MCP API", version="0.1.0")
apply_cors(app)
app.include_router(file_router, prefix="/api/v1/layout")

if __name__ == "__main__":
    uvicorn.run("main:app", host="0.0.0.0", port=8602, reload=False)


⸻

test_scripts.py

# test_scripts.py
import requests
import json

BASE_URL = "http://localhost:8602/api/v1/layout/layout_detection_mcp"

def run_test():
    payload = {
        "attachment_url": "https://agenticail.blob.core.windows.net/attachment-downloader/Acord_125_FILLED.pdf"
    }
    try:
        resp = requests.post(BASE_URL, json=payload, timeout=180)
        print("HTTP Status:", resp.status_code)
        try:
            data = resp.json()
            print(json.dumps(data, indent=4))
        except Exception:
            print("Response not JSON:", resp.text)
    except Exception as e:
        print("Error calling API:", str(e))

if __name__ == "__main__":
    print("Testing /layout_detection_mcp ...")
    run_test()


⸻

How this addresses your requirement
	•	PDF is downloaded into input/Acord_125_FILLED.pdf (root-level input).
	•	First page converted to input/rendered_images/Acord_125_FILLED.png.
	•	No upload to blob.
	•	Image is sent to LLM (as data URI) but is downscaled to keep size small — preventing the token explosion you saw.
	•	The returned payload contains only the verdict layout_type and status. llm_raw is present for debugging and can be removed.

If you still hit a token / size error
	•	Reduce max_width in convert_first_page_to_png_file(...) to 600 or 400.
	•	Or use the quick alternative: upload the image to blob (small) and pass the URL — that avoids embedding image bytes entirely and is more robust for larger images.
	•	Or pin openai==0.28.0 temporarily while you validate the API shape you have on Azure, then migrate to the new client.

⸻

If you want now I can:
	•	Remove llm_raw from responses.
	•	Add a tiny local fallback heuristic (OpenCV) so if the LLM call fails you get a quick local guess (e.g., detect many table-like lines => structured).
	•	Convert the llm_client to use the responses vs chat.completions variant specifically for your installed OpenAI package version (I can detect installed package and adapt).

Tell me which of the three follow-ups you prefer (or say “do it all”) and I will update the code accordingly.
