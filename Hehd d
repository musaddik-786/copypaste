attachment_handler.py



import os
import json
import base64
import re
from azure.storage.blob import BlobServiceClient
from dotenv import load_dotenv

load_dotenv()

STATE_FILE = os.path.join("storage", "history_state.json")
OUTPUT_DIR = os.path.join("storage", "attachments")
os.makedirs(OUTPUT_DIR, exist_ok=True)
os.makedirs(os.path.dirname(STATE_FILE), exist_ok=True)

AZURE_STORAGE_CONNECTION_STRING = os.environ.get("AZURE_STORAGE_CONNECTION_STRING", "")
AZURE_BLOB_CONTAINER = os.environ.get("AZURE_BLOB_CONTAINER", "")

def _get_blob_service_client():
    if not AZURE_STORAGE_CONNECTION_STRING:
        raise RuntimeError("Missing AZURE_STORAGE_CONNECTION_STRING")
    return BlobServiceClient.from_connection_string(AZURE_STORAGE_CONNECTION_STRING)

def _ensure_container_exists(blob_service_client):
    if not AZURE_BLOB_CONTAINER:
        raise RuntimeError("Missing AZURE_BLOB_CONTAINER")
    container_client = blob_service_client.get_container_client(AZURE_BLOB_CONTAINER)
    try:
        container_client.create_container()
    except Exception:
        pass
    return container_client

def load_state():
    try:
        with open(STATE_FILE, "r", encoding="utf-8") as f:
            return json.load(f)
    except Exception:
        return {}

def save_state(state):
    with open(STATE_FILE, "w", encoding="utf-8") as f:
        json.dump(state, f, indent=2, ensure_ascii=False)

def _walk_parts(parts):
    for p in parts or []:
        yield p
        for sub in p.get("parts", []) or []:
            yield from _walk_parts([sub])

def _blob_exists_for_attachment(container_client, attachment_name):
    """
    Returns True if any blob corresponds to the given attachment_name using the new naming rule:
      - New format: <referenceid>_attachment_<filename>
    We no longer check for exact blob name == attachment_name.
    """
    search_token = f"_attachment_{attachment_name}"
    try:
        # list_blobs may be expensive for very large containers; this directly scans blob names.
        for blob in container_client.list_blobs():
            # match if the search token appears in the blob name (handles folder prefixes)
            if search_token in blob.name:
                return True
    except Exception as e:
        print(f"Error while listing blobs for pattern search '{search_token}': {e}")

    return False

def save_attachments_from_message(service, message):
    """
    Extract attachment filenames from Gmail message and compare with Azure Blob Storage.
    Success: returns {"attachments": [{"filename": "...", "is_duplicate": bool}, ...]}
    No attachments: returns {"attachments": []}
    Azure/Gmail errors: raise exception → caller returns status:false + error
    """
    filenames = []
    parts = message.get("payload", {}).get("parts", [])
    for part in _walk_parts(parts):
        filename = part.get("filename")
        _ = part.get("body", {}) or {}
        if filename:
            filenames.append(filename)

    if not filenames:
        try:
            out_path = os.path.join(OUTPUT_DIR, f"{message.get('id', 'unknown')}_comparison.json")
            with open(out_path, "w", encoding="utf-8") as f:
                json.dump({"attachments": []}, f, indent=2, ensure_ascii=False)
        except Exception as exc:
            print(f"Failed to write comparison result file: {exc}")
        return {"attachments": []}

    # --- NEW: use regex to find attachments containing the word "Accord"
    # If any filenames match, only those will be checked against blob storage.
    accord_pattern = re.compile(r"acord|quote", re.IGNORECASE)
    accord_matches = [fn for fn in filenames if accord_pattern.search(fn)]

    if accord_matches:
        filenames_to_check = accord_matches
    else:
        # fallback to original behaviour (check all attachments)
        # filenames_to_check = filenames
        return {"attachments": "NA"}
    # --- END NEW

    blob_service_client = _get_blob_service_client()
    container_client = _ensure_container_exists(blob_service_client)

    attachment_results = []
    for name in filenames_to_check:
        try:
            # New behavior: only pattern-based match against blobs named like <ref>_attachment_<filename>
            is_duplicate = _blob_exists_for_attachment(container_client, name)
        except Exception as e:
            # if blob check fails for any reason, report as not duplicate and log the error
            print(f"Error checking blob existence for '{name}': {e}")
            is_duplicate = False

        attachment_results.append({
            "filename": name,
            "is_duplicate": is_duplicate
        })

    try:
        out_path = os.path.join(OUTPUT_DIR, f"{message.get('id', 'unknown')}_comparison.json")
        with open(out_path, "w", encoding="utf-8") as f:
            json.dump({"attachments": attachment_results}, f, indent=2, ensure_ascii=False)
    except Exception as exc:
        print(f"Failed to write comparison result file: {exc}")

    return {"attachments": attachment_results}


attachment_service

import asyncio
import json
from datetime import datetime
from typing import Dict, Any
from .gmail_watch import get_gmail_service
from .attachment_handler import save_attachments_from_message

_last_check_time = None
_is_polling = True

def _extract_attachment_comparison(service, msg) -> Dict[str, Any]:
    comparison = save_attachments_from_message(service, msg)
    return {
        "message_id": msg.get("id", ""),
        "attachments": comparison.get("attachments", [])
    }

def get_latest_email_attachment_check() -> Dict[str, Any]:
    """
    Success (even if no emails or no attachments):
      {"status": True, "attachments": [...]}
    Failure (on any exception):
      {"status": False, "error": "..."}
    """
    try:
        service = get_gmail_service()
        results = service.users().messages().list(userId="me", maxResults=1).execute()
        messages = results.get("messages", [])

        if not messages:
            return {"status": True, "attachments": []}

        msg = service.users().messages().get(
            userId="me",
            id=messages[0]["id"],
            format="full"
        ).execute()

        comparison = _extract_attachment_comparison(service, msg)
        return {"status": True, "attachments": comparison.get("attachments", [])}

    except Exception as e:
        return {"status": False, "error": str(e)}

async def _check_for_new_emails():
    """
    During polling: print full JSON ONLY for messages found in this cycle (i.e., since last_check_time).
    On per-message errors: print full JSON with status:false + error.
    """
    global _last_check_time
    try:
        service = get_gmail_service()

        if not _last_check_time:
            results = service.users().messages().list(userId="me", maxResults=1).execute()
        else:
            query = f"after:{int(_last_check_time.timestamp())}"
            results = service.users().messages().list(userId="me", q=query).execute()

        messages = results.get("messages", [])
        for message in messages:
            try:
                msg = service.users().messages().get(
                    userId="me",
                    id=message["id"],
                    format="full"
                ).execute()
                comparison = _extract_attachment_comparison(service, msg)
                result = {
                    "status": True,
                    "attachments": comparison.get("attachments", [])
                }
                print(json.dumps(result, indent=2))
            except Exception as inner_e:
                error_result = {"status": False, "error": str(inner_e)}
                print(json.dumps(error_result, indent=2))

        _last_check_time = datetime.now()

    except Exception as e:
        print(f"Error checking for new emails: {e}")

async def _email_polling_loop():
    global _is_polling
    print(" Email polling loop started.")
    while _is_polling:
        await _check_for_new_emails()
        await asyncio.sleep(10)
    print(" Email polling loop stopped.")

async def start_email_polling():
    global _is_polling, _last_check_time
    _is_polling = True

    # bootstrap = get_latest_email_attachment_check()
    print(" Latest Attachment Comparison:")
    # print(json.dumps(bootstrap, indent=2))

    asyncio.create_task(_email_polling_loop())

async def stop_email_polling():
    global _is_polling
    _is_polling = False
    print("Shutting down email monitoring...")



main.py

from fastapi import FastAPI

from fastapi.middleware.cors import CORSMiddleware

from fastapi_mcp import FastApiMCP

import uvicorn
 
from test_email_router import router as email_router

from MCP.email_reader_mcp.email_handler import get_latest_email, start_email_polling, stop_email_polling

from contextlib import asynccontextmanager

from MCP.attachment_downloader_mcp.attachment_handler import get_latest_email_attachment_downloader, start_email_polling, stop_email_polling

# from data_extractor_mcp.image_processor import process_input_folder_on_startup

import asyncio

# from email_output_saver_mcp.email_output_saver import start_output_saver_polling, stop_output_saver_polling

#custom_model_dataextractor
from MCP.custom_model_dataextractor_mcp.image_processor import process_input_folder_on_startup

# risk type checker mcp


# Risk score Calculator MCP 
# from risk_score_calculator_mcp.risk_calculator import RuleBasedRiskCalculator 




async def _start_processing_task() -> None:
    """
    Create and store a background task that runs the processor.
    This returns immediately after scheduling the task.
    """
    # schedule background processing
    task = asyncio.create_task(process_input_folder_on_startup())
    # stash on the running app (will be attached in lifespan)
    return task

async def _stop_processing_task(task: asyncio.Task) -> None:
    """
    Cancel and await the background task if it's still running.
    """
    if task and not task.done():
        task.cancel()
        try:
            await task
        except Exception:
            # swallow exceptions on shutdown
            pass


# @asynccontextmanager
# async def lifespan(app: FastAPI):
#     # Startup
#     await start_email_polling()
#     yield
#     # Shutdown
#     await stop_email_polling()
#     # Startup: schedule the processing task and attach to app.state
#     try:
#         app.state.processing_task = await _start_processing_task()
#     except Exception:
#         app.state.processing_task = None

#     yield

#     # Shutdown: cancel the task if running
#     task = getattr(app.state, "processing_task", None)
#     if task:
#         await _stop_processing_task(task)

#     # Startup: perform a single upload pass
#     await start_output_saver_polling()
#     yield
#     # Shutdown: no-op
#     await stop_output_saver_polling()


app = FastAPI()

# app = FastAPI(lifespan=lifespan)

def apply_cors(app: FastAPI):
    app.add_middleware(
        CORSMiddleware,
        allow_origins=["*"],
        allow_methods=["*"],
        allow_headers=["*"],
    )

def create_sub_app(title: str, description: str, version: str = "0.1.0") -> FastAPI:
    app = FastAPI(title=title, description=description, version=version)
    #app = FastAPI(title=title, description=description, version=version, lifespan=lifespan)
    apply_cors(app)
    return app

apply_cors(app)

email_intent_agent_app = create_sub_app(title="email_intent_agent_mcps", description="role is to handle the initial stage of the underwriting workflow — focused solely on understanding the incoming emails")
email_intent_agent_app.include_router(email_router)
FastApiMCP(email_intent_agent_app, include_operations=["email_reader_mcp","attachment_checker_mcp","email_attachment_mcp","layout_detection_mcp","document_extract_mcp","excel_loader_mcp", "pdf_vectorizer_mcp", "retriever_mcp"]).mount_http()
#app.mount("/mcp",email_reader_app)
app.mount("/api/v1/email_intent_agent",email_intent_agent_app)

if __name__=="__main__":
    uvicorn.run(app, host="0.0.0.0", port=8466)



test_email_router.py

class AttachmentCheckerMCP(BaseModel):
    
    """Represents the functionality of checking email attachments for duplicate in Azure Blob."""

    AgentName: str = Field(default="EmailAttachmentChecker", description="Agent name.")
    UserId: str = Field(default="markRuffalo", description="User id (default).")
@router.post("/attachment_checker_mcp", operation_id="attachment_checker_mcp")
# async def attachment_checker_mcp(p_body: AttachmentCheckerMCP):
async def attachment_checker_mcp():

    """
    Fetches the latest Gmail email and checks if its attachments already exist in Azure Blob Storage.
    Args:
        p_body (AttachmentCheckerMCP): Request body containing:
            AgentName (str): The name of the MCP agent calling this endpoint.
            UserId (str): Identifier for the user making the request.
    Returns:
        JSONResponse: Returns JSON-RPC response containing:
            status (bool): True if executed successfully, else False.
            attachments (list): List of attachment comparison results.
            error (str, optional): Error message if any exception occurred.
    """

    try:
        result = get_latest_email_attachment_check()
        if result.get("status") is True and "attachments" not in result:
            result["attachments"] = []
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": result
            }
        )
    except Exception as e:
        return JSONResponse(
            content={
                "jsonrpc": "2.0",
                "id": 1,
                "result": {"status": False, "error": str(e)}
            },
            status_code=200
        )

